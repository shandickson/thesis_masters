
@article{grovesExperimentsProducingNonresponse2006,
	title = {Experiments in {Producing} {Nonresponse} {Bias}},
	volume = {70},
	issn = {1537-5331, 0033-362X},
	url = {http://academic.oup.com/poq/article/70/5/720/4084446/Experiments-in-Producing-Nonresponse-Bias},
	doi = {10.1093/poq/nfl036},
	abstract = {While nonresponse rates in household surveys are increasing in most industrialized nations, the increasing rates do not always produce nonresponse bias in survey estimates. The linkage between nonresponse rates and nonresponse bias arises from the presence of a covariance between response propensity and the survey variables of interest. To understand the covariance term, researchers must think about the common influences on response propensity and the survey variable. Three variables appear to be especially relevant in this regard: interest in the survey topic, reactions to the survey sponsor, and the use of incentives. A set of randomized experiments tests whether those likely to be interested in the stated survey topic participate at higher rates and whether nonresponse bias on estimates involving variables central to the survey topic is affected by this. The experiments also test whether incentives disproportionately increase the participation of those less interested in the topic. The experiments show mixed results in support of these key hypotheses.},
	language = {en},
	number = {5},
	urldate = {2022-11-21},
	journal = {Public Opinion Quarterly},
	author = {Groves, Robert M. and Couper, Mick P. and Presser, Stanley and Singer, Eleanor and Tourangeau, Roger and Acosta, Giorgina Piani and Nelson, Lindsay},
	year = {2006},
	pages = {720--736},
	file = {Groves et al. - 2006 - Experiments in Producing Nonresponse Bias.pdf:/Users/shannondickson/Zotero/storage/9D3H7DB2/Groves et al. - 2006 - Experiments in Producing Nonresponse Bias.pdf:application/pdf},
}

@article{olsonSurveyParticipationNonresponse2006,
	title = {Survey {Participation}, {Nonresponse} {Bias}, {Measurement} {Error} {Bias}, and {Total} {Bias}},
	volume = {70},
	issn = {1537-5331, 0033-362X},
	url = {http://academic.oup.com/poq/article/70/5/737/4084448/Survey-Participation-Nonresponse-Bias-Measurement},
	doi = {10.1093/poq/nfl038},
	abstract = {A common hypothesis about practices to reduce survey nonresponse is that those persons brought into the respondent pool through persuasive efforts may provide data filled with measurement error. Two questions flow from this hypothesis. First, does the mean square error of a statistic increase when sample persons who are less likely to be contacted or cooperate are incorporated into the respondent pool? Second, do nonresponse bias estimates made on the respondents, using survey reports instead of records, provide accurate information about nonresponse bias? Using a unique data set, the Wisconsin Divorce Study, with divorce records as the frame and questions about the frame information included in the questionnaire, this article takes a first look into these two issues. We find that the relationship between nonresponse bias, measurement error bias, and response propensity is statistic- specific and specific to the type of nonresponse. Total bias tends to be lower on estimates calculated using all respondents, compared with those with only the highest contact and cooperation propensities, and nonresponse bias analyses based on respondents yield conclusions similar to those based on records. Finally, we find that error properties of statistics may differ from error properties of the individual variables used to calculate the statistics.},
	language = {en},
	number = {5},
	urldate = {2022-11-21},
	journal = {Public Opinion Quarterly},
	author = {Olson, Kristen},
	year = {2006},
	pages = {737--758},
	file = {Olson - 2006 - Survey Participation, Nonresponse Bias, Measuremen.pdf:/Users/shannondickson/Zotero/storage/RDAT4HQS/Olson - 2006 - Survey Participation, Nonresponse Bias, Measuremen.pdf:application/pdf},
}

@article{bethlehemIndicatorsRepresentativenessSurvey,
	title = {Indicators for the {Representativeness} of {Survey} {Response}},
	abstract = {Many survey organizations use the response rate as an indicator for the quality of survey data. As a consequence, a variety of measures are implemented to reduce non-response or to maintain response at an acceptable level. However, the response rate is not necessarily a good indicator of non-response bias. A higher response rate does not imply smaller non-response bias. What matters is how the composition of the response differs from the composition of the sample as a whole. This paper describes the concept of R-indicators to assess potential differences between the sample and the response. Such indicators may facilitate analysis of survey response over time, between various fieldwork strategies or data collection modes. Some practical examples are given.},
	language = {en},
	author = {Bethlehem, Jelke and Cobben, Fannie and Schouten, Barry},
	pages = {8},
	file = {Bethlehem et al. - Indicators for the Representativeness of Survey Re.pdf:/Users/shannondickson/Zotero/storage/5ULJ6YT9/Bethlehem et al. - Indicators for the Representativeness of Survey Re.pdf:application/pdf},
}

@article{huaLinkSurveyResponse2023,
	title = {The {Link} {Between} {Survey} {Response} {Rates} and {Nonresponse} {Bias}: {Theory}, {Simulations}, and {Empirical} {Evidence} {From} the {Household} {Pulse} {Survey}},
	language = {en},
	author = {Hua, Tim Tian},
	year = {2023},
	pages = {20},
	file = {Hua - The Link Between Survey Response Rates and Nonresp.pdf:/Users/shannondickson/Zotero/storage/4F52CTRH/Hua - The Link Between Survey Response Rates and Nonresp.pdf:application/pdf},
}

@article{gundgaardEffectNonresponseEstimates2007,
	title = {The effect of non-response on estimates of health care utilisation: linking health surveys and registers},
	volume = {18},
	issn = {1101-1262, 1464-360X},
	shorttitle = {The effect of non-response on estimates of health care utilisation},
	url = {https://academic.oup.com/eurpub/article-lookup/doi/10.1093/eurpub/ckm103},
	doi = {10.1093/eurpub/ckm103},
	language = {en},
	number = {2},
	urldate = {2022-11-21},
	journal = {The European Journal of Public Health},
	author = {Gundgaard, J. and Ekholm, O. and Hansen, E. H. and Rasmussen, N. Kr.},
	month = dec,
	year = {2007},
	pages = {189--194},
	file = {Gundgaard et al. - 2007 - The effect of non-response on estimates of health .pdf:/Users/shannondickson/Zotero/storage/FQRIK5ZW/Gundgaard et al. - 2007 - The effect of non-response on estimates of health .pdf:application/pdf},
}

@article{masseyWhereWeGo2013,
	title = {Where {Do} {We} {Go} from {Here}? {Nonresponse} and {Social} {Measurement}},
	volume = {645},
	issn = {0002-7162, 1552-3349},
	shorttitle = {Where {Do} {We} {Go} from {Here}?},
	url = {http://journals.sagepub.com/doi/10.1177/0002716212464191},
	doi = {10.1177/0002716212464191},
	abstract = {Surveys undergird government statistical systems and social scientific research throughout the world. Rates of nonresponse are rising in cross-sectional surveys (those conducted during a fixed period of time and not repeated). Although this trend worries those concerned with the validity of survey data, there is no necessary relationship between the rate of nonresponse and the degree of bias. A high rate of nonresponse merely creates the potential for bias, but the degree of bias depends on how factors promoting nonresponse are related to variables of interest. Nonresponse can be reduced by offering financial incentives to respondents and by careful design before entering the field, creating a trade-off between cost and potential bias. When bias is suspected, it can be countered by weighting individual cases by the inverse of their response propensity. Response propensities are typically estimated using a logistic regression equation to predict the dichotomous outcome of survey participation as a function of auxiliary variables. The Multi-level Integrated Database Approach employs multiple databases to collect as much information as possible about the target sample during the initial sampling stage and at all possible levels of aggregation to maximize the accuracy of estimated response propensities.},
	language = {en},
	number = {1},
	urldate = {2022-11-21},
	journal = {The ANNALS of the American Academy of Political and Social Science},
	author = {Massey, Douglas S. and Tourangeau, Roger},
	month = jan,
	year = {2013},
	pages = {222--236},
	file = {Massey and Tourangeau - 2013 - Where Do We Go from Here Nonresponse and Social M.pdf:/Users/shannondickson/Zotero/storage/ANSYLER6/Massey and Tourangeau - 2013 - Where Do We Go from Here Nonresponse and Social M.pdf:application/pdf;v48i04.pdf:/Users/shannondickson/Zotero/storage/5GDLBDRV/v48i04.pdf:application/pdf},
}

@article{wagnerComparisonAlternativeIndicators2012,
	title = {A {Comparison} of {Alternative} {Indicators} for the {Risk} of {Nonresponse} {Bias}},
	volume = {76},
	issn = {0033-362X, 1537-5331},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfs032},
	doi = {10.1093/poq/nfs032},
	abstract = {The response rate has played a key role in measuring the risk of nonresponse bias. However, recent empirical evidence has called into question the utility of the response rate for predicting nonresponse bias. The search for alternatives to the response rate has begun. The present article offers a typology for these indicators, briefly describes the strengths and weaknesses of each type, and suggests directions for future research. New standards for reporting on the risk of nonresponse bias may be needed. Certainly, any analysis into the risk of nonresponse bias will need to be multifaceted and include sensitivity analyses designed to test the impact of key assumptions about the data that are missing due to nonresponse.},
	language = {en},
	number = {3},
	urldate = {2022-11-21},
	journal = {Public Opinion Quarterly},
	author = {Wagner, J.},
	month = sep,
	year = {2012},
	pages = {555--575},
	file = {Wagner - 2012 - A Comparison of Alternative Indicators for the Ris.pdf:/Users/shannondickson/Zotero/storage/FT7926UI/Wagner - 2012 - A Comparison of Alternative Indicators for the Ris.pdf:application/pdf},
}

@article{nishimuraAlternativeIndicatorsRisk2016,
	title = {Alternative {Indicators} for the {Risk} of {Non}-response {Bias}: {A} {Simulation} {Study}: {Alternative} {Indicators} for {Non}-response {Bias}},
	volume = {84},
	issn = {03067734},
	shorttitle = {Alternative {Indicators} for the {Risk} of {Non}-response {Bias}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/insr.12100},
	doi = {10.1111/insr.12100},
	abstract = {The growth of nonresponse rates for social science surveys has led to increased concern about the risk of nonresponse bias. Unfortunately, the nonresponse rate is a poor indicator of when nonresponse bias is likely to occur. We consider in this paper a set of alternative indicators. A large-scale simulation study is used to explore how each of these indicators performs in a variety of circumstances. Although, as expected, none of the indicators fully depicts the impact of nonresponse in survey esti mates, we discuss how they can be used when creating a plausible account of the risks for nonresponse bias for a survey. We also describe an interesting characteristic of the FMI that may be helpful in diagnosing NMAR mechanisms in certain situations.},
	language = {en},
	number = {1},
	urldate = {2022-11-21},
	journal = {International Statistical Review},
	author = {Nishimura, Raphael and Wagner, James and Elliott, Michael},
	month = apr,
	year = {2016},
	pages = {43--62},
	file = {Nishimura et al. - 2016 - Alternative Indicators for the Risk of Non-respons.pdf:/Users/shannondickson/Zotero/storage/UQE8F8WA/Nishimura et al. - 2016 - Alternative Indicators for the Risk of Non-respons.pdf:application/pdf},
}

@article{grovesImpactNonresponseRates2008,
	title = {The {Impact} of {Nonresponse} {Rates} on {Nonresponse} {Bias}: {A} {Meta}-{Analysis}},
	volume = {72},
	issn = {0033-362X, 1537-5331},
	shorttitle = {The {Impact} of {Nonresponse} {Rates} on {Nonresponse} {Bias}},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfn011},
	doi = {10.1093/poq/nfn011},
	abstract = {Fifty-nine methodological studies were designed to estimate the magnitude of nonresponse bias in statistics of interest. These studies use a variety of designs: sampling frames with rich variables, data from administrative records matched to sample case, use of screeninginterview data to describe nonrespondents to main interviews, followup of nonrespondents to initial phases of ﬁeld effort, and measures of behavior intentions to respond to a survey. This permits exploration of which circumstances produce a relationship between nonresponse rates and nonresponse bias and which, do not. The predictors are design features of the surveys, characteristics of the sample, and attributes of the survey statistics computed in the surveys.},
	language = {en},
	number = {2},
	urldate = {2022-11-21},
	journal = {Public Opinion Quarterly},
	author = {Groves, R. M. and Peytcheva, E.},
	month = may,
	year = {2008},
	pages = {167--189},
	file = {Groves and Peytcheva - 2008 - The Impact of Nonresponse Rates on Nonresponse Bia.pdf:/Users/shannondickson/Zotero/storage/GVCDTC5L/Groves and Peytcheva - 2008 - The Impact of Nonresponse Rates on Nonresponse Bia.pdf:application/pdf},
}

@article{kohlerAllOddsRobustness2022,
	title = {Against all odds: {On} the robustness of probability samples against decreases in response rates},
	abstract = {Responses rates in surveys with probability samples have decreased in the last decades, but has this decrease caused a decline in sample quality? Our presentation addresses this question with an analysis of methodological data describing 739 surveys from four cross-national survey projects: European Quality of Life Survey, European Social Survey, European Values Study, and International Social Survey Programme, between 1999 and 2019. Based on a theoretical model of factors that shape unit nonresponse and unit nonresponse bias, we estimate causal effects of historical time on both, nonresponse and nonresponse bias, as well as the contribution of nonresponse to nonresponse bias. Results show that the decline in response rates was not accompanied by a drop in nonresponse bias.},
	language = {en},
	author = {Kohler, Ulrich and Jabkowski, Piotr and Kołczyńska, Marta},
	year = {2022},
	pages = {44},
	file = {Kohler et al. - Against all odds On the robustness of probability.pdf:/Users/shannondickson/Zotero/storage/HHC28DUF/Kohler et al. - Against all odds On the robustness of probability.pdf:application/pdf},
}

@article{tourangeauSensitiveTopicsReluctant2010,
	title = {Sensitive {Topics} and {Reluctant} {Respondents}: {Demonstrating} a {Link} between {Nonresponse} {Bias} and {Measurement} {Error}},
	volume = {74},
	issn = {0033-362X, 1537-5331},
	shorttitle = {Sensitive {Topics} and {Reluctant} {Respondents}},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfq004},
	doi = {10.1093/poq/nfq004},
	language = {en},
	number = {3},
	urldate = {2022-11-24},
	journal = {Public Opinion Quarterly},
	author = {Tourangeau, R. and Groves, R. M. and Redline, C. D.},
	month = sep,
	year = {2010},
	pages = {413--432},
	file = {0002716212461748.pdf:/Users/shannondickson/Zotero/storage/SBWLMMYS/0002716212461748.pdf:application/pdf;0049124117701479.pdf:/Users/shannondickson/Zotero/storage/CF4YPSMZ/0049124117701479.pdf:application/pdf;brick and tourangeau (2017) re-analysis of grives and peytcheva.pdf:/Users/shannondickson/Zotero/storage/IFY9MJGA/brick and tourangeau (2017) re-analysis of grives and peytcheva.pdf:application/pdf;draft_20221019_clean.pdf:/Users/shannondickson/Zotero/storage/5WVT84TX/draft_20221019_clean.pdf:application/pdf;MAGNA_preprint.pdf:/Users/shannondickson/Zotero/storage/FXXPZ5WA/MAGNA_preprint.pdf:application/pdf;ORM2007-Nonresponse.pdf:/Users/shannondickson/Zotero/storage/Q5QHPZ5H/ORM2007-Nonresponse.pdf:application/pdf;preprint_sample_size_analysis_method.pdf:/Users/shannondickson/Zotero/storage/HMVJEYGJ/preprint_sample_size_analysis_method.pdf:application/pdf;Tourangeau (2017) presidential address.pdf:/Users/shannondickson/Zotero/storage/MW78U97Z/Tourangeau (2017) presidential address.pdf:application/pdf;Tourangeau et al. - 2010 - Sensitive Topics and Reluctant Respondents Demons.pdf:/Users/shannondickson/Zotero/storage/AXCK5N7U/Tourangeau et al. - 2010 - Sensitive Topics and Reluctant Respondents Demons.pdf:application/pdf;TourangeauGrovesRedline.pdf:/Users/shannondickson/Zotero/storage/UH3EUFGE/TourangeauGrovesRedline.pdf:application/pdf;v93i08.pdf:/Users/shannondickson/Zotero/storage/X9ZPMJTH/v93i08.pdf:application/pdf},
}

@article{peytchevNotAllSurvey2009,
	title = {Not {All} {Survey} {Effort} {Is} {Equal}: {Reduction} of {Nonresponse} {Bias} and {Nonresponse} {Error}},
	volume = {73},
	issn = {0033-362X},
	shorttitle = {Not {All} {Survey} {Effort} {Is} {Equal}},
	url = {http://www.jstor.org/stable/40467642},
	abstract = {Nonexperimental and experimental studies have shown a lack of association between survey effort and nonresponse bias. This does not necessarily mean, however, that additional effort could not reduce nonresponse bias. Theories on nonresponse would suggest the use of different recruiting methods for additional survey effort in order to address nonresponse bias. This study looks at changes in survey estimates as a function of making additional calls under the same protocol and additional calls under a different protocol. Respondents who were interviewed as a result of more than five call attempts were not significantly different on any of the key survey variables than those interviewed with fewer than five calls. Those interviewed under a different survey protocol, however, were different on 5 of 12 measures. Additional interviews under both the same and different protocols contributed to the reduction of total nonresponse error. In sum, the use of multiple protocols for part of the survey effort increased the response rate, changed point estimates, and achieved lower total nonresponse error. Future work is needed on optimizing survey designs that implement multiple survey protocols.},
	number = {4},
	urldate = {2022-12-01},
	journal = {The Public Opinion Quarterly},
	author = {Peytchev, Andy and Baxter, Rodney K. and Carley-Baxter, Lisa R.},
	year = {2009},
	note = {Publisher: [Oxford University Press, American Association for Public Opinion Research]},
	pages = {785--806},
	file = {JSTOR Full Text PDF:/Users/shannondickson/Zotero/storage/9TSKX9XD/Peytchev et al. - 2009 - Not All Survey Effort Is Equal Reduction of Nonre.pdf:application/pdf},
}

@misc{AssessingTrendsDecomposing,
	title = {Assessing {Trends} and {Decomposing} {Change} in {Nonresponse} {Bias}: {The} {Case} of {Bias} in {Cohort} {Distributions}},
	shorttitle = {Assessing {Trends} and {Decomposing} {Change} in {Nonresponse} {Bias}},
	url = {http://journals.sagepub.com/doi/epub/10.1177/0049124117701479},
	language = {en},
	urldate = {2022-12-01},
	doi = {10.1177/0049124117701479},
}

@article{hedlinThereSafeArea2020,
	title = {Is there a 'safe area' where the nonresponse rate has only a modest effect on bias despite non-ignorable nonresponse?},
	volume = {88},
	issn = {1751-5823},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12359},
	doi = {10.1111/insr.12359},
	abstract = {Rising nonresponse rates in social surveys make the issue of nonresponse bias contentious. There are conflicting messages about the importance of high response rates and the hazards of low rates. Some articles (e.g. Groves and Peytcheva, 2008) suggest that the response rate is in general not a good predictor of survey quality. Equally, it is well known that nonresponse may induce bias and increase data collection costs. We go back in the history of the literature of nonresponse and suggest a possible reason to the notion that even a rather small nonresponse rate makes the quality of a survey debatable. We also explore the relationship between nonresponse rate and bias, assuming non-ignorable nonresponse and focusing on estimates of totals or means. We show that there is a ‘safe area’ enclosed by the response rate on the one hand and the correlation between the response propensity and the study variable on the other hand; in this area, (1) the response rate does not greatly affect the nonresponse bias, and (2) the nonresponse bias is small.},
	language = {en},
	number = {3},
	urldate = {2022-12-01},
	journal = {International Statistical Review},
	author = {Hedlin, Dan},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12359},
	keywords = {non-ignorable response, nonresponse bias, nonresponse rate, survey quality},
	pages = {642--657},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/XNEQ2BLQ/Hedlin - 2020 - Is there a 'safe area' where the nonresponse rate .pdf:application/pdf},
}

@article{borsboomNetworkAnalysisMultivariate2021,
	title = {Network analysis of multivariate data in psychological science},
	volume = {1},
	copyright = {2021 Springer Nature Limited},
	issn = {2662-8449},
	url = {https://www.nature.com/articles/s43586-021-00055-w},
	doi = {10.1038/s43586-021-00055-w},
	abstract = {In recent years, network analysis has been applied to identify and analyse patterns of statistical association in multivariate psychological data. In these approaches, network nodes represent variables in a data set, and edges represent pairwise conditional associations between variables in the data, while conditioning on the remaining variables. This Primer provides an anatomy of these techniques, describes the current state of the art and discusses open problems. We identify relevant data structures in which network analysis may be applied: cross-sectional data, repeated measures and intensive longitudinal data. We then discuss the estimation of network structures in each of these cases, as well as assessment techniques to evaluate network robustness and replicability. Successful applications of the technique in different research areas are highlighted. Finally, we discuss limitations and challenges for future research.},
	language = {en},
	number = {1},
	urldate = {2022-12-01},
	journal = {Nature Reviews Methods Primers},
	author = {Borsboom, Denny and Deserno, Marie K. and Rhemtulla, Mijke and Epskamp, Sacha and Fried, Eiko I. and McNally, Richard J. and Robinaugh, Donald J. and Perugini, Marco and Dalege, Jonas and Costantini, Giulio and Isvoranu, Adela-Maria and Wysocki, Anna C. and van Borkulo, Claudia D. and van Bork, Riet and Waldorp, Lourens J.},
	month = aug,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Scientific data, Statistics},
	pages = {1--18},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/DS3WBAZ3/Borsboom et al. - 2021 - Network analysis of multivariate data in psycholog.pdf:application/pdf},
}

@misc{haslbeckMgmEstimatingTimeVarying2020,
	title = {mgm: {Estimating} {Time}-{Varying} {Mixed} {Graphical} {Models} in {High}-{Dimensional} {Data}},
	shorttitle = {mgm},
	url = {http://arxiv.org/abs/1510.06871},
	abstract = {We present the R-package mgm for the estimation of k-order Mixed Graphical Models (MGMs) and mixed Vector Autoregressive (mVAR) models in high-dimensional data. These are a useful extensions of graphical models for only one variable type, since data sets consisting of mixed types of variables (continuous, count, categorical) are ubiquitous. In addition, we allow to relax the stationarity assumption of both models by introducing time-varying versions MGMs and mVAR models based on a kernel weighting approach. Time-varying models oﬀer a rich description of temporally evolving systems and allow to identify external inﬂuences on the model structure such as the impact of interventions. We provide the background of all implemented methods and provide fully reproducible examples that illustrate how to use the package.},
	language = {en},
	urldate = {2022-12-01},
	publisher = {arXiv},
	author = {Haslbeck, Jonas M. B. and Waldorp, Lourens J.},
	month = feb,
	year = {2020},
	note = {arXiv:1510.06871 [stat]},
	keywords = {Statistics - Applications},
	file = {Haslbeck and Waldorp - 2020 - mgm Estimating Time-Varying Mixed Graphical Model.pdf:/Users/shannondickson/Zotero/storage/S8955N6K/Haslbeck and Waldorp - 2020 - mgm Estimating Time-Varying Mixed Graphical Model.pdf:application/pdf},
}

@article{peytchevConsequencesSurveyNonresponse2013a,
	title = {Consequences of {Survey} {Nonresponse}},
	volume = {645},
	issn = {0002-7162, 1552-3349},
	url = {http://journals.sagepub.com/doi/10.1177/0002716212461748},
	doi = {10.1177/0002716212461748},
	abstract = {Nonresponse is a prominent problem in sample surveys. At face value, it reduces the trust in survey estimates. Nonresponse undermines the probability-based inferential mechanism and introduces the potential for nonresponse bias. In addition, there are other important consequences. The effort to limit increasing nonresponse has led to higher survey costs—allocation of greater resources to measure and reduce nonresponse. Nonresponse has also led to greater survey complexity in terms of design, implementation, and processing of survey data, such as the use of multiphase and responsive designs. The use of mixed-mode and multiframe designs to address nonresponse increases complexity but also introduces other sources of error. Surveys have to rely to a greater extent on statistical adjustments and auxiliary data. This article describes the major consequences of survey nonresponse, with particular attention to recent years.},
	language = {en},
	number = {1},
	urldate = {2022-12-01},
	journal = {The ANNALS of the American Academy of Political and Social Science},
	author = {Peytchev, Andy},
	month = jan,
	year = {2013},
	pages = {88--111},
	file = {Peytchev - 2013 - Consequences of Survey Nonresponse.pdf:/Users/shannondickson/Zotero/storage/YBSUTPXN/Peytchev - 2013 - Consequences of Survey Nonresponse.pdf:application/pdf},
}

@article{gummerAssessingTrendsDecomposing2019,
	title = {Assessing {Trends} and {Decomposing} {Change} in {Nonresponse} {Bias}: {The} {Case} of {Bias} in {Cohort} {Distributions}},
	volume = {48},
	issn = {0049-1241, 1552-8294},
	shorttitle = {Assessing {Trends} and {Decomposing} {Change} in {Nonresponse} {Bias}},
	url = {http://journals.sagepub.com/doi/10.1177/0049124117701479},
	doi = {10.1177/0049124117701479},
	abstract = {Survey research is still confronted by a trend of increasing nonresponse rates. In this context, several methodological advances have been made to stimulate participation and avoid bias. Yet, despite the growing number of tools and methods to deal with nonresponse, little is known about whether nonresponse biases show similar trends as nonresponse rates and what mechanisms (if any) drive changes in bias. Our article focuses on biases in cohort distributions in the U.S. and German general social surveys from 1980 to 2012 as one of the key variables in the social sciences. To supplement our cross-national comparison of these trends, we decompose changes into within-cohort change (WCC) and between-cohort change. We find that biases in cohort distributions have remained relatively stable and at a relatively low level in both countries. Furthermore, WCC (i.e., survey climate) accounts for the major part of the change in nonresponse bias.},
	language = {en},
	number = {1},
	urldate = {2022-12-01},
	journal = {Sociological Methods \& Research},
	author = {Gummer, Tobias},
	month = feb,
	year = {2019},
	pages = {92--115},
	file = {Gummer - 2019 - Assessing Trends and Decomposing Change in Nonresp.pdf:/Users/shannondickson/Zotero/storage/R2FEWDZZ/Gummer - 2019 - Assessing Trends and Decomposing Change in Nonresp.pdf:application/pdf},
}

@article{brickResponsiveSurveyDesigns2017,
	title = {Responsive {Survey} {Designs} for {Reducing} {Nonresponse} {Bias}},
	volume = {33},
	issn = {2001-7367},
	url = {https://www.sciendo.com/article/10.1515/jos-2017-0034},
	doi = {10.1515/jos-2017-0034},
	abstract = {Abstract
            Survey researchers have been investigating alternative approaches to reduce data collection costs while mitigating the risk of nonresponse bias or to produce more accurate estimates within the same budget. Responsive or adaptive design has been suggested as one means for doing this. Falling survey response rates and the need to find effective ways of implementing responsive design has focused attention on the relationship between response rates and nonresponse bias. In our article, we re-examine the data compiled by Groves and Peytcheva (2008) in their influential article and show there is an important between-study component of variance in addition to the within-study variance highlighted in the original analysis. We also show that theory implies that raising response rates can help reduce the nonresponse bias on average across the estimates within a study. We then propose a typology of response propensity models that help explain the empirical findings, including the relative weak relationship between nonresponse rates and nonresponse bias. Using these results, we explore when responsive design tools such as switching modes, giving monetary incentives, and increasing the level of effort are likely to be effective. We conclude with some comments on the use of responsive design and weighting to control nonresponse bias.},
	language = {en},
	number = {3},
	urldate = {2022-12-01},
	journal = {Journal of Official Statistics},
	author = {Brick, J. Michael and Tourangeau, Roger},
	month = sep,
	year = {2017},
	pages = {735--752},
	file = {Brick and Tourangeau - 2017 - Responsive Survey Designs for Reducing Nonresponse.pdf:/Users/shannondickson/Zotero/storage/Z4Q8YCM8/Brick and Tourangeau - 2017 - Responsive Survey Designs for Reducing Nonresponse.pdf:application/pdf},
}

@article{wernerReportingNonresponseAnalyses2007,
	title = {The {Reporting} of {Nonresponse} {Analyses} in {Survey} {Research}},
	volume = {10},
	issn = {1094-4281, 1552-7425},
	url = {http://journals.sagepub.com/doi/10.1177/1094428106292892},
	doi = {10.1177/1094428106292892},
	abstract = {Because survey respondents may not be representative of the population being studied, the external validity of many research conclusions may be of concern. Nonresponse analyses helps address this concern. The purpose of this article is to identify how frequently nonresponse analyses are reported and what variables are related to these rates. The authors find that less than one third of the survey studies include nonresponse analyses. A number of journal and article quality measures and sample characteristics were found to be related to the reporting of nonresponse analyses.},
	language = {en},
	number = {2},
	urldate = {2022-12-01},
	journal = {Organizational Research Methods},
	author = {Werner, Steve and Praxedes, Moira and Kim, Hyun-Gyu},
	month = apr,
	year = {2007},
	pages = {287--295},
	file = {Werner et al. - 2007 - The Reporting of Nonresponse Analyses in Survey Re.pdf:/Users/shannondickson/Zotero/storage/2PH5639R/Werner et al. - 2007 - The Reporting of Nonresponse Analyses in Survey Re.pdf:application/pdf},
}

@techreport{constantinGeneralMonteCarlo2021,
	type = {preprint},
	title = {A {General} {Monte} {Carlo} {Method} for {Sample} {Size} {Analysis} in the {Context} of {Network} {Models}},
	url = {https://osf.io/j5v7u},
	abstract = {We introduce a general method for sample size computations in the context of cross-sectional network models. The method takes the form of an automated Monte Carlo algorithm, designed to find an optimal sample size while iteratively concentrating the computations on the sample sizes that seem most relevant. The method requires three inputs: 1) a hypothesized network structure or desired characteristics of that structure, 2) an estimation performance measure and its corresponding target value (e.g., a sensitivity of 0.6), and 3) a statistic and its corresponding target value that determines how the target value for the performance measure be reached (e.g., reaching a sensitivity of 0.6 with a probability of 0.8). The method consists of a Monte Carlo simulation step for computing the performance measure and the statistic for several sample sizes selected from an initial candidate sample size range, a curve-fitting step for interpolating the statistic across the entire candidate range, and a stratified bootstrapping step to quantify the uncertainty around the recommendation provided. We evaluated the performance of the method for the Gaussian Graphical Model, but it can easily extend to other models. The method displayed good performance, providing sample size recommendations that were, on average, within three observations of a benchmark sample size, with the highest standard deviation of 25.87 observations. The method discussed is implemented in the form of an R package called powerly, available on GitHub and CRAN.},
	language = {en},
	urldate = {2022-12-01},
	institution = {PsyArXiv},
	author = {Constantin, Mihai Alexandru and Schuurman, Noémi Katalin and Vermunt, Jeroen},
	month = sep,
	year = {2021},
	doi = {10.31234/osf.io/j5v7u},
	file = {Constantin et al. - 2021 - A General Monte Carlo Method for Sample Size Analy.pdf:/Users/shannondickson/Zotero/storage/IDYYL72L/Constantin et al. - 2021 - A General Monte Carlo Method for Sample Size Analy.pdf:application/pdf},
}

@article{tourangeauPresidentialAddress2017,
	title = {Presidential {Address}},
	volume = {81},
	issn = {0033-362X, 1537-5331},
	url = {http://academic.oup.com/poq/article/81/3/803/4085250/Presidential-AddressParadoxes-of-Nonresponse},
	doi = {10.1093/poq/nfx031},
	language = {en},
	number = {3},
	urldate = {2022-12-01},
	journal = {Public Opinion Quarterly},
	author = {Tourangeau, Roger},
	year = {2017},
	pages = {803--814},
	file = {Tourangeau - 2017 - Presidential Address.pdf:/Users/shannondickson/Zotero/storage/VMXIBYYF/Tourangeau - 2017 - Presidential Address.pdf:application/pdf},
}

@article{haslbeckMgmEstimatingTimeVarying2020a,
	title = {\textbf{mgm} : {Estimating} {Time}-{Varying} {Mixed} {Graphical} {Models} in {High}-{Dimensional} {Data}},
	volume = {93},
	issn = {1548-7660},
	shorttitle = {\textbf{mgm}},
	url = {http://www.jstatsoft.org/v93/i08/},
	doi = {10.18637/jss.v093.i08},
	abstract = {We present the R package mgm for the estimation of k-order mixed graphical models (MGMs) and mixed vector autoregressive (mVAR) models in high-dimensional data. These are a useful extensions of graphical models for only one variable type, since data sets consisting of mixed types of variables (continuous, count, categorical) are ubiquitous. In addition, we allow to relax the stationarity assumption of both models by introducing time-varying versions of MGMs and mVAR models based on a kernel weighting approach. Time-varying models oﬀer a rich description of temporally evolving systems and allow to identify external inﬂuences on the model structure such as the impact of interventions. We provide the background of all implemented methods and provide fully reproducible examples that illustrate how to use the package.},
	language = {en},
	number = {8},
	urldate = {2022-12-01},
	journal = {Journal of Statistical Software},
	author = {Haslbeck, Jonas M. B. and Waldorp, Lourens J.},
	year = {2020},
	file = {Haslbeck and Waldorp - 2020 - mgm  Estimating Time-Varying Mixed Graphic.pdf:/Users/shannondickson/Zotero/storage/GJ4MBNNP/Haslbeck and Waldorp - 2020 - mgm  Estimating Time-Varying Mixed Graphic.pdf:application/pdf},
}

@misc{haslbeckStructureEstimationMixed2015,
	title = {Structure estimation for mixed graphical models in high-dimensional data},
	url = {http://arxiv.org/abs/1510.05677},
	abstract = {Undirected graphical models are a key component in the analysis of complex observational data in a large variety of disciplines. In many of these applications one is interested in estimating the undirected graphical model underlying a distribution over variables with different domains. Despite the pervasive need for such an estimation method, to date there is no such method that models all variables on their proper domain. We close this methodological gap by combining a new class of mixed graphical models with a structure estimation approach based on generalized covariance matrices. We report the performance of our methods using simulations, illustrate the method with a dataset on Autism Spectrum Disorder (ASD) and provide an implementation as an R-package.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Haslbeck, Jonas M. B. and Waldorp, Lourens J.},
	month = oct,
	year = {2015},
	note = {arXiv:1510.05677 [math, stat]},
	keywords = {Statistics - Applications, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/shannondickson/Zotero/storage/INYKH4EP/Haslbeck and Waldorp - 2015 - Structure estimation for mixed graphical models in.pdf:application/pdf},
}

@article{ryanChallengeGeneratingCausal2022,
	title = {The {Challenge} of {Generating} {Causal} {Hypotheses} {Using} {Network} {Models}},
	volume = {29},
	issn = {1070-5511, 1532-8007},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2022.2056039},
	doi = {10.1080/10705511.2022.2056039},
	abstract = {Statistical network models based on Pairwise Markov Random Fields (PMRFs) are popular tools for analyzing multivariate psychological data, in large part due to their perceived role in generating insights into causal relationships: a practice known as causal discovery in the causal modeling literature. However, since network models are not presented as causal discovery tools, the role they play in generating causal insights is poorly understood among empirical researchers. In this paper, we provide a treatment of how PMRFs such as the Gaussian Graphical Model (GGM) work as causal discovery tools, using Directed Acyclic Graphs (DAGs) and Structural Equation Models (SEMs) as causal models. We describe the key assumptions needed for causal discovery and show the equivalence class of causal models that networks identify from data. We clarify four common misconceptions found in the empirical literature relating to networks as causal skeletons; chains of relationships; collider bias; and cyclic causal models.},
	language = {en},
	number = {6},
	urldate = {2022-12-21},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Ryan, Oisín and Bringmann, Laura F. and Schuurman, Noémi K.},
	month = nov,
	year = {2022},
	pages = {953--970},
	file = {Ryan et al. - 2022 - The Challenge of Generating Causal Hypotheses Usin.pdf:/Users/shannondickson/Zotero/storage/CKUM5P3M/Ryan et al. - 2022 - The Challenge of Generating Causal Hypotheses Usin.pdf:application/pdf},
}

@article{daikelerWebOtherSurvey2020,
	title = {Web {Versus} {Other} {Survey} {Modes}: {An} {Updated} and {Extended} {Meta}-{Analysis} {Comparing} {Response} {Rates}},
	volume = {8},
	issn = {2325-0984},
	shorttitle = {Web {Versus} {Other} {Survey} {Modes}},
	url = {https://doi.org/10.1093/jssam/smz008},
	doi = {10.1093/jssam/smz008},
	abstract = {Do web surveys still yield lower response rates compared with other survey modes? To answer this question, we replicated and extended a meta-analysis done in 2008 which found that, based on 45 experimental comparisons, web surveys had an 11 percentage points lower response rate compared with other survey modes. Fundamental changes in internet accessibility and use since the publication of the original meta-analysis would suggest that people’s propensity to participate in web surveys has changed considerably in the meantime. However, in our replication and extension study, which comprised 114 experimental comparisons between web and other survey modes, we found almost no change: web surveys still yielded lower response rates than other modes (a difference of 12 percentage points in response rates). Furthermore, we found that prenotifications, the sample recruitment strategy, the survey’s solicitation mode, the type of target population, the number of contact attempts, and the country in which the survey was conducted moderated the magnitude of the response rate differences. These findings have substantial implications for web survey methodology and operations.},
	number = {3},
	urldate = {2023-01-15},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Daikeler, Jessica and Bošnjak, Michael and Lozar Manfreda, Katja},
	month = jun,
	year = {2020},
	pages = {513--539},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/SXY6EY7G/Daikeler et al. - 2020 - Web Versus Other Survey Modes An Updated and Exte.pdf:application/pdf},
}

@article{boseNONRESPONSEBIASANALYSES,
	title = {{NONRESPONSE} {BIAS} {ANALYSES} {AT} {THE} {NATIONAL} {CENTER} {FOR} {EDUCATION} {STATISTICS}},
	abstract = {In surveys with low response rates, nonresponse bias can be a major concern. While it is not always possible to measure the actual bias due to nonresponse, there are different approaches that help identify potential sources of nonresponse bias. In the National Center for Education Statistics (NCES), surveys with a response rate lower than 70 percent must conduct a nonresponse bias analysis. This paper discusses the different approaches to nonresponse bias analyses using examples from NCES.},
	language = {en},
	author = {Bose, Jonaki},
	file = {Bose - NONRESPONSE BIAS ANALYSES AT THE NATIONAL CENTER F.pdf:/Users/shannondickson/Zotero/storage/5WVD3G56/Bose - NONRESPONSE BIAS ANALYSES AT THE NATIONAL CENTER F.pdf:application/pdf},
}

@article{brickExplainingRisingNonresponse2013,
	title = {Explaining {Rising} {Nonresponse} {Rates} in {Cross}-{Sectional} {Surveys}},
	volume = {645},
	issn = {0002-7162},
	url = {https://doi.org/10.1177/0002716212456834},
	doi = {10.1177/0002716212456834},
	abstract = {This review of nonresponse in cross-sectional household surveys in the United States shows trends in nonresponse rates, the main reasons for nonresponse, and changes in the components of nonresponse. It shows that nonresponse is increasing but that existing methods for modeling response mechanisms do not adequately explain these changes.},
	language = {en},
	number = {1},
	urldate = {2023-01-15},
	journal = {The ANNALS of the American Academy of Political and Social Science},
	author = {Brick, J. Michael and Williams, Douglas},
	month = jan,
	year = {2013},
	note = {Publisher: SAGE Publications Inc},
	pages = {36--59},
	file = {SAGE PDF Full Text:/Users/shannondickson/Zotero/storage/J34DGYMC/Brick and Williams - 2013 - Explaining Rising Nonresponse Rates in Cross-Secti.pdf:application/pdf},
}

@article{teitlerCostsBenefitsImproving2003,
	title = {Costs and {Benefits} of {Improving} {Response} {Rates} for a {Hard}-to-{Reach} {Population}},
	volume = {67},
	issn = {0033-362X},
	url = {https://www.jstor.org/stable/3521668},
	number = {1},
	urldate = {2023-01-16},
	journal = {The Public Opinion Quarterly},
	author = {Teitler, Julien O. and Reichman, Nancy E. and Sprachman, Susan},
	year = {2003},
	note = {Publisher: [Oxford University Press, American Association for Public Opinion Research]},
	pages = {126--138},
	file = {JSTOR Full Text PDF:/Users/shannondickson/Zotero/storage/39GYHEZ8/Teitler et al. - 2003 - Costs and Benefits of Improving Response Rates for.pdf:application/pdf},
}

@article{kreuterFacingNonresponseChallenge2013,
	title = {Facing the {Nonresponse} {Challenge}},
	volume = {645},
	issn = {0002-7162},
	url = {https://doi.org/10.1177/0002716212456815},
	doi = {10.1177/0002716212456815},
	abstract = {This article provides a brief overview of key trends in the survey research to address the nonresponse challenge. Noteworthy are efforts to develop new quality measures and to combine several data sources to enhance either the data collection process or the quality of resulting survey estimates. Mixtures of survey data collection modes and less burdensome survey designs are additional steps taken by survey researchers to address nonresponse.},
	language = {en},
	number = {1},
	urldate = {2023-01-16},
	journal = {The ANNALS of the American Academy of Political and Social Science},
	author = {Kreuter, Frauke},
	month = jan,
	year = {2013},
	note = {Publisher: SAGE Publications Inc},
	pages = {23--35},
	file = {SAGE PDF Full Text:/Users/shannondickson/Zotero/storage/LMRSTXRB/Kreuter - 2013 - Facing the Nonresponse Challenge.pdf:application/pdf},
}

@article{mcgonagleEffectsIncentiveBoost2020,
	title = {The {Effects} of an {Incentive} {Boost} on {Response} {Rates}, {Fieldwork} {Effort}, and {Costs} across {Two} {Waves} of a {Panel} {Study}},
	volume = {14},
	issn = {1864-6956},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8136583/},
	doi = {10.12758/mda.2020.04},
	abstract = {This paper describes the association between an incentive boost and data collection outcomes across two waves of a long-running panel study. In a recent wave, with the aim of achieving response rate goals, all remaining sample members were offered a substantial incentive increase in the final weeks of data collection, despite uncertainty about potential effects on fieldwork outcomes in the following wave. The analyses examine response rates and the average number of interviewer attempts to complete the interview in the waves during and after the incentive boost, and provide an estimate of the cost of the incentives and fieldwork in the waves during and following the boost. The findings provide suggestive evidence that the use of variable incentive strategies from one wave to the next in the context of an ongoing panel study may be an effective strategy to reduce nonresponse and may yield enduring positive effects on subsequent data collection outcomes.},
	number = {2},
	urldate = {2023-01-17},
	journal = {Methoden, daten, analysen},
	author = {McGonagle, Katherine A.},
	year = {2020},
	pmid = {34025812},
	pmcid = {PMC8136583},
	pages = {241--250},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/CS4XX5YI/McGonagle - 2020 - The Effects of an Incentive Boost on Response Rate.pdf:application/pdf},
}

@article{andreskiResponseRatesNational2012,
	title = {Response {Rates} in {National} {Panel} {Surveys} - {Robert} {F}. {Schoeni}, {Frank} {Stafford}, {Katherine} {A}. {Mcgonagle}, {Patricia} {Andreski}, 2013},
	url = {https://journals.sagepub.com/doi/full/10.1177/0002716212456363},
	abstract = {It has been well documented that response rates to cross-sectional surveys have declined over the past few decades. It is less clear whether response rates to l...},
	language = {en},
	urldate = {2023-01-17},
	journal = {The ANNALS of the American Academy of Political and Social Science},
	author = {Andreski, Frank Stafford, Katherine A. Mcgonagle, Patricia, Robert F. Schoeni},
	month = nov,
	year = {2012},
}

@article{sedgewickLearningMixedGraphical2016,
	title = {Learning mixed graphical models with separate sparsity parameters and stability-based model selection},
	volume = {17},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-016-1039-0},
	doi = {10.1186/s12859-016-1039-0},
	abstract = {Mixed graphical models (MGMs) are graphical models learned over a combination of continuous and discrete variables. Mixed variable types are common in biomedical datasets. MGMs consist of a parameterized joint probability density, which implies a network structure over these heterogeneous variables. The network structure reveals direct associations between the variables and the joint probability density allows one to ask arbitrary probabilistic questions on the data. This information can be used for feature selection, classification and other important tasks.},
	number = {5},
	urldate = {2023-01-17},
	journal = {BMC Bioinformatics},
	author = {Sedgewick, Andrew J. and Shi, Ivy and Donovan, Rory M. and Benos, Panayiotis V.},
	month = jun,
	year = {2016},
	keywords = {Chronic Obstructive Pulmonary Disease, Edge Type, Idiopathic Pulmonary Fibrosis, Lasso, Model Selection Method},
	pages = {S175},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/B6LAZ8FJ/Sedgewick et al. - 2016 - Learning mixed graphical models with separate spar.pdf:application/pdf},
}

@article{vandeschootOpenSourceMachine2021,
	title = {An open source machine learning framework for efficient and transparent systematic reviews},
	volume = {3},
	copyright = {2021 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00287-7},
	doi = {10.1038/s42256-020-00287-7},
	abstract = {To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool to accelerate the step of screening titles and abstracts. For many tasks—including but not limited to systematic reviews and meta-analyses—the scientific literature needs to be checked systematically. Scholars and practitioners currently screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that active learning can yield far more efficient reviewing than manual reviewing while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice.},
	language = {en},
	number = {2},
	urldate = {2023-01-20},
	journal = {Nature Machine Intelligence},
	author = {van de Schoot, Rens and de Bruin, Jonathan and Schram, Raoul and Zahedi, Parisa and de Boer, Jan and Weijdema, Felix and Kramer, Bianca and Huijts, Martijn and Hoogerwerf, Maarten and Ferdinands, Gerbrich and Harkema, Albert and Willemsen, Joukje and Ma, Yongchao and Fang, Qixiang and Hindriks, Sybren and Tummers, Lars and Oberski, Daniel L.},
	month = feb,
	year = {2021},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Computer science, Medical research, SARS-CoV-2},
	pages = {125--133},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/XUAQNIGV/van de Schoot et al. - 2021 - An open source machine learning framework for effi.pdf:application/pdf},
}

@article{yangMixedGraphicalModels,
	title = {Mixed {Graphical} {Models} via {Exponential} {Families}},
	abstract = {Markov Random Fields, or undirected graphical models are widely used to model highdimensional multivariate data. Classical instances of these models, such as Gaussian Graphical and Ising Models, as well as recent extensions (Yang et al., 2012) to graphical models speciﬁed by univariate exponential families, assume all variables arise from the same distribution. Complex data from high-throughput genomics and social networking for example, often contain discrete, count, and continuous variables measured on the same set of samples. To model such heterogeneous data, we develop a novel class of mixed graphical models by specifying that each node-conditional distribution is a member of a possibly diﬀerent univariate exponential family. We study several instances of our model, and propose scalable M -estimators for recovering the underlying network structure. Simulations as well as an application to learning mixed genomic networks from next generation sequencing and mutation data demonstrate the versatility of our methods.},
	language = {en},
	author = {Yang, Eunho and Baker, Yulia and Ravikumar, Pradeep and Allen, Genevera I and Liu, Zhandong},
	file = {Yang et al. - Mixed Graphical Models via Exponential Families.pdf:/Users/shannondickson/Zotero/storage/PCFPUF2D/Yang et al. - Mixed Graphical Models via Exponential Families.pdf:application/pdf},
}

@inproceedings{yangMixedGraphicalModels2014,
	title = {Mixed {Graphical} {Models} via {Exponential} {Families}},
	url = {https://proceedings.mlr.press/v33/yang14a.html},
	abstract = {Markov Random Fields, or undirected graphical models are widely used to model high-dimensional multivariate data. Classical instances of these models, such as Gaussian Graphical and Ising Models, as well as recent extensions to graphical models specified by univariate exponential families, assume all variables arise from the same distribution. Complex data from high-throughput genomics and social networking for example, often contain discrete, count, and continuous variables measured on the same set of samples. To model such heterogeneous data, we develop a {\textbackslash}emphnovel class of mixed graphical models by specifying that each node-conditional distribution is a member of a possibly different univariate exponential family. We study several instances of our model, and propose scalable M-estimators for recovering the underlying network structure. Simulations as well as an application to learning mixed genomic networks from next generation sequencing and mutation data demonstrate the versatility of our methods.},
	language = {en},
	urldate = {2023-01-21},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Yang, Eunho and Baker, Yulia and Ravikumar, Pradeep and Allen, Genevera and Liu, Zhandong},
	month = apr,
	year = {2014},
	note = {ISSN: 1938-7228},
	pages = {1042--1050},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/L2IX6UM4/Yang et al. - 2014 - Mixed Graphical Models via Exponential Families.pdf:application/pdf},
}

@article{chenSelectionEstimationMixed2015,
	title = {Selection and estimation for mixed graphical models},
	volume = {102},
	issn = {0006-3444},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5018402/},
	doi = {10.1093/biomet/asu051},
	abstract = {We consider the problem of estimating the parameters in a pairwise graphical model in which the distribution of each node, conditioned on the others, may have a different exponential family form. We identify restrictions on the parameter space required for the existence of a well-defined joint density, and establish the consistency of the neighbourhood selection approach for graph reconstruction in high dimensions when the true underlying graph is sparse. Motivated by our theoretical results, we investigate the selection of edges between nodes whose conditional distributions take different parametric forms, and show that efficiency can be gained if edge estimates obtained from the regressions of particular nodes are used to reconstruct the graph. These results are illustrated with examples of Gaussian, Bernoulli, Poisson and exponential distributions. Our theoretical findings are corroborated by evidence from simulation studies.},
	number = {1},
	urldate = {2023-01-21},
	journal = {Biometrika},
	author = {Chen, Shizhe and Witten, Daniela M. and shojaie, Ali},
	month = mar,
	year = {2015},
	pmid = {27625437},
	pmcid = {PMC5018402},
	pages = {47--64},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/M4QUSGL3/Chen et al. - 2015 - Selection and estimation for mixed graphical model.pdf:application/pdf},
}

@article{epskampQgraphNetworkVisualizations2012,
	title = {\textbf{qgraph} : {Network} {Visualizations} of {Relationships} in {Psychometric} {Data}},
	volume = {48},
	issn = {1548-7660},
	shorttitle = {\textbf{qgraph}},
	url = {http://www.jstatsoft.org/v48/i04/},
	doi = {10.18637/jss.v048.i04},
	abstract = {We present the qgraph package for R, which provides an interface to visualize data through network modeling techniques. For instance, a correlation matrix can be represented as a network in which each variable is a node and each correlation an edge; by varying the width of the edges according to the magnitude of the correlation, the structure of the correlation matrix can be visualized. A wide variety of matrices that are used in statistics can be represented in this fashion, for example matrices that contain (implied) covariances, factor loadings, regression parameters and p values. qgraph can also be used as a psychometric tool, as it performs exploratory and conﬁrmatory factor analysis, using sem and lavaan; the output of these packages is automatically visualized in qgraph, which may aid the interpretation of results. In this article, we introduce qgraph by applying the package functions to data from the NEO-PI-R, a widely used personality questionnaire.},
	language = {en},
	number = {4},
	urldate = {2023-01-22},
	journal = {Journal of Statistical Software},
	author = {Epskamp, Sacha and Cramer, Angélique O. J. and Waldorp, Lourens J. and Schmittmann, Verena D. and Borsboom, Denny},
	year = {2012},
	file = {Epskamp et al. - 2012 - qgraph  Network Visualizations of Relation.pdf:/Users/shannondickson/Zotero/storage/U53VH3QH/Epskamp et al. - 2012 - qgraph  Network Visualizations of Relation.pdf:application/pdf},
}

@article{bethlehemSelectionBiasWeb2010,
	title = {Selection {Bias} in {Web} {Surveys}},
	volume = {78},
	issn = {1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2010.00112.x},
	doi = {10.1111/j.1751-5823.2010.00112.x},
	abstract = {At first sight, web surveys seem to be an interesting and attractive means of data collection. They provide simple, cheap, and fast access to a large group of potential respondents. However, web surveys are not without methodological problems. Specific groups in the populations are under-represented because they have less access to Internet. Furthermore, recruitment of respondents is often based on self-selection. Both under-coverage and self-selection may lead to biased estimates. This paper describes these methodological problems. It also explores the effect of various correction techniques (adjustment weighting and use of reference surveys). This all leads to the question whether properly design web surveys can be used for data collection. The paper attempts to answer this question. It concludes that under-coverage problems may solve itself in the future, but that self-selection leads to unreliable survey outcomes.},
	language = {en},
	number = {2},
	urldate = {2023-01-22},
	journal = {International Statistical Review},
	author = {Bethlehem, Jelke},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1751-5823.2010.00112.x},
	keywords = {Adjustment weighting, bias, online survey, reference survey, self-selection, under-coverage, web survey},
	pages = {161--188},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/SEYGEGMN/Bethlehem - 2010 - Selection Bias in Web Surveys.pdf:application/pdf},
}

@article{curtinEffectsResponseRate2000,
	title = {The {Effects} of {Response} {Rate} {Changes} on the {Index} of {Consumer} {Sentiment}},
	volume = {64},
	issn = {0033362X},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1086/318638},
	doi = {10.1086/318638},
	abstract = {From 1979 to 1996, the Survey of Consumer Attitudes response rate remained roughly 70 percent. But number of calls to complete an interview and proportion of interviews requiring refusal conversion doubled. Using call-record histories, we explore what the consequences of lower response rates would have been if these additional efforts had not been undertaken. Both number of calls and initially cooperating (vs. initially refusing) are related to the Index of Consumer Sentiment (ICS), but only number of calls survives a control for demographic characteristics. We assess the impact of excluding respondents who required refusal conversion (which reduces the response rate 5–10 percentage points), respondents who required more than ﬁve calls to complete the interview (reducing the response rate about 25 percentage points), and those who required more than two calls (a reduction of about 50 percentage points). We found no effect of excluding any of these respondent groups on cross-sectional estimates of the ICS using monthly samples of hundreds of cases. For yearly estimates, based on thousands of cases, the exclusion of respondents who required more calls (though not of initial refusers) had an effect, but a very small one. One of the exclusions generally affected estimates of change over time in the ICS, irrespective of sample size.},
	language = {en},
	number = {4},
	urldate = {2023-01-23},
	journal = {Public Opinion Quarterly},
	author = {Curtin, Richard and Presser, Stanley and Singer, Eleanor},
	year = {2000},
	pages = {413--428},
}

@article{keeterConsequencesReducingNonresponse2000,
	title = {Consequences of {Reducing} {Nonresponse} in a {National} {Telephone} {Survey}},
	volume = {64},
	issn = {0033-362X},
	url = {https://www.jstor.org/stable/3078812},
	abstract = {Critics of public opinion polls often claim that methodological shortcuts taken to collect timely data produce biased results. This study compares two random digit dial national telephone surveys that used identical questionnaires but very different levels of effort: a "Standard" survey conducted over a 5-day period that used a sample of adults who were home when the interviewer called, and a "Rigorous" survey conducted over an 8-week period that used random selection from among all adult household members. Response rates, computed according to AAPOR guidelines, were 60.6 percent for the Rigorous and 36.0 percent for the Standard study. Nonetheless, the two surveys produced similar results. Across 91 comparisons, no difference exceeded 9 percentage points, and the average difference was about 2 percentage points. Most of the statistically significant differences were among demographic items. Very few significant differences were found on attention to media and engagement in politics, social trust and connectedness, and most social and political attitudes, including even those toward surveys.},
	number = {2},
	urldate = {2023-01-23},
	journal = {The Public Opinion Quarterly},
	author = {Keeter, Scott and Miller, Carolyn and Kohut, Andrew and Groves, Robert M. and Presser, Stanley},
	year = {2000},
	note = {Publisher: [Oxford University Press, American Association for Public Opinion Research]},
	pages = {125--148},
	file = {3097-should-high-response-rates-really-be-a-primary-objective.pdf:/Users/shannondickson/Zotero/storage/CLQVESS9/3097-should-high-response-rates-really-be-a-primary-objective.pdf:application/pdf;JSTOR Full Text PDF:/Users/shannondickson/Zotero/storage/3ZSHYW35/Keeter et al. - 2000 - Consequences of Reducing Nonresponse in a National.pdf:application/pdf},
}

@article{beullensShouldHighResponse2012,
	title = {Should high response rates really be a primary objective?},
	volume = {5},
	issn = {2168-0094},
	url = {https://surveypractice.scholasticahq.com/article/3097-should-high-response-rates-really-be-a-primary-objective},
	doi = {10.29115/SP-2012-0019},
	language = {en},
	number = {3},
	urldate = {2023-01-23},
	journal = {Survey Practice},
	author = {Beullens, Koen and Loosveldt, Geert},
	month = oct,
	year = {2012},
	pages = {1--5},
	file = {Beullens and Loosveldt - 2012 - Should high response rates really be a primary obj.pdf:/Users/shannondickson/Zotero/storage/NYL96Z4Z/Beullens and Loosveldt - 2012 - Should high response rates really be a primary obj.pdf:application/pdf},
}

@article{cornesseReviewConceptualApproaches2020,
	title = {A {Review} of {Conceptual} {Approaches} and {Empirical} {Evidence} on {Probability} and {Nonprobability} {Sample} {Survey} {Research}},
	volume = {8},
	issn = {2325-0984},
	url = {https://doi.org/10.1093/jssam/smz041},
	doi = {10.1093/jssam/smz041},
	abstract = {There is an ongoing debate in the survey research literature about whether and when probability and nonprobability sample surveys produce accurate estimates of a larger population. Statistical theory provides a justification for confidence in probability sampling as a function of the survey design, whereas inferences based on nonprobability sampling are entirely dependent on models for validity. This article reviews the current debate about probability and nonprobability sample surveys. We describe the conditions under which nonprobability sample surveys may provide accurate results in theory and discuss empirical evidence on which types of samples produce the highest accuracy in practice. From these theoretical and empirical considerations, we derive best-practice recommendations and outline paths for future research.},
	number = {1},
	urldate = {2023-01-30},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Cornesse, Carina and Blom, Annelies G and Dutwin, David and Krosnick, Jon A and De Leeuw, Edith D and Legleye, Stéphane and Pasek, Josh and Pennay, Darren and Phillips, Benjamin and Sakshaug, Joseph W and Struminskaya, Bella and Wenz, Alexander},
	month = feb,
	year = {2020},
	pages = {4--36},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/5XSVW587/Cornesse et al. - 2020 - A Review of Conceptual Approaches and Empirical Ev.pdf:application/pdf},
}

@article{epskampGaussianGraphicalModel2018,
	title = {The {Gaussian} {Graphical} {Model} in {Cross}-{Sectional} and {Time}-{Series} {Data}},
	volume = {53},
	issn = {0027-3171},
	url = {https://doi.org/10.1080/00273171.2018.1454823},
	doi = {10.1080/00273171.2018.1454823},
	abstract = {We discuss the Gaussian graphical model (GGM; an undirected network of partial correlation coefficients) and detail its utility as an exploratory data analysis tool. The GGM shows which variables predict one-another, allows for sparse modeling of covariance structures, and may highlight potential causal relationships between observed variables. We describe the utility in three kinds of psychological data sets: data sets in which consecutive cases are assumed independent (e.g., cross-sectional data), temporally ordered data sets (e.g., n = 1 time series), and a mixture of the 2 (e.g., n {\textgreater} 1 time series). In time-series analysis, the GGM can be used to model the residual structure of a vector-autoregression analysis (VAR), also termed graphical VAR. Two network models can then be obtained: a temporal network and a contemporaneous network. When analyzing data from multiple subjects, a GGM can also be formed on the covariance structure of stationary means—the between-subjects network. We discuss the interpretation of these models and propose estimation methods to obtain these networks, which we implement in the R packages graphicalVAR and mlVAR. The methods are showcased in two empirical examples, and simulation studies on these methods are included in the supplementary materials.},
	number = {4},
	urldate = {2023-01-30},
	journal = {Multivariate Behavioral Research},
	author = {Epskamp, Sacha and Waldorp, Lourens J. and Mõttus, René and Borsboom, Denny},
	month = jul,
	year = {2018},
	pmid = {29658809},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171.2018.1454823},
	keywords = {exploratory-data analysis, multilevel modeling, multivariate analysis, network modeling, Time-series analysis},
	pages = {453--480},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/RBCLMYJZ/Epskamp et al. - 2018 - The Gaussian Graphical Model in Cross-Sectional an.pdf:application/pdf},
}

@article{grovesNonresponseRatesNonresponse2006,
	title = {Nonresponse {Rates} and {Nonresponse} {Bias} in {Household} {Surveys}},
	volume = {70},
	issn = {0033-362X},
	url = {https://www.jstor.org/stable/4124220},
	abstract = {Many surveys of the U.S. household population are experiencing higher refusal rates. Nonresponse can, but need not, induce nonresponse bias in survey estimates. Recent empirical findings illustrate cases when the linkage between nonresponse rates and nonresponse biases is absent. Despite this, professional standards continue to urge high response rates. Statistical expressions of nonresponse bias can be translated into causal models to guide hypotheses about when nonresponse causes bias. Alternative designs to measure nonresponse bias exist, providing different but incomplete information about the nature of the bias. A synthesis of research studies estimating nonresponse bias shows the bias often present. A logical question at this moment in history is what advantage probability sample surveys have if they suffer from high nonresponse rates. Since postsurvey adjustment for nonresponse requires auxiliary variables, the answer depends on the nature of the design and the quality of the auxiliary variables.},
	number = {5},
	urldate = {2023-01-31},
	journal = {The Public Opinion Quarterly},
	author = {Groves, Robert M.},
	year = {2006},
	note = {Publisher: [Oxford University Press, American Association for Public Opinion Research]},
	pages = {646--675},
	file = {JSTOR Full Text PDF:/Users/shannondickson/Zotero/storage/WDDD7PQ4/Groves - 2006 - Nonresponse Rates and Nonresponse Bias in Househol.pdf:application/pdf},
}

@article{epskampTutorialRegularizedPartial2018a,
	title = {A tutorial on regularized partial correlation networks},
	volume = {23},
	issn = {1939-1463},
	doi = {10.1037/met0000167},
	abstract = {Recent years have seen an emergence of network modeling applied to moods, attitudes, and problems in the realm of psychology. In this framework, psychological variables are understood to directly affect each other rather than being caused by an unobserved latent entity. In this tutorial, we introduce the reader to estimating the most popular network model for psychological data: the partial correlation network. We describe how regularization techniques can be used to efficiently estimate a parsimonious and interpretable network structure in psychological data. We show how to perform these analyses in R and demonstrate the method in an empirical example on posttraumatic stress disorder data. In addition, we discuss the effect of the hyperparameter that needs to be manually set by the researcher, how to handle non-normal data, how to determine the required sample size for a network analysis, and provide a checklist with potential solutions for problems that can arise when estimating regularized partial correlation networks. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
	journal = {Psychological Methods},
	author = {Epskamp, Sacha and Fried, Eiko I.},
	year = {2018},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Methodology, Models, Neural Networks, Posttraumatic Stress Disorder, Statistical Analysis},
	pages = {617--634},
	file = {Full Text:/Users/shannondickson/Zotero/storage/2YWJR4EU/Epskamp and Fried - 2018 - A tutorial on regularized partial correlation netw.pdf:application/pdf},
}

@article{epskampEstimatingPsychologicalNetworks2018,
	title = {Estimating psychological networks and their accuracy: {A} tutorial paper},
	volume = {50},
	issn = {1554-3528},
	shorttitle = {Estimating psychological networks and their accuracy},
	url = {https://doi.org/10.3758/s13428-017-0862-1},
	doi = {10.3758/s13428-017-0862-1},
	abstract = {The usage of psychological networks that conceptualize behavior as a complex interplay of psychological and other components has gained increasing popularity in various research fields. While prior publications have tackled the topics of estimating and interpreting such networks, little work has been conducted to check how accurate (i.e., prone to sampling variation) networks are estimated, and how stable (i.e., interpretation remains similar with less observations) inferences from the network structure (such as centrality indices) are. In this tutorial paper, we aim to introduce the reader to this field and tackle the problem of accuracy under sampling variation. We first introduce the current state-of-the-art of network estimation. Second, we provide a rationale why researchers should investigate the accuracy of psychological networks. Third, we describe how bootstrap routines can be used to (A) assess the accuracy of estimated network connections, (B) investigate the stability of centrality indices, and (C) test whether network connections and centrality estimates for different variables differ from each other. We introduce two novel statistical methods: for (B) the correlation stability coefficient, and for (C) the bootstrapped difference test for edge-weights and centrality indices. We conducted and present simulation studies to assess the performance of both methods. Finally, we developed the free R-package bootnet that allows for estimating psychological networks in a generalized framework in addition to the proposed bootstrap methods. We showcase bootnet in a tutorial, accompanied by R syntax, in which we analyze a dataset of 359 women with posttraumatic stress disorder available online.},
	language = {en},
	number = {1},
	urldate = {2023-01-31},
	journal = {Behavior Research Methods},
	author = {Epskamp, Sacha and Borsboom, Denny and Fried, Eiko I.},
	month = feb,
	year = {2018},
	pages = {195--212},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/GNJZBLIY/Epskamp et al. - 2018 - Estimating psychological networks and their accura.pdf:application/pdf},
}

@article{laurieStrategiesReducingNonresponse1999,
	title = {Strategies for {Reducing} {Nonresponse} in a {Longitudinal} {Panel} {Survey}},
	volume = {15},
	abstract = {This article provides an evaluation of some of the Æeldwork procedures and survey systems used on the British Household Panel Study (BHPS). The BHPS procedures for dealing with nonresponse through panel maintenance systems, tracking procedures, and refusal conversion during Æeldwork are described. The analysis uses data from the Ærst four waves of BHPS from 1991 to 1994, to examine longitudinal patterns of response and reasons for refusal. The reasons for refusal or for becoming a non-contact over the life of the panel are discussed. The process of refusal conversion is described together with conversion outcomes. Finally the effect of interviewer continuity on maintaining the co-operation of sample members is examined. The article argues that in the context of a longitudinal panel survey, having a relatively complex set of procedures in place is critically important to minimise nonresponse and maintain high response rates over time.},
	language = {en},
	number = {2},
	journal = {Journal of OfÆcial Statistics},
	author = {Laurie, Heather and Smith, Rachel and Scott, Lynne},
	year = {1999},
	pages = {269--282},
	file = {Laurie et al. - Strategies for Reducing Nonresponse in a Longitudi.pdf:/Users/shannondickson/Zotero/storage/ZWTBNJF5/Laurie et al. - Strategies for Reducing Nonresponse in a Longitudi.pdf:application/pdf},
}

@article{sakshaugReducingNonresponseData2022,
	title = {Reducing {Nonresponse} and {Data} {Linkage} {Consent} {Bias} in {Large}-{Scale} {Panel} {Surveys}},
	volume = {25},
	issn = {1558-9544},
	url = {https://www.degruyter.com/document/doi/10.1515/fhep-2021-0060/html?lang=en},
	doi = {10.1515/fhep-2021-0060},
	abstract = {Selection bias is an ongoing concern in large-scale panel surveys where the cumulative effects of unit nonresponse increase at each subsequent wave of data collection. A second source of selection bias in panel studies is the inability to link respondents to supplementary administrative records, either because respondents do not consent to link or the matching algorithm fails to locate their administrative records. Both sources of selection bias can affect the validity of conclusions drawn from these data sources. In this article, I discuss recently proposed methods of reducing both sources of selection bias in panel studies, with a special emphasis on reducing selection bias in the US Health and Retirement Study.},
	language = {en},
	number = {1-2},
	urldate = {2023-01-31},
	journal = {Forum for Health Economics and Policy},
	author = {Sakshaug, Joseph W.},
	month = dec,
	year = {2022},
	note = {Publisher: De Gruyter
Section: Forum for Health Economics \& Policy},
	keywords = {health and retirement study, post-survey adjustments, questionnaire design, selection bias},
	pages = {41--55},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/JTY9SR62/Sakshaug - 2022 - Reducing Nonresponse and Data Linkage Consent Bias.pdf:application/pdf},
}

@article{lippsAttritionHouseholdsIndividuals2009,
	title = {Attrition of {Households} and {Individuals} in {Panel} {Surveys}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=1367371},
	doi = {10.2139/ssrn.1367371},
	abstract = {Attrition is mostly caused by not contacted or refusing sample members. On one hand it is well-known that reasons to attrite due to non-contact are different from those that are due to refusal. On the other hand does non-contact most probably affect household attrition, while refusal can be effective on both households and individuals. In this article, attrition on both the household and (conditional on household participation) the individual level is analysed in three panel surveys from the Cross National Equivalent File (CNEF): the German SocioEconomic Panel (GSOEP), the British Household Panel Study (BHPS), and the Swiss Household Panel (SHP). To follow households over time we use a common rule in all three surveys. First, we find different attrition magnitudes and patterns both across the surveys and also on the household and the individual level. Second, there is more evidence for reinforced rather than compensated household level selection effects if the individual level is also taken into account.},
	language = {en},
	urldate = {2023-01-31},
	journal = {SSRN Electronic Journal},
	author = {Lipps, Oliver},
	year = {2009},
	file = {Lipps - 2009 - Attrition of Households and Individuals in Panel S.pdf:/Users/shannondickson/Zotero/storage/39UUB26G/Lipps - 2009 - Attrition of Households and Individuals in Panel S.pdf:application/pdf},
}

@misc{kernLongitudinalFrameworkPredicting2019,
	title = {A {Longitudinal} {Framework} for {Predicting} {Nonresponse} in {Panel} {Surveys}},
	url = {http://arxiv.org/abs/1909.13361},
	abstract = {Nonresponse in panel studies can lead to a substantial loss in data quality due to its potential to introduce bias and distort survey estimates. Recent work investigates the usage of machine learning to predict nonresponse in advance, such that predicted nonresponse propensities can be used to inform the data collection process. However, predicting nonresponse in panel studies requires accounting for the longitudinal data structure in terms of model building, tuning, and evaluation. This study proposes a longitudinal framework for predicting nonresponse with machine learning and multiple panel waves and illustrates its application. With respect to model building, this approach utilizes information from multiple waves by introducing features that aggregate previous (non)response patterns. Concerning model tuning and evaluation, temporal cross-validation is employed by iterating through pairs of panel waves such that the training and test sets move in time. Implementing this approach with data from a German probability-based mixed-mode panel shows that aggregating information over multiple panel waves can be used to build prediction models with competitive and robust performance over all test waves.},
	language = {en},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Kern, Christoph and Weiss, Bernd and Kolb, Jan-Philipp},
	month = nov,
	year = {2019},
	note = {arXiv:1909.13361 [cs, stat]},
	keywords = {Statistics - Applications, Statistics - Methodology, Computer Science - Machine Learning},
	file = {Kern et al. - 2019 - A Longitudinal Framework for Predicting Nonrespons.pdf:/Users/shannondickson/Zotero/storage/ZBZB4AMR/Kern et al. - 2019 - A Longitudinal Framework for Predicting Nonrespons.pdf:application/pdf},
}

@article{lippsEffectsDifferentIncentives,
	title = {Eﬀects of diﬀerent {Incentives} on {Attrition} and {Fieldwork} {Eﬀort} in {Telephone} {Household} {Panel} {Surveys}},
	abstract = {Little is known about sample behavior and ﬁeldwork eﬀects of diﬀerent incentives introduced in a household panel survey. This is especially true for telephone surveys. In a randomized experiment, the Swiss Household Panel implemented one prepaid and two promised nonmonetary incentives in the range of 10 to 15 Swiss Francs (7-10 e), plus a no incentive control group. The aim of the paper is to compare eﬀects of these incentives especially on cooperation, but also on sample selection and ﬁeldwork eﬀort, separated by the household and the subsequent individual level. We ﬁnd small positive cooperation eﬀects of the prepaid incentive on both the household and the individual level especially in larger households. Sample composition is aﬀected to a very minor extent. Finally, incentives tend to save ﬁeldwork time and partially the number of contacts needed on the individual level.},
	language = {en},
	author = {Lipps, Oliver},
	file = {Lipps - Eﬀects of diﬀerent Incentives on Attrition and Fie.pdf:/Users/shannondickson/Zotero/storage/TTD6Y2FT/Lipps - Eﬀects of diﬀerent Incentives on Attrition and Fie.pdf:application/pdf},
}

@article{schoeniResponseRatesNational2013,
	title = {Response {Rates} in {National} {Panel} {Surveys}},
	volume = {645},
	issn = {0002-7162},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3555140/},
	doi = {10.1177/0002716212456363},
	number = {1},
	urldate = {2023-01-31},
	journal = {The Annals of the American Academy of Political and Social Science},
	author = {Schoeni, Robert F. and Stafford, Frank and McGonagle, Katherine A. and Andreski, Patricia},
	month = jan,
	year = {2013},
	pmid = {23358122},
	pmcid = {PMC3555140},
	pages = {60--87},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/CMP9K4SL/Schoeni et al. - 2013 - Response Rates in National Panel Surveys.pdf:application/pdf},
}

@article{leeperWhereHaveRespondents2019,
	title = {Where {Have} the {Respondents} {Gone}? {Perhaps} {We} {Ate} {Them} {All}},
	volume = {83},
	issn = {0033-362X},
	shorttitle = {Where {Have} the {Respondents} {Gone}?},
	url = {https://doi.org/10.1093/poq/nfz010},
	doi = {10.1093/poq/nfz010},
	abstract = {Rising rates of nonresponse are one of the most-debated issues in contemporary survey research. While early survey research regularly achieved response rates close to 100 percent, contemporary telephone interviewing methods in the United States regularly obtain response rates below 10 percent, due to a mix of noncontact and refusals. Existing research has examined a number of factors that explain variation in response rates, yet almost all such work considers survey response as an isolated, independent event. This note aims to stimulate debate by suggesting that a paradigm shift in theorizing nonresponse is needed. I diagnose the problem of nonresponse not only as an individual-level, survey-specific phenomenon, but as something larger and more collective: namely, as a common pool resource (CPR) problem. Because researchers acting independently might each seek to maximize their response rate and achieve intended sample sizes, the common pool resource of human respondents can be prone to overextraction. In addition to thinking about “benefit-cost” explanations for why respondents might respond to a specific survey, considering responses as a shared resource focuses attention on cross-level theory on how the survey field might collectively govern responses from human populations. Rather than testing CPR theory directly, I instead describe why thinking of nonresponse as a CPR problem may be useful, use the United States as a case study to demonstrate the possible scale of response extraction, and leverage findings from CPR studies to suggest directions for future research into nonresponse.},
	number = {S1},
	urldate = {2023-01-31},
	journal = {Public Opinion Quarterly},
	author = {Leeper, Thomas J},
	month = jul,
	year = {2019},
	pages = {280--288},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/DJ9YWIIJ/Leeper - 2019 - Where Have the Respondents Gone Perhaps We Ate Th.pdf:application/pdf},
}

@misc{czajkaDecliningResponseRates2016,
	title = {Declining {Response} {Rates} in {Federal} {Surveys}: {Trends} and {Implications}},
	language = {en},
	publisher = {Office of the Assistant Secretary for Planning and Evaluation},
	author = {Czajka, John L and Beyler, Amy},
	year = {2016},
	file = {Czajka and Beyler - Declining Response Rates in Federal Surveys Trend.pdf:/Users/shannondickson/Zotero/storage/GEQDML6J/Czajka and Beyler - Declining Response Rates in Federal Surveys Trend.pdf:application/pdf},
}

@article{beullensResponseRatesEuropean2018,
	title = {Response {Rates} in the {European} {Social} {Survey}: {Increasing}, {Decreasing}, or a {Matter} of {Fieldwork} {Efforts}?},
	issn = {2296-4754},
	shorttitle = {Response {Rates} in the {European} {Social} {Survey}},
	url = {https://surveyinsights.org/?p=9673},
	doi = {10.13094/SMIF-2018-00003},
	abstract = {Response rates are declining increasing the risk of nonresponse error. The reasons for this decline are multiple: the rise of online surveys, mobile phones, and information requests, societal changes, greater awareness of privacy issues, etc. To combat this decline, fieldwork efforts have become increasingly intensive: widespread use of respondent incentives, advance letters, and an increased number of contact attempts. In addition, complex fieldwork strategies such as adaptive call scheduling or responsive designs have been implemented. The additional efforts to counterbalance nonresponse complicate the measurement of the increased difficulty of contacting potential respondents and convincing them to cooperate. 
To observe developments in response rates we use the first seven rounds of the European Social Survey, a biennial face-to-face survey. Despite some changes to the fieldwork efforts in some countries (choice of survey agency, available sample frame, incentives, number of contact attempts), many characteristics have been stable: effective sample size,  (contact and) survey mode, and questionnaire design. To control for the different country composition in different rounds, we use a multilevel model with countries as level 2 units and response rates in each country-year combination as level 1 units. The results show a declining trend, although only round 7 has a significant negative effect.},
	language = {en-US},
	urldate = {2023-01-31},
	journal = {Survey Methods: Insights from the Field (SMIF)},
	author = {Beullens, Koen and Loosveldt, Geert and Vandenplas, Caroline and Stoop, Ineke},
	month = apr,
	year = {2018},
}

@article{cornesseThereAssociationSurvey2018,
	title = {Is there an association between survey characteristics and representativeness? {A} meta-analysis},
	volume = {Vol 12},
	shorttitle = {Is there an association between survey characteristics and representativeness?},
	url = {https://ojs.ub.uni-konstanz.de/srm/article/view/7205},
	doi = {10.18148/SRM/2018.V12I1.7205},
	abstract = {How to achieve survey representativeness is a controversially debated issue in the field of survey methodology. Common questions include whether probability-based samples produce more representative data than nonprobability samples, whether the response rate determines the overall degree of survey representativeness, and which survey modes are effective in generating highly representative data. This meta-analysis contributes to this debate by synthesizing and analyzing the literature on two common measures of survey representativeness (R-Indicators and descriptive benchmark comparisons). Our findings indicate that probability-based samples (compared to nonprobability samples), mixed-mode surveys (compared to single-mode surveys), and other-than-Web modes (compared to Web surveys) are more representative, respectively. In addition, we find that there is a positive association between representativeness and the response rate as well as the number of auxiliary variables used in representativeness assessments. Furthermore, we identify significant gaps in the research literature that we hope might encourage further research in this area.},
	language = {en},
	urldate = {2023-01-31},
	journal = {Survey Research Methods},
	author = {Cornesse, Carina and Bosnjak, Michael},
	month = apr,
	year = {2018},
	note = {Artwork Size: 1-13 Pages
Publisher: European Survey Research Association},
	keywords = {Meta-analysis, representativeness, R-Indicator, response rate, nonprobability sampling, mixed mode, web surveys, auxiliary data},
	pages = {1--13 Pages},
	annote = {SeriesInformation
Survey Research Methods, Vol 12, No 1 (2018)},
	file = {Cornesse and Bosnjak - 2018 - Is there an association between survey characteris.pdf:/Users/shannondickson/Zotero/storage/IQX8FWEZ/Cornesse and Bosnjak - 2018 - Is there an association between survey characteris.pdf:application/pdf},
}

@article{lippsEffectsDifferentIncentives2010,
	title = {Effects of different {Incentives} on {Attrition} and {Fieldwork} {Effort} in {Telephone} {Household} {Panel} {Surveys}},
	volume = {4},
	copyright = {Copyright (c) 2015 Survey Research Methods},
	issn = {1864-3361},
	url = {https://ojs.ub.uni-konstanz.de/srm/article/view/3538},
	doi = {10.18148/srm/2010.v4i2.3538},
	abstract = {Little is known about sample behavior and fieldwork effects of different incentives introduced in a household panel survey. This is especially true for telephone surveys. In a randomized experiment, the Swiss Household Panel implemented one prepaid and two promised non-monetary incentives in the range of 10 to 15 Swiss Francs (7-10 EUR), plus a no incentive control group. The aim of the paper is to compare effects of these incentives especially on cooperation, but also on sample selection and fieldwork effort, separated by the household and the subsequent individual level. We find small positive cooperation effects of the prepaid incentive on both the household and the individual level especially in larger households. Sample composition is affected to a very minor extent. Finally, incentives tend to save fieldwork time and partially the number of contacts needed on the individual level.},
	language = {en},
	number = {2},
	urldate = {2023-01-31},
	journal = {Survey Research Methods},
	author = {Lipps, Oliver},
	month = sep,
	year = {2010},
	note = {Number: 2},
	keywords = {bias, attrition, fieldwork effort, incentive effects, nonresponse},
	pages = {81--90},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/BCIJ2MNZ/Lipps - 2010 - Effects of different Incentives on Attrition and F.pdf:application/pdf;TourangeauGrovesRedline.pdf:/Users/shannondickson/Zotero/storage/HG3EZZ9L/TourangeauGrovesRedline.pdf:application/pdf},
}

@misc{SamplingHardtoreachPopulations,
	title = {Sampling 'hard-to-reach' populations in health research: yield from a study targeting {Americans} living in {Canada} {\textbar} {BMC} {Medical} {Research} {Methodology} {\textbar} {Full} {Text}},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-8-57},
	urldate = {2023-01-31},
}

@article{southernSamplingHardtoreachPopulations2008,
	title = {Sampling 'hard-to-reach' populations in health research: yield from a study targeting {Americans} living in {Canada}},
	volume = {8},
	issn = {1471-2288},
	shorttitle = {Sampling 'hard-to-reach' populations in health research},
	url = {https://doi.org/10.1186/1471-2288-8-57},
	doi = {10.1186/1471-2288-8-57},
	abstract = {Some populations targeted in survey research can be hard to reach, either because of lack of contact information, or non-existent databases to inform sampling. Here, we present a methodological "case-report" of the yield of a multi-step survey study assessing views on health care among American emigres to Canada, a hard-to-reach population.},
	number = {1},
	urldate = {2023-01-31},
	journal = {BMC Medical Research Methodology},
	author = {Southern, Danielle A. and Lewis, Steven and Maxwell, Colleen J. and Dunn, James R. and Noseworthy, Tom W. and Corbett, Gail and Thomas, Karen and Ghali, William A.},
	month = aug,
	year = {2008},
	keywords = {Internet Protocol Address, Media Coverage, Newspaper Story, Press Release, Recruitment Goal},
	pages = {57},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/3K7E29NW/Southern et al. - 2008 - Sampling 'hard-to-reach' populations in health res.pdf:application/pdf},
}

@article{beebeTestingImpactMixed2018,
	title = {Testing the {Impact} of {Mixed}‐{Mode} {Designs} ({Mail} and {Web}) and {Multiple} {Contact} {Attempts} within {Mode} ({Mail} or {Web}) on {Clinician} {Survey} {Response}},
	volume = {53},
	issn = {0017-9124},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6056581/},
	doi = {10.1111/1475-6773.12827},
	abstract = {Objective
To compare response rate and nonresponse bias across two mixed‐mode survey designs and two single‐mode designs.

Data Sources
This experiment was embedded in a clinician survey of knowledge and attitudes regarding HPV vaccination (n = 275).

Study Design
Clinicians were randomly assigned one of two mixed‐mode (mail/web or web/mail) or single‐mode designs (mail‐only/web‐only). Differences in response rate and nonresponse bias were assessed.

Principal Findings
Using a multiple‐contact protocol increased response, and sending a web survey first provided the more rapid response. Overall, the mixed‐mode survey designs generated final response rates approximately 10 percentage points higher than their single‐mode counterparts, although only the final response differences between the mail‐only and web/mail conditions attained statistical significance (32.1 percent vs. 48 percent, respectively; p = .005). Observed differences did not result in nonresponse bias.

Conclusions
Results support mixing modes of survey administration and web‐based data collection in a multiple contact survey data collection protocol.},
	number = {Suppl Suppl 1},
	urldate = {2023-02-01},
	journal = {Health Services Research},
	author = {Beebe, Timothy J. and Jacobson, Robert M. and Jenkins, Sarah M. and Lackore, Kandace A. and Rutten, Lila J. Finney},
	month = aug,
	year = {2018},
	pmid = {29355920},
	pmcid = {PMC6056581},
	pages = {3070--3083},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/VHXNZN9C/Beebe et al. - 2018 - Testing the Impact of Mixed‐Mode Designs (Mail and.pdf:application/pdf},
}

@article{lohrTestsEvaluatingNonresponse,
	title = {Tests for evaluating nonresponse bias in surveys},
	abstract = {How do we tell whether weighting adjustments reduce nonresponse bias? If a variable is measured for everyone in the selected sample, then the design weights can be used to calculate an approximately unbiased estimate of the population mean or total for that variable. A second estimate of the population mean or total can be calculated using the survey respondents only, with weights that have been adjusted for nonresponse. If the two estimates disagree, then there is evidence that the weight adjustments may not have removed the nonresponse bias for that variable. In this paper we develop the theoretical properties of linearization and jackknife variance estimators for evaluating the bias of an estimated population mean or total by comparing estimates calculated from overlapping subsets of the same data with different sets of weights, when poststratification or inverse propensity weighting is used for the nonresponse adjustments to the weights. We provide sufficient conditions on the population, sample, and response mechanism for the variance estimators to be consistent, and demonstrate their small-sample properties through a simulation study.},
	language = {en},
	author = {Lohr, Sharon L},
	file = {Lohr - Tests for evaluating nonresponse bias in surveys.pdf:/Users/shannondickson/Zotero/storage/AJQKLPS6/Lohr - Tests for evaluating nonresponse bias in surveys.pdf:application/pdf},
}

@article{dalkaNetworkAnalysisApproach2022,
	title = {Network analysis approach to {Likert}-style surveys},
	volume = {18},
	issn = {2469-9896},
	url = {https://link.aps.org/doi/10.1103/PhysRevPhysEducRes.18.020113},
	doi = {10.1103/PhysRevPhysEducRes.18.020113},
	language = {en},
	number = {2},
	urldate = {2023-03-12},
	journal = {Physical Review Physics Education Research},
	author = {Dalka, Robert P. and Sachmpazidi, Diana and Henderson, Charles and Zwolak, Justyna P.},
	month = sep,
	year = {2022},
	pages = {020113},
	file = {Full Text:/Users/shannondickson/Zotero/storage/IKAASZHH/Dalka et al. - 2022 - Network analysis approach to Likert-style surveys.pdf:application/pdf},
}

@incollection{lugtigNonresponseAttritionProbabilitybased2014,
	title = {Nonresponse and attrition in a probability-based online panel for the general population},
	isbn = {978-1-118-76352-0},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118763520.ch6},
	abstract = {Since the mid 2000s, several research organizations have set up large probability-based online panel surveys. In these panels, individuals or households are followed over time to study change. When the respondents also represent the general population well, these panel studies can also be used to study change at the population level, or do cross-sectional surveys of the general population. To make sure that these online panels can be used to generalize study findings to the population, they start out with a probability-based sample. After recruiting people offline, those without Internet access are then given access and a computer at home. In this way, the panel surveys will be representative of people with- and without Internet access. Both initial nonresponse in the panel recruitment phase, and attrition over time threaten the external validity of probability-based online surveys however. This paper uses the Dutch LISS panel as an example to investigate the extent of nonresponse and attrition bias in the panel. To do so, we separate 9 groups of respondents, who each follow a distinct pattern of dropout. Then, within each group we look at the correlates of attrition, and compare these to the correlates of initial nonresponse bias. We show that initial nonresponse and attrition are two very different processes in a probability-based panel survey. The correlates of early attrition in the panel survey are very different from the correlates of initial nonresponse. We also find large differences between the correlates of different types of attrition, implying that attrition at various stages of the panel survey is selective. In terms of the contribution to nonresponse bias, we find initial nonresponse bias to contribute more to overall nonresponse bias than attrition. The chapter concludes with a discussion of our findings and implications for survey practice.},
	language = {en},
	urldate = {2023-03-15},
	booktitle = {Online {Panel} {Research}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Lugtig, Peter and Das, Marcel and Scherpenzeel, Annette},
	year = {2014},
	doi = {10.1002/9781118763520.ch6},
	note = {Section: 6
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118763520.ch6},
	keywords = {attrition bias, inclusion of non-Internet households, Initial nonresponse bias, probability-based online panel, survey},
	pages = {135--153},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/CKM2579N/Lugtig et al. - 2014 - Nonresponse and attrition in a probability-based o.pdf:application/pdf},
}

@article{baisCanSurveyItem2019,
	title = {Can {Survey} {Item} {Characteristics} {Relevant} to {Measurement} {Error} {Be} {Coded} {Reliably}? {A} {Case} {Study} on 11 {Dutch} {General} {Population} {Surveys}},
	volume = {48},
	issn = {0049-1241},
	shorttitle = {Can {Survey} {Item} {Characteristics} {Relevant} to {Measurement} {Error} {Be} {Coded} {Reliably}?},
	url = {https://doi.org/10.1177/0049124117729692},
	doi = {10.1177/0049124117729692},
	abstract = {Item characteristics can have a significant effect on survey data quality and may be associated with measurement error. Literature on data quality and measurement error is often inconclusive. This could be because item characteristics used for detecting measurement error are not coded unambiguously. In our study, we use a systematic coding procedure with multiple coders to investigate the extent to which the coding of item characteristics could be done reliably. For this purpose, we constructed an item characteristics scheme that is based on typologies of characteristics. High intercoder reliability indicates a clear relation between item characteristic, item content, and measurement error. Our results show that intercoder reliability is often low, especially for item characteristics that are hard to code due to subjectivity. Low intercoder reliability complicates comparisons between studies about item characteristics and measurement error. We give suggestions for coping with low intercoder reliability.},
	language = {en},
	number = {2},
	urldate = {2023-03-15},
	journal = {Sociological Methods \& Research},
	author = {Bais, Frank and Schouten, Barry and Lugtig, Peter and Toepoel, Vera and Arends-Tòth, Judit and Douhou, Salima and Kieruj, Natalia and Morren, Mattijn and Vis, Corrie},
	month = may,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	pages = {263--295},
	file = {SAGE PDF Full Text:/Users/shannondickson/Zotero/storage/4323UMR2/Bais et al. - 2019 - Can Survey Item Characteristics Relevant to Measur.pdf:application/pdf},
}

@article{eleveltDoingTimeUse2019,
	title = {Doing a {Time} {Use} {Survey} on {Smartphones} {Only}: {What} {Factors} {Predict} {Nonresponse} at {Different} {Stages} of the {Survey} {Process}?},
	shorttitle = {Doing a {Time} {Use} {Survey} on {Smartphones} {Only}},
	url = {https://ojs.ub.uni-konstanz.de/srm/article/view/7385},
	doi = {10.18148/SRM/2019.V13I2.7385},
	abstract = {The increasing use of smartphones opens up opportunities for novel ways of survey data collection, but also poses new challenges. Collecting more and different types of data means that studies can become increasingly intrusive. We risk over-asking participants, leading to nonresponse. This study documents nonresponse and nonresponse bias in a smartphone-only version of the Dutch Time Use Survey (TUS). Respondents from the Dutch LISS panel were asked to perform five sets of tasks to complete the whole TUS: 1) accept an invitation to participate in the study and install an app, 2) fill out a questionnaire on the web, 3) participate in the smartphone time use diary on their smartphone, 4) answer pop-up questions and 5) give permission to record sensor data (GPS locations and call data). Results show that 42.9\% of invited panel members responded positively to the invitation to participate in a smartphone survey. However, only 28.9\% of these willing panel members completed all stages of the study. Predictors of nonresponse are somewhat different at every stage. In addition, respondents who complete all smartphone tasks are different from groups who do not participate at some or any stage of the study. By using data collected in previous waves we show that nonresponse leads to nonresponse bias in estimates of time use. We conclude by discussing implications for using smartphone apps in survey research.},
	language = {en},
	urldate = {2023-03-15},
	journal = {Survey Research Methods},
	author = {Elevelt, Anne and Lugtig, Peter and Toepoel, Vera},
	month = apr,
	year = {2019},
	note = {Artwork Size: 195-213 Pages
Publisher: Survey Research Methods},
	pages = {195--213 Pages},
	annote = {SeriesInformation
Survey Research Methods, Vol 13 No 2 (2019)},
	file = {Elevelt et al. - 2019 - Doing a Time Use Survey on Smartphones Only What .pdf:/Users/shannondickson/Zotero/storage/3IXEFN9M/Elevelt et al. - 2019 - Doing a Time Use Survey on Smartphones Only What .pdf:application/pdf},
}

@article{lugtigShorterStatedSurvey2021,
	title = {Do shorter stated survey length and inclusion of a {QR} code in an invitation letter lead to better response rates?},
	issn = {2296-4754},
	url = {https://surveyinsights.org/?p=14216},
	doi = {10.13094/SMIF-2021-00001},
	abstract = {Invitation letters to web surveys often contain information on how long it will take to complete a web survey. When the stated length in an invitation of a survey is short, it could help to convince respondents to participate in the survey. When it is long respondents may choose not to participate, and when the actual length is longer than the stated length there may be a risk of dropout.
This paper reports on an Randomised Control Trial (RCT) conducted in a cross-sectional survey conducted in the Netherlands. The RCT included different version of the stated length of a survey and inclusion of a Quick Response (QR) code  as ways to communicate to potential respondents that the survey was short or not.
Results from the RCT show that there are no effects of the stated length on actual participation in the survey, nor do we find an effect on dropout. We do however find that inclusion of a QR code leads respondents to be more likely to use a smartphone, and find some evidence for a different composition of our respondent sample in terms of age.},
	language = {en-US},
	urldate = {2023-03-15},
	journal = {Survey Methods: Insights from the Field (SMIF)},
	author = {Lugtig, Peter and Luiten, Annemieke},
	month = feb,
	year = {2021},
}

@article{maslovskayaRepresentativenessSixWaves2022,
	title = {Representativeness in {Six} {Waves} of {Cross}-{National} {Online} {Survey} ({CRONOS}) {Panel}},
	volume = {185},
	issn = {0964-1998},
	url = {https://doi.org/10.1111/rssa.12801},
	doi = {10.1111/rssa.12801},
	abstract = {Driven by innovations in the digital space, surveys have started to move towards online data collection across the world. However, evidence is needed to demonstrate that online data collection strategy will produce reliable data which could be confidently used to inform policy decisions. This issue is even more pertinent in cross-national surveys, where the comparability of data is of the utmost importance. Due to differences in internet coverage and willingness to participate in online surveys across Europe, there is a risk that any strategy to move existing surveys online will introduce differential coverage and nonresponse bias. This paper explores representativeness across waves in the first cross-national online probability-based panel (CRONOS) by employing R-indicators that summarize the representativeness of the data across a range of variables. The analysis allows comparison of the results over time and across three countries (Estonia, Great Britain and Slovenia). The results suggest that there are differences in representativeness over time in each country and across countries. Those with lower levels of education and those who are in the oldest age category contribute more to the lack of representativeness in the three countries. However, the representativeness of CRONOS panel does not become worse when compared to the regular face-to-face interviewing conducted in the European Social Survey (ESS).},
	number = {3},
	urldate = {2023-03-15},
	journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	author = {Maslovskaya, Olga and Lugtig, Peter},
	month = jul,
	year = {2022},
	pages = {851--871},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/T6MBWRN6/Maslovskaya and Lugtig - 2022 - Representativeness in Six Waves of Cross-National .pdf:application/pdf},
}

@article{struminskayaSharingDataCollected2021,
	title = {Sharing {Data} {Collected} with {Smartphone} {Sensors}: {Willingness}, {Participation}, and {Nonparticipation} {Bias}},
	volume = {85},
	issn = {0033-362X},
	shorttitle = {Sharing {Data} {Collected} with {Smartphone} {Sensors}},
	url = {https://doi.org/10.1093/poq/nfab025},
	doi = {10.1093/poq/nfab025},
	abstract = {Smartphone sensors allow measurement of phenomena that are difficult or impossible to capture via self-report (e.g., geographical movement, physical activity). Sensors can reduce respondent burden by eliminating survey questions and improve measurement accuracy by replacing/augmenting self-reports. However, if respondents who are not willing to collect sensor data differ on critical attributes from those who are, the results can be biased. Research on the mechanisms of willingness to collect sensor data mostly comes from (nonprobability) online panels and is hypothetical (i.e., asks participants about the likelihood of participation in a sensor-based study). In a cross-sectional general population randomized experiment, we investigate how features of the request and respondent characteristics influence willingness to share (WTS) and actually sharing smartphone-sensor data. We manipulate the request to either mention or not mention (1) how participation will benefit the participant, (2) participants’ autonomy over data collection, and (3) that data will be kept confidential. We assess nonparticipation bias using the administrative records. WTS and actually sharing varies by sensor task, participants’ autonomy over data sharing, their smartphone skills, level of privacy concerns, and attitudes toward surveys. Fewer people agree to share photos and a video than geolocation, but all who agreed to share photos or a video actually did. Some nonresponse and nonparticipation biases are substantial and make each other worse, but others jointly reduce the overall bias. Our findings suggest that sensor-data-sharing decisions depend on sample members’ situation when asked to share and the nature of the sensor task rather than the sensor type.},
	number = {S1},
	urldate = {2023-03-15},
	journal = {Public Opinion Quarterly},
	author = {Struminskaya, Bella and Lugtig, Peter and Toepoel, Vera and Schouten, Barry and Giesen, Deirdre and Dolmans, Ralph},
	month = sep,
	year = {2021},
	pages = {423--462},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/MUIENRVS/Struminskaya et al. - 2021 - Sharing Data Collected with Smartphone Sensors Wi.pdf:application/pdf},
}

@article{struminskayaRiskNonresponseBias2022,
	title = {Risk of {Nonresponse} {Bias} and the {Length} of the {Field} {Period} in a {Mixed}-{Mode} {General} {Population} {Panel}},
	volume = {10},
	issn = {2325-0984},
	url = {https://doi.org/10.1093/jssam/smab011},
	doi = {10.1093/jssam/smab011},
	abstract = {Survey researchers are often confronted with the question of how long to set the length of the field period. Longer fielding time might lead to greater participation yet requires survey managers to devote more of their time to data collection efforts. With the aim of facilitating the decision about the length of the field period, we investigated whether a longer fielding time reduces the risk of nonresponse bias to judge whether field periods can be ended earlier without endangering the performance of the survey. By using data from six waves of a probability-based mixed-mode (online and mail) panel of the German population, we analyzed whether the risk of nonresponse bias decreases over the field period by investigating how day-by-day coefficients of variation develop during the field period. We then determined the optimal cut-off points for each mode after which data collection can be terminated without increasing the risk of nonresponse bias and found that the optimal cut-off points differ by mode. Our study complements prior research by shifting the perspective in the investigation of the risk of nonresponse bias to panel data as well as to mixed-mode surveys, in particular. Our proposed method of using coefficients of variation to assess whether the risk of nonresponse bias decreases significantly with each additional day of fieldwork can aid survey practitioners in finding the optimal field period for their mixed-mode surveys.},
	number = {1},
	urldate = {2023-03-15},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Struminskaya, Bella and Gummer, Tobias},
	month = feb,
	year = {2022},
	pages = {161--182},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/96PETH9J/Struminskaya and Gummer - 2022 - Risk of Nonresponse Bias and the Length of the Fie.pdf:application/pdf},
}

@article{schoutenDisentanglingModespecificSelection2013,
	title = {Disentangling mode-specific selection and measurement bias in social surveys},
	volume = {42},
	issn = {0049-089X},
	url = {https://www.sciencedirect.com/science/article/pii/S0049089X13001087},
	doi = {10.1016/j.ssresearch.2013.07.005},
	abstract = {A large-scale mixed-mode experiment linked to the Dutch Crime Victimization Survey was conducted in 2011. The experiment consisted of two waves; one wave with random assignment to one of the modes web, paper, telephone and face-to-face, and one follow-up wave to the full sample with interviewer modes only. The objective of the experiment is to estimate total mode effects and more specifically the corresponding mode effect components arising from undercoverage, nonresponse and measurement. In this paper, mode-specific selection and measurement bias are defined, and estimators for the bias terms based on the experimental design are introduced and discussed. The proposed estimators are applied to a number of key survey variables from the Labour Force Survey and the Crime Victimization Survey.},
	language = {en},
	number = {6},
	urldate = {2023-03-15},
	journal = {Social Science Research},
	author = {Schouten, Barry and van den Brakel, Jan and Buelens, Bart and van der Laan, Jan and Klausch, Thomas},
	month = nov,
	year = {2013},
	keywords = {Coverage, Measurement, Mixed-mode, Nonresponse, Survey modes},
	pages = {1555--1570},
	file = {ScienceDirect Full Text PDF:/Users/shannondickson/Zotero/storage/8MBGUG75/Schouten et al. - 2013 - Disentangling mode-specific selection and measurem.pdf:application/pdf},
}

@article{schoutenAdaptiveSurveyDesign,
	title = {Adaptive {Survey} {Design}},
	language = {en},
	author = {Schouten, Barry and Peytchev, Andy and Wagner, James},
	file = {Schouten et al. - Adaptive Survey Design.pdf:/Users/shannondickson/Zotero/storage/Y82JBYZB/Schouten et al. - Adaptive Survey Design.pdf:application/pdf},
}

@article{schoutenDoesMoreBalanced2016,
	title = {Does {More} {Balanced} {Survey} {Response} {Imply} {Less} {Non}-{Response} {Bias}?},
	volume = {179},
	issn = {0964-1998},
	url = {https://doi.org/10.1111/rssa.12152},
	doi = {10.1111/rssa.12152},
	abstract = {Recently, various indicators have been proposed as indirect measures of non-response error in surveys. They employ auxiliary variables, external to the survey, to detect non-representative or unbalanced response. A class of designs known as adaptive survey designs maximizes these indicators by applying different treatments to different subgroups. The natural question is whether the decrease in non-response bias that is caused by adaptive survey designs could also be achieved by non-response adjustment methods. We discuss this question and provide theoretical and empirical considerations, supported by a range of household and business surveys. We find evidence that more balanced response coincides with less non-response bias, even after adjustment.},
	number = {3},
	urldate = {2023-03-15},
	journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	author = {Schouten, Barry and Cobben, Fannie and Lundquist, Peter and Wagner, James},
	month = jun,
	year = {2016},
	pages = {727--748},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/SVZV7DFM/Schouten et al. - 2016 - Does More Balanced Survey Response Imply Less Non-.pdf:application/pdf},
}

@article{haslbeckHowWellNetwork2018,
	title = {How well do network models predict observations? {On} the importance of predictability in network models},
	volume = {50},
	issn = {1554-3528},
	shorttitle = {How well do network models predict observations?},
	url = {https://doi.org/10.3758/s13428-017-0910-x},
	doi = {10.3758/s13428-017-0910-x},
	abstract = {Network models are an increasingly popular way to abstract complex psychological phenomena. While studying the structure of network models has led to many important insights, little attention has been paid to how well they predict observations. This is despite the fact that predictability is crucial for judging the practical relevance of edges: for instance in clinical practice, predictability of a symptom indicates whether an intervention on that symptom through the symptom network is promising. We close this methodological gap by introducing nodewise predictability, which quantifies how well a given node can be predicted by all other nodes it is connected to in the network. In addition, we provide fully reproducible code examples of how to compute and visualize nodewise predictability both for cross-sectional and time series data.},
	language = {en},
	number = {2},
	urldate = {2023-03-15},
	journal = {Behavior Research Methods},
	author = {Haslbeck, Jonas M. B. and Waldorp, Lourens J.},
	month = apr,
	year = {2018},
	keywords = {Clinical relevance, Network analysis, Network models, Predictability},
	pages = {853--861},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/LPC6KU3U/Haslbeck and Waldorp - 2018 - How well do network models predict observations O.pdf:application/pdf},
}

@article{rybakSurveyModeNonresponse2023,
	title = {Survey mode and nonresponse bias: {A} meta-analysis based on the data from the international social survey programme waves 1996–2018 and the {European} social survey rounds 1 to 9},
	volume = {18},
	issn = {1932-6203},
	shorttitle = {Survey mode and nonresponse bias},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0283092},
	doi = {10.1371/journal.pone.0283092},
	abstract = {The constant increase in survey nonresponse and fieldwork costs are the reality of survey research. Together with other unpredictable events occurring in the world today, this increase poses a challenge: the necessity to accelerate a switch from face-to-face data collection to different modes, that have usually been considered to result in lower response rates. However, recent research has established that the simple response rate is a feeble measure of study quality. Therefore, this article aims to analyze the effect of survey characteristics, especially the survey mode, on the nonresponse bias. The bias measure used is the internal criteria first proposed by Sodeur and first applied by Kohler. The analysis is based on the survey documentation and results from the International Social Survey Programme waves 1996–2018 and the European Social Survey rounds 1 to 9. Random-effects three-level meta-regression models, based on data from countries from each inhabited continent, were created in order to estimate the impact of the survey mode or modes, sampling design, fieldwork experience, year of data collection, and response rate on the nonresponse bias indicator. Several ways of nesting observations within clusters were also proposed. The results suggest that using mail and some types of mixed-mode surveys were connected to lower nonresponse bias than using face-to-face mode surveys.},
	language = {en},
	number = {3},
	urldate = {2023-03-19},
	journal = {PLOS ONE},
	author = {Rybak, Adam},
	month = mar,
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {Australia, Computer software, Europe, Metaanalysis, New Zealand, Research reporting guidelines, Survey research, Surveys},
	pages = {e0283092},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/3U62VJ6Z/Rybak - 2023 - Survey mode and nonresponse bias A meta-analysis .pdf:application/pdf},
}

@techreport{kochAssessingESSSample2014,
	address = {Mannheim: European Social Survey, GESIS.},
	title = {Assessing {ESS} sample quality by using external and internal criteria},
	language = {en},
	author = {Koch, Achim and Halbherr, Verena and Stoop, Ineke A L and Kappelhof, Joost W S},
	year = {2014},
	file = {Koch et al. - Assessing ESS sample quality by using external and.pdf:/Users/shannondickson/Zotero/storage/42DKWQTQ/Koch et al. - Assessing ESS sample quality by using external and.pdf:application/pdf},
}

@book{bethlehemHandbookNonresponseHousehold2011,
	title = {Handbook of {Nonresponse} in {Household} {Surveys}},
	volume = {568},
	isbn = {ISBN: 978-0-470-54279-8},
	abstract = {A comprehensive, one-stop guide to identifying, reducing, and managing nonresponse in household surveys Nonresponse and its impact on the sample selection mechanism of a survey is a common problem that often arises while collecting survey data. Handbook of Nonresponse in Household Surveys is a complete guide to handling the nonresponse problem, outlining statistical methods and techniques for improving response rates and correcting response data. The authors begin with an introduction to the nonresponse problem along with basic concepts and definitions. Subsequent chapters present current theories and methods that enable survey researchers to skillfully account for nonresponse in their research. Exploring the latest developments in the field, the book also features: An introduction to the R-indicator as an indicator of survey quality Discussion of the different causes of nonresponse Extensive treatment of the selection and use of auxiliary information Best practices for re-approaching nonrespondents An overview of advanced nonresponse correction techniques Coverage of adaptive survey design Throughout the book, the treatment of each topic is presented in a uniform fashion. Following an introduction, each chapter presents the key theories and formulas underlying the topic and then illustrates common applications. Discussion concludes with a summary of the main concepts as well as a glossary of key terms and a set of exercises that allows readers to test their comprehension of the presented material. Examples using real survey data are provided, and a related website features additional data sets, which can be easily analyzed using Stata® or SPSS® software. Handbook of Nonresponse in Household Surveys is an essential reference for survey researchers working in the fields of business, economics, government, and the social sciences who gather, analyze, and draw results from data. It is also a suitable supplement for courses on survey methods at the upper-undergraduate and graduate levels.},
	language = {en-us},
	publisher = {John Wiley \& Sons},
	author = {Bethlehem, Jelke and Cobben, Fannie and Schouten, Barry},
	year = {2011},
}

@book{lohrSamplingDesignAnalysis2021,
	address = {New York},
	edition = {3},
	title = {Sampling: {Design} and {Analysis}},
	isbn = {978-0-429-29889-9},
	shorttitle = {Sampling},
	abstract = {"The level is appropriate for an upper-level undergraduate or graduate-level statistics major. Sampling: Design and Analysis (SDA) will also benefit a non-statistics major with a desire to understand the concepts of sampling from a finite population. A student with patience to delve into the rigor of survey statistics will gain even more from the content that SDA offers. The updates to SDA have potential to enrich traditional survey sampling classes at both the undergraduate and graduate levels. The new discussions of low response rates, non-probability surveys, and internet as a data collection mode hold particular value, as these statistical issues have become increasingly important in survey practice in recent years… I would eagerly adopt the new edition of SDA as the required textbook." (Emily Berg, Iowa State University)
What is the unemployment rate? What is the total area of land planted with soybeans? How many persons have antibodies to the virus causing COVID-19? Sampling: Design and Analysis, Third Edition shows you how to design and analyze surveys to answer these and other questions. This authoritative text, used as a standard reference by numerous survey organizations, teaches the principles of sampling with examples from social sciences, public opinion research, public health, business, agriculture, and ecology. Readers should be familiar with concepts from an introductory statistics class including probability and linear regression; optional sections contain statistical theory for readers familiar with mathematical statistics.
Key Features:

Has been thoroughly revised to incorporate recent research and applications.
Includes a new chapter on nonprobability samples, and more than 200 new examples and exercises have been added.
Teaches the principles of sampling with examples from social sciences, public opinion research, public health, business, agriculture, and ecology.

 SDA’s companion website contains data sets, computer code, and links to two free downloadable supplementary books (also available in paperback) that provide step-by-step guides—with code, annotated output, and helpful tips—for working through the SDA examples. Instructors can use either R or SAS® software.

SAS® Software Companion for Sampling: Design and Analysis, Third Edition by Sharon L. Lohr (2022, CRC Press)
R Companion for Sampling: Design and Analysis, Third Edition by Yan Lu and Sharon L. Lohr (2022, CRC Press)},
	publisher = {Chapman and Hall/CRC},
	author = {Lohr, Sharon L.},
	month = nov,
	year = {2021},
	doi = {10.1201/9780429298899},
}

@article{wrightEmpiricalExaminationRelationship2015,
	title = {An empirical examination of the relationship between nonresponse rate and nonresponse bias},
	volume = {31},
	issn = {18747655, 18759254},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/sji-140844},
	doi = {10.3233/sji-140844},
	abstract = {The dramatic decline in survey response rates over the past three decades raises signiﬁcant concerns about the possibility of bias in survey results. Current theory emphasizes that it is the relationship between response propensity and variables of interest that determines the extent of the bias, and that a low response rate in itself does not necessarily imply a high level of bias. This assertion is supported by a number of studies which have shown that response rate alone is a fairly poor predictor of nonresponse bias. However, most of these studies suffer from methodological features that in some way compromise their attempts to isolate the relationship between response rate and bias. This paper describes the results of a pair of studies which allow for a near-ideal examination of this relationship. The results support the conclusions of prior research, showing that even achieved samples with response rates as low as 10 percent may produce highly accurate estimates in certain cases.},
	language = {en},
	number = {2},
	urldate = {2023-03-27},
	journal = {Statistical Journal of the IAOS},
	author = {Wright, Graham},
	month = may,
	year = {2015},
	pages = {305--315},
	file = {Wright - 2015 - An empirical examination of the relationship betwe.pdf:/Users/shannondickson/Zotero/storage/DDNBPITS/Wright - 2015 - An empirical examination of the relationship betwe.pdf:application/pdf},
}

@misc{WebOtherSurvey,
	title = {Web {Versus} {Other} {Survey} {Modes}: {An} {Updated} and {Extended} {Meta}-{Analysis} {Comparing} {Response} {Rates} {\textbar} {Journal} of {Survey} {Statistics} and {Methodology} {\textbar} {Oxford} {Academic}},
	url = {https://academic.oup.com/jssam/article/8/3/513/5488703?searchresult=1#supplementary-data},
	urldate = {2023-03-29},
}

@misc{aaporBestPracticesSurvey2022,
	title = {Best {Practices} for {Survey} {Research} - {AAPOR}},
	url = {https://aapor.org/standards-and-ethics/best-practices/},
	language = {en-US},
	urldate = {2023-03-29},
	journal = {American Association for Public Opinion Research},
	author = {AAPOR},
	month = nov,
	year = {2022},
}

@article{leeExploringNonresponseBias2009,
	title = {Exploring {Nonresponse} {Bias} in a {Health} {Survey} {Using} {Neighborhood} {Characteristics}},
	volume = {99},
	issn = {0090-0036},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2741510/},
	doi = {10.2105/AJPH.2008.154161},
	abstract = {Objectives. We examined potential nonresponse bias in a large-scale, population-based, random-digit-dialed telephone survey in California and its association with the response rate., Methods. We used California Health Interview Survey (CHIS) data and US Census data and linked the two data sets at the census tract level. We compared a broad range of neighborhood characteristics of respondents and nonrespondents to CHIS. We projected individual-level nonresponse bias using the neighborhood characteristics., Results. We found little to no substantial difference in neighborhood characteristics between respondents and nonrespondents. The response propensity of the CHIS sample was similarly distributed across these characteristics. The projected nonresponse bias appeared very small., Conclusions. The response rate in CHIS did not result in significant nonresponse bias and did not substantially affect the level of data representativeness, and it is not valid to focus on response rates alone in determining the quality of survey data.},
	number = {10},
	urldate = {2023-03-29},
	journal = {American Journal of Public Health},
	author = {Lee, Sunghee and Brown, E. Richard and Grant, David and Belin, Thomas R. and Brick, J. Michael},
	month = oct,
	year = {2009},
	pmid = {19696379},
	pmcid = {PMC2741510},
	pages = {1811--1817},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/FNYNB73F/Lee et al. - 2009 - Exploring Nonresponse Bias in a Health Survey Usin.pdf:application/pdf},
}

@article{fayCausalModelsPatterns1986,
	title = {Causal {Models} for {Patterns} of {Nonresponse}},
	volume = {81},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2289224},
	doi = {10.2307/2289224},
	abstract = {The problem of missing data for categorical variables is examined from the perspective of modeling the mechanisms of nonresponse. Log-linear causal models, as formulated by Goodman, are studied for the relationship of the survey variables to response; under some conditions several such models are estimable from the observed data. For nested patterns of nonresponse, a specific causal model represents exactly the assumption of ignorable response. Most causal models, however, imply nonignorable response mechanisms and yield alternative estimates for the distribution of the survey variables.},
	number = {394},
	urldate = {2023-03-31},
	journal = {Journal of the American Statistical Association},
	author = {Fay, Robert E.},
	year = {1986},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {354--365},
	file = {JSTOR Full Text PDF:/Users/shannondickson/Zotero/storage/J9F8PQ2E/Fay - 1986 - Causal Models for Patterns of Nonresponse.pdf:application/pdf},
}

@techreport{essEuropeanSocialSurvey2016,
	title = {The {European} {Social} {Survey} ({ESS}) – a research instrument for the social sciences in {Europe}},
	url = {https://www.europeansocialsurvey.org/docs/about/ESS_blueprint.pdf},
	urldate = {2023-04-04},
	author = {ESS},
	year = {2016},
}

@article{grovesLeveragesaliencyTheorySurvey2000,
	title = {Leverage-saliency theory of survey participation: description and an illustration},
	volume = {64},
	issn = {0033-362X},
	shorttitle = {Leverage-saliency theory of survey participation},
	doi = {10.1086/317990},
	language = {eng},
	number = {3},
	journal = {Public Opinion Quarterly},
	author = {Groves, R. M. and Singer, E. and Corning, A.},
	year = {2000},
	pmid = {11114270},
	pages = {299--308},
}

@article{singerBenefitCostTheorySurvey2011,
	title = {Toward a {Benefit}-{Cost} {Theory} of {Survey} {Participation}: {Evidence}, {Further} {Tests}, and {Implications}},
	volume = {27},
	abstract = {This article uses survey respondents’ own reasons for participating or not participating in
surveys, as well as experiments carried out over many years, to propose a benefit-cost theory
of survey participation. The argument is that people choose to act, in surveys as in life, when,
in their subjective calculus, the benefits of doing so outweigh the costs. The process of
reaching a decision may be carefully reasoned or it may proceed almost instantaneously, with
the aid of heuristics. But regardless of the process, the outcome depends on a judgment that
the benefits of acting outweigh the costs of doing so – even if, objectively speaking, the actors
are badly informed and their decision leads to an undesirable outcome. The article reviews
research on confidentiality assurances and risk perceptions with reference to a benefit-cost
theory of behavior, and concludes by suggesting research to test the theory’s predictions and
by drawing testable implications for survey practice.},
	language = {en},
	number = {2},
	journal = {Journal of Ofﬁcial Statistics},
	author = {Singer, Eleanor},
	year = {2011},
	pages = {379--392},
	file = {Singer - Toward a Benefit-Cost Theory of Survey Participati.pdf:/Users/shannondickson/Zotero/storage/VJTTWZYB/Singer - Toward a Benefit-Cost Theory of Survey Participati.pdf:application/pdf},
}

@article{mercerTheoryPracticeNonprobability2017,
	title = {Theory and {Practice} in {Nonprobability} {Surveys}: {Parallels} between {Causal} {Inference} and {Survey} {Inference}},
	volume = {81},
	issn = {0033-362X},
	shorttitle = {Theory and {Practice} in {Nonprobability} {Surveys}},
	url = {https://doi.org/10.1093/poq/nfw060},
	doi = {10.1093/poq/nfw060},
	abstract = {Many in the survey research community have expressed concern at the growing popularity of nonprobability surveys. The absence of random selection prompts justified concerns about self-selection producing biased results and means that traditional, design-based estimation is inappropriate. This paper seeks to provide insight into the conditions under which nonprobability surveys can be expected to provide estimates free of selection bias. In fields such as epidemiology and economics that routinely work with observational data, researchers have identified the necessary conditions for unbiased estimation of causal effects when treatments are not assigned randomly. Similar conditions apply to survey estimates when respondents are not randomly selected. Drawing on this body of research, we propose a framework composed of three elements that determine the level of selection bias in survey estimates. In this paper, we first provide a general overview of these components and demonstrate the link between causal inference and survey inference in the probability-based setting. Second, we give simplified examples to demonstrate how each of the components can contribute to bias in survey estimates. Finally, we review current practices in the area of nonprobability data collection and estimation, and specify how these methods relate to the elements identified here.},
	number = {S1},
	urldate = {2023-04-04},
	journal = {Public Opinion Quarterly},
	author = {Mercer, Andrew W. and Kreuter, Frauke and Keeter, Scott and Stuart, Elizabeth A.},
	month = apr,
	year = {2017},
	pages = {250--271},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/39IVNNQ4/Mercer et al. - 2017 - Theory and Practice in Nonprobability Surveys Par.pdf:application/pdf},
}

@incollection{bethlehemWeightingNonresponseAdjustments2002,
	address = {New York},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Weighting {Nonresponse} {Adjustments} {Based} on {Auxiliary} {Information}},
	isbn = {978-0-471-39627-7},
	booktitle = {Survey {Nonresponse}},
	publisher = {Wiley},
	author = {Bethlehem, Jelke},
	year = {2002},
	pages = {275--288},
}

@article{biemerTotalSurveyError2010,
	title = {Total {Survey} {Error}: {Design}, {Implementation}, and {Evaluation}},
	volume = {74},
	issn = {0033-362X},
	shorttitle = {Total {Survey} {Error}},
	url = {https://doi.org/10.1093/poq/nfq058},
	doi = {10.1093/poq/nfq058},
	abstract = {The total survey error (TSE) paradigm provides a theoretical framework for optimizing surveys by maximizing data quality within budgetary constraints. In this article, the TSE paradigm is viewed as part of a much larger design strategy that seeks to optimize surveys by maximizing total survey quality; i.e., quality more broadly defined to include user-specified dimensions of quality. Survey methodology, viewed within this larger framework, alters our perspectives on the survey design, implementation, and evaluation. As an example, although a major objective of survey design is to maximize accuracy subject to costs and timeliness constraints, the survey budget must also accommodate additional objectives related to relevance, accessibility, interpretability, comparability, coherence, and completeness that are critical to a survey's “fitness for use.” The article considers how the total survey quality approach can be extended beyond survey design to include survey implementation and evaluation. In doing so, the “fitness for use” perspective is shown to influence decisions regarding how to reduce survey error during design implementation and what sources of error should be evaluated in order to assess the survey quality today and to prepare for the surveys of the future.},
	number = {5},
	urldate = {2023-04-04},
	journal = {Public Opinion Quarterly},
	author = {Biemer, Paul P.},
	month = jan,
	year = {2010},
	pages = {817--848},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/K4WWPZ32/Biemer - 2010 - Total Survey Error Design, Implementation, and Ev.pdf:application/pdf},
}

@article{petersenDiagnosingRespondingViolations2012,
	title = {Diagnosing and responding to violations in the positivity assumption},
	volume = {21},
	issn = {0962-2802},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4107929/},
	doi = {10.1177/0962280210386207},
	abstract = {The assumption of positivity or experimental treatment assignment requires that observed treatment levels vary within confounder strata. This article discusses the positivity assumption in the context of assessing model and parameter-specific identifiability of causal effects. Positivity violations occur when certain subgroups in a sample rarely or never receive some treatments of interest. The resulting sparsity in the data may increase bias with or without an increase in variance and can threaten valid inference. The parametric bootstrap is presented as a tool to assess the severity of such threats and its utility as a diagnostic is explored using simulated and real data. Several approaches for improving the identifiability of parameters in the presence of positivity violations are reviewed. Potential responses to data sparsity include restriction of the covariate adjustment set, use of an alternative projection function to define the target parameter within a marginal structural working model, restriction of the sample, and modification of the target intervention. All of these approaches can be understood as trading off proximity to the initial target of inference for identifiability; we advocate approaching this tradeoff systematically.},
	number = {1},
	urldate = {2023-04-04},
	journal = {Statistical methods in medical research},
	author = {Petersen, Maya L and Porter, Kristin E and Gruber, Susan and Wang, Yue and van der Laan, Mark J},
	month = feb,
	year = {2012},
	pmid = {21030422},
	pmcid = {PMC4107929},
	pages = {31--54},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/IGRC2DIY/Petersen et al. - 2012 - Diagnosing and responding to violations in the pos.pdf:application/pdf},
}

@article{hernanEstimatingCausalEffects2006,
	title = {Estimating causal effects from epidemiological data},
	volume = {60},
	issn = {0143-005X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2652882/},
	doi = {10.1136/jech.2004.029496},
	abstract = {In ideal randomised experiments, association is causation: association measures can be interpreted as effect measures because randomisation ensures that the exposed and the unexposed are exchangeable. On the other hand, in observational studies, association is not generally causation: association measures cannot be interpreted as effect measures because the exposed and the unexposed are not generally exchangeable. However, observational research is often the only alternative for causal inference. This article reviews a condition that permits the estimation of causal effects from observational data, and two methods—standardisation and inverse probability weighting—to estimate population causal effects under that condition. For simplicity, the main description is restricted to dichotomous variables and assumes that no random error attributable to sampling variability exists. The appendix provides a generalisation of inverse probability weighting.},
	number = {7},
	urldate = {2023-04-04},
	journal = {Journal of Epidemiology and Community Health},
	author = {Hernán, Miguel A and Robins, James M},
	month = jul,
	year = {2006},
	pmid = {16790829},
	pmcid = {PMC2652882},
	pages = {578--586},
}

@book{littleStatisticalAnalysisMissing2002,
	address = {Hoboken, NJ},
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Statistical {Analysis} with {Missing} {Data}},
	isbn = {978-1-119-01356-3},
	url = {http://dx.doi.org/10.1002/9781119013563},
	urldate = {2023-04-04},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Little, J. A., Roderik and Rubin, B., Donald},
	year = {2002},
}

@article{greenlandIdentifiabilityExchangeabilityEpidemiological1986,
	title = {Identifiability, exchangeability, and epidemiological confounding},
	volume = {15},
	issn = {0300-5771},
	doi = {10.1093/ije/15.3.413},
	abstract = {Non-identifiability of parameters is a well-recognized problem in classical statistics, and Bayesian statisticians have long recognized the importance of exchangeability assumptions in making statistical inferences. A seemingly unrelated problem in epidemiology is that of confounding: bias in estimation of the effects of an exposure on disease risk, due to inherent differences in risk between exposed and unexposed individuals. Using a simple deterministic model for exposure effects, a logical connection is drawn between the concepts of identifiability, exchangeability, and confounding. This connection allows one to view the problem of confounding as arising from problems of identifiability, and reveals the exchangeability assumptions that are implicit in confounder control methods. It also provides further justification for confounder definitions based on comparability of exposure groups, as opposed to collapsibility-based definitions.},
	language = {eng},
	number = {3},
	journal = {International Journal of Epidemiology},
	author = {Greenland, S. and Robins, J. M.},
	month = sep,
	year = {1986},
	pmid = {3771081},
	keywords = {Bayes Theorem, Epidemiologic Methods, Humans, Models, Biological, Probability, Random Allocation, Statistics as Topic, Stochastic Processes},
	pages = {413--419},
}

@incollection{garyAdjustingNonresponseSurveys2007,
	address = {Dordrecht},
	title = {Adjusting for {Nonresponse} in {Surveys}.},
	volume = {22},
	isbn = {978-1-4020-5666-6},
	url = {https://doi.org/10.1007/978-1-4020-5666-6_8},
	booktitle = {Higher {Education}: {Handbook} of {Theory} and {Research}},
	publisher = {Springer},
	author = {Gary, P. R.},
	year = {2007},
}

@article{lundstromCalibrationMethodDeriving1999,
	title = {Calibration as a method for deriving nonresponse adjusted weights.},
	volume = {15},
	language = {en},
	number = {2},
	journal = {Journal of Official Statistics},
	author = {Lundström, Sixten and Särndal, Carl-Erik},
	year = {1999},
	pages = {305--327},
	file = {Lundström and Särndal - Calibration as a method for deriving nonresponse a.pdf:/Users/shannondickson/Zotero/storage/54HGKX9Q/Lundström and Särndal - Calibration as a method for deriving nonresponse a.pdf:application/pdf},
}

@article{toepoelDealingNonresponseStrategies2017,
	title = {Dealing with nonresponse: {Strategies} to increase participation and methods for postsurvey adjustments},
	volume = {24},
	issn = {0889-8480},
	shorttitle = {Dealing with nonresponse},
	url = {https://doi.org/10.1080/08898480.2017.1299988},
	doi = {10.1080/08898480.2017.1299988},
	number = {2},
	urldate = {2023-04-04},
	journal = {Mathematical Population Studies},
	author = {Toepoel, Vera and Schonlau, Matthias},
	month = apr,
	year = {2017},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/08898480.2017.1299988},
	pages = {79--83},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/KGT7BCRE/Toepoel and Schonlau - 2017 - Dealing with nonresponse Strategies to increase p.pdf:application/pdf},
}

@phdthesis{leeSTATISTICALMETHODSREDUCING2011,
	address = {Canada},
	title = {{STATISTICAL} {METHODS} {FOR} {REDUCING} {BIAS} {IN} {WEB} {SURVEYS}},
	url = {https://summit.sfu.ca/item/11783},
	language = {en},
	school = {Simon Fraser University},
	author = {Lee, Myoung Ho},
	year = {2011},
	file = {Lee - STATISTICAL METHODS FOR REDUCING BIAS IN WEB SURVE.pdf:/Users/shannondickson/Zotero/storage/VECGLGUL/Lee - STATISTICAL METHODS FOR REDUCING BIAS IN WEB SURVE.pdf:application/pdf},
}

@article{wuResponseRatesOnline2022,
	title = {Response rates of online surveys in published research: {A} meta-analysis},
	volume = {7},
	issn = {2451-9588},
	shorttitle = {Response rates of online surveys in published research},
	url = {https://www.sciencedirect.com/science/article/pii/S2451958822000409},
	doi = {10.1016/j.chbr.2022.100206},
	abstract = {The response rates of online surveys were often examined in the literature by comparing to other modes of surveys. Questions regarding what constitutes a respectable response rate for online surveys in research remained unanswered. To fill in the knowledge gap, we conducted a comprehensive search, screened 8672 studies, and examined 1071 online survey response rates reported in education-related research. Our analyses showed the number of online surveys in published research grew steadily across the years. The average online survey response rate is 44.1\%. Our results indicate that sending an online survey to more participants did not generate a higher response rate. Instead, sending surveys to a clearly defined and refined population positively impacts the online survey response rate. In addition, pre-contacting potential participants, using other types of surveys in conjunction with online surveys, and using phone calls to remind participants about the online survey could also yield a higher response rate. The use of incentives did not show a significant impact on the response rate of online surveys. Other factors that impacted the rates included the funding status of a project, and the age and occupation of the participants. Concrete suggestions for reviewing and improving the online survey response rates are provided.},
	language = {en},
	urldate = {2023-04-04},
	journal = {Computers in Human Behavior Reports},
	author = {Wu, Meng-Jia and Zhao, Kelly and Fils-Aime, Francisca},
	month = aug,
	year = {2022},
	keywords = {Meta-analysis, Online survey, Response rate},
	pages = {100206},
	file = {ScienceDirect Full Text PDF:/Users/shannondickson/Zotero/storage/SFDAV3VJ/Wu et al. - 2022 - Response rates of online surveys in published rese.pdf:application/pdf},
}

@article{draugalisBestPracticesSurvey2008,
	title = {Best {Practices} for {Survey} {Research} {Reports}: {A} {Synopsis} for {Authors} and {Reviewers}},
	volume = {72},
	issn = {0002-9459},
	shorttitle = {Best {Practices} for {Survey} {Research} {Reports}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2254236/},
	number = {1},
	urldate = {2023-04-04},
	journal = {American Journal of Pharmaceutical Education},
	author = {Draugalis, JoLaine Reierson and Coons, Stephen Joel and Plaza, Cecilia M.},
	month = feb,
	year = {2008},
	pmid = {18322573},
	pmcid = {PMC2254236},
	pages = {11},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/K2LRXUPT/Draugalis et al. - 2008 - Best Practices for Survey Research Reports A Syno.pdf:application/pdf},
}

@article{finchamResponseRatesResponsiveness2008,
	title = {Response {Rates} and {Responsiveness} for {Surveys}, {Standards}, and the {Journal}},
	volume = {72},
	issn = {0002-9459},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2384218/},
	number = {2},
	urldate = {2023-04-04},
	journal = {American Journal of Pharmaceutical Education},
	author = {Fincham, Jack E.},
	month = apr,
	year = {2008},
	pmid = {18483608},
	pmcid = {PMC2384218},
	pages = {43},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/Y6IGD3SL/Fincham - 2008 - Response Rates and Responsiveness for Surveys, Sta.pdf:application/pdf},
}

@article{beckerGenderSurveyParticipation2017,
	title = {Gender and {Survey} {Participation} {An} {Event} {History} {Analysis} of the {Gender} {Effects} of {Survey} {Participation} in a {Probability}-based {Multi}-wave {Panel} {Study} with a {Sequential} {Mixed}-mode {Design}},
	volume = {data},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://mda.gesis.org/index.php/mda/article/view/2021.08},
	doi = {10.12758/MDA.2021.08},
	abstract = {In cross-sectional surveys, as well as in longitudinal panel studies, systematic gender differences in survey participation are routinely observed. Since there has been little research on this issue, this study seeks to reveal this association for web-based online surveys and computer-assisted telephone interviews in the context of a sequential mixed-mode design with a push-to-web method. Based on diverse versions of benefit–cost theories relating to deliberative and heuristic decision-making, several hypotheses are deduced and then tested by longitudinal data in the context of a multi-wave panel study on the educational and occupational trajectories of juveniles. Employing event history data on the survey participation of young panelists living in German-speaking cantons in Switzerland and matching them with geographical data at the macro level and panel characteristics at the meso level, none of the hypotheses is confirmed empirically. It is concluded that indirect measures of an individual’s perceptions of a situation, and of the benefits and costs as well as the process and mechanisms of the decision relating to survey participation, are insufficient to explain this gender difference. Direct tests of these theoretical approaches are needed in future.},
	language = {en},
	urldate = {2023-04-04},
	journal = {methods},
	author = {Becker, Rolf},
	month = jul,
	year = {2017},
	note = {Artwork Size: 29 Pages
Publisher: methods, data, analyses},
	keywords = {Gender, survey participation, nonresponse, event history analysis, societal environment, panel study, web-based online survey, sequential mixed-mode design, push-to-web method},
	pages = {29 Pages},
	annote = {SeriesInformation
methods, data, analyses, Online First},
	file = {Becker - 2017 - Gender and Survey Participation An Event History A.pdf:/Users/shannondickson/Zotero/storage/GJ4U8QGM/Becker - 2017 - Gender and Survey Participation An Event History A.pdf:application/pdf},
}

@article{rosenbaumCentralRolePropensity,
	title = {The central role of the propensity score in observational studies for causal effects},
	abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a twodimensional plot.},
	language = {en},
	author = {Rosenbaum, Paul R and Rubin, Donald B},
	file = {Rosenbaum and Rubin - The central role of the propensity score in observ.pdf:/Users/shannondickson/Zotero/storage/63G2PLLZ/Rosenbaum and Rubin - The central role of the propensity score in observ.pdf:application/pdf},
}

@article{elliottPatternsUnitItem2005,
	title = {Patterns of {Unit} and {Item} {Nonresponse} in the {CAHPS}® {Hospital} {Survey}},
	volume = {40},
	issn = {0017-9124},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1361246/},
	doi = {10.1111/j.1475-6773.2005.00476.x},
	abstract = {Objective
To examine the predictors of unit and item nonresponse, the magnitude of nonresponse bias, and the need for nonresponse weights in the Consumer Assessment of Health Care Providers and Systems (CAHPS®) Hospital Survey.

Methods
A common set of 11 administrative variables (41 degrees of freedom) was used to predict unit nonresponse and the rate of item nonresponse in multivariate models. Descriptive statistics were used to examine the impact of nonresponse on CAHPS Hospital Survey ratings and reports.

Results
Unit nonresponse was highest for younger patients and patients other than non-Hispanic whites (p{\textless}.001); item nonresponse increased steadily with age (p{\textless}.001). Fourteen of 20 reports of ratings of care had significant (p{\textless}.05) but small negative correlations with nonresponse weights (median −0.06; maximum −0.09). Nonresponse weights do not improve overall precision below sample sizes of 300–1,000, and are unlikely to improve the precision of hospital comparisons. In some contexts, case-mix adjustment eliminates most observed nonresponse bias.

Conclusions
Nonresponse weights should not be used for between-hospital comparisons of the CAHPS Hospital Survey, but may make small contributions to overall estimates or demographic comparisons, especially in the absence of case-mix adjustment.},
	number = {6 Pt 2},
	urldate = {2023-04-04},
	journal = {Health Services Research},
	author = {Elliott, Marc N and Edwards, Carol and Angeles, January and Hambarsoomians, Katrin and Hays, Ron D},
	month = dec,
	year = {2005},
	pmid = {16316440},
	pmcid = {PMC1361246},
	pages = {2096--2119},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/YDN9EX2U/Elliott et al. - 2005 - Patterns of Unit and Item Nonresponse in the CAHPS.pdf:application/pdf},
}

@article{wagnerExperimentalEvaluationTwo2023,
	title = {An {Experimental} {Evaluation} of {Two} {Approaches} for {Improving} {Response} to {Household} {Screening} {Efforts} in {National} {Mail}/{Web} {Surveys}},
	volume = {11},
	issn = {2325-0984},
	url = {https://doi.org/10.1093/jssam/smac024},
	doi = {10.1093/jssam/smac024},
	abstract = {Survey researchers have carefully modified their data collection operations for various reasons, including the rising costs of data collection and the ongoing Coronavirus disease (COVID-19) pandemic, both of which have made in-person interviewing difficult. For large national surveys that require household (HH) screening to determine survey eligibility, cost-efficient screening methods that do not include in-person visits need additional evaluation and testing. A new study, known as the American Family Health Study (AFHS), recently initiated data collection with a national probability sample, using a sequential mixed-mode mail/web protocol for push-to-web US HH screening (targeting persons aged 18–49 years). To better understand optimal approaches for this type of national screening effort, we embedded two randomized experiments in the AFHS data collection. The first tested the use of bilingual respondent materials where mailed invitations to the screener were sent in both English and Spanish to 50 percent of addresses with a high predicted likelihood of having a Spanish speaker and 10 percent of all other addresses. We found that the bilingual approach did not increase the response rate of high-likelihood Spanish-speaking addresses, but consistent with prior work, it increased the proportion of eligible Hispanic respondents identified among completed screeners, especially among addresses predicted to have a high likelihood of having Spanish speakers. The second tested a form of nonresponse follow-up, where a subsample of active sampled HHs that had not yet responded to the screening invitations was sent a priority mailing with a \$5 incentive, adding to the \$2 incentive provided for all sampled HHs in the initial screening invitation. We found this approach to be quite valuable for increasing the screening survey response rate.},
	number = {1},
	urldate = {2023-04-04},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Wagner, James and West, Brady T and Couper, Mick P and Zhang, Shiyu and Gatward, Rebecca and Nishimura, Raphael and Saw, Htay-Wah},
	month = feb,
	year = {2023},
	pages = {124--140},
	file = {Accepted Version:/Users/shannondickson/Zotero/storage/L2MMDKH8/Wagner et al. - 2023 - An Experimental Evaluation of Two Approaches for I.pdf:application/pdf},
}

@article{kernPredictingNonresponseFuture2023,
	title = {Predicting {Nonresponse} in {Future} {Waves} of a {Probability}-{Based} {Mixed}-{Mode} {Panel} with {Machine} {Learning}*},
	volume = {11},
	issn = {2325-0984},
	url = {https://doi.org/10.1093/jssam/smab009},
	doi = {10.1093/jssam/smab009},
	abstract = {Nonresponse in panel studies can lead to a substantial loss in data quality owing to its potential to introduce bias and distort survey estimates. Recent work investigates the usage of machine learning to predict nonresponse in advance, such that predicted nonresponse propensities can be used to inform the data collection process. However, predicting nonresponse in panel studies requires accounting for the longitudinal data structure in terms of model building, tuning, and evaluation. This study proposes a longitudinal framework for predicting nonresponse with machine learning and multiple panel waves and illustrates its application. With respect to model building, this approach utilizes information from multiple waves by introducing features that aggregate previous (non)response patterns. Concerning model tuning and evaluation, temporal crossvalidation is employed by iterating through pairs of panel waves such that the training and test sets move in time. Implementing this approach with data from a German probability-based mixed-mode panel shows that aggregating information over multiple panel waves can be used to build prediction models with competitive and robust performance over all test waves.},
	number = {1},
	urldate = {2023-04-04},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Kern, Christoph and Weiß, Bernd and Kolb, Jan-Philipp},
	month = feb,
	year = {2023},
	pages = {100--123},
}

@book{pearlCausalityModelsReasoning2009,
	address = {New York},
	edition = {2nd},
	title = {Causality. {Models}, {Reasoning}, and {Inference}},
	isbn = {978-0-521-89560-6},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	year = {2009},
}

@article{bethlehemReductionNonresponseBia1988,
	title = {Reduction of nonresponse bia through regression estimation.},
	volume = {4},
	number = {3},
	journal = {Journal of Official Statistics},
	author = {Bethlehem, Jelke},
	year = {1988},
	pages = {251--260},
}
