
@article{grovesExperimentsProducingNonresponse2006,
	title = {Experiments in {Producing} {Nonresponse} {Bias}},
	volume = {70},
	issn = {1537-5331, 0033-362X},
	url = {http://academic.oup.com/poq/article/70/5/720/4084446/Experiments-in-Producing-Nonresponse-Bias},
	doi = {10.1093/poq/nfl036},
	abstract = {While nonresponse rates in household surveys are increasing in most industrialized nations, the increasing rates do not always produce nonresponse bias in survey estimates. The linkage between nonresponse rates and nonresponse bias arises from the presence of a covariance between response propensity and the survey variables of interest. To understand the covariance term, researchers must think about the common influences on response propensity and the survey variable. Three variables appear to be especially relevant in this regard: interest in the survey topic, reactions to the survey sponsor, and the use of incentives. A set of randomized experiments tests whether those likely to be interested in the stated survey topic participate at higher rates and whether nonresponse bias on estimates involving variables central to the survey topic is affected by this. The experiments also test whether incentives disproportionately increase the participation of those less interested in the topic. The experiments show mixed results in support of these key hypotheses.},
	language = {en},
	number = {5},
	urldate = {2022-11-21},
	journal = {Public Opinion Quarterly},
	author = {Groves, R. M. and Couper, Mick P. and Presser, Stanley and Singer, Eleanor and Tourangeau, Roger and Acosta, Giorgina Piani and Nelson, Lindsay},
	year = {2006},
	pages = {720--736},
	file = {Groves et al. - 2006 - Experiments in Producing Nonresponse Bias.pdf:/Users/shannondickson/Zotero/storage/9D3H7DB2/Groves et al. - 2006 - Experiments in Producing Nonresponse Bias.pdf:application/pdf},
}

@article{olsonSurveyParticipationNonresponse2006,
	title = {Survey {Participation}, {Nonresponse} {Bias}, {Measurement} {Error} {Bias}, and {Total} {Bias}},
	volume = {70},
	issn = {1537-5331, 0033-362X},
	url = {http://academic.oup.com/poq/article/70/5/737/4084448/Survey-Participation-Nonresponse-Bias-Measurement},
	doi = {10.1093/poq/nfl038},
	abstract = {A common hypothesis about practices to reduce survey nonresponse is that those persons brought into the respondent pool through persuasive efforts may provide data filled with measurement error. Two questions flow from this hypothesis. First, does the mean square error of a statistic increase when sample persons who are less likely to be contacted or cooperate are incorporated into the respondent pool? Second, do nonresponse bias estimates made on the respondents, using survey reports instead of records, provide accurate information about nonresponse bias? Using a unique data set, the Wisconsin Divorce Study, with divorce records as the frame and questions about the frame information included in the questionnaire, this article takes a first look into these two issues. We find that the relationship between nonresponse bias, measurement error bias, and response propensity is statistic- specific and specific to the type of nonresponse. Total bias tends to be lower on estimates calculated using all respondents, compared with those with only the highest contact and cooperation propensities, and nonresponse bias analyses based on respondents yield conclusions similar to those based on records. Finally, we find that error properties of statistics may differ from error properties of the individual variables used to calculate the statistics.},
	language = {en},
	number = {5},
	urldate = {2022-11-21},
	journal = {Public Opinion Quarterly},
	author = {Olson, Kristen},
	year = {2006},
	pages = {737--758},
	file = {Olson - 2006 - Survey Participation, Nonresponse Bias, Measuremen.pdf:/Users/shannondickson/Zotero/storage/RDAT4HQS/Olson - 2006 - Survey Participation, Nonresponse Bias, Measuremen.pdf:application/pdf},
}

@article{bethlehemIndicatorsRepresentativenessSurvey,
	title = {Indicators for the {Representativeness} of {Survey} {Response}},
	abstract = {Many survey organizations use the response rate as an indicator for the quality of survey data. As a consequence, a variety of measures are implemented to reduce non-response or to maintain response at an acceptable level. However, the response rate is not necessarily a good indicator of non-response bias. A higher response rate does not imply smaller non-response bias. What matters is how the composition of the response differs from the composition of the sample as a whole. This paper describes the concept of R-indicators to assess potential differences between the sample and the response. Such indicators may facilitate analysis of survey response over time, between various fieldwork strategies or data collection modes. Some practical examples are given.},
	language = {en},
	author = {Bethlehem, Jelke and Cobben, Fannie and Schouten, Barry},
	pages = {8},
	file = {Bethlehem et al. - Indicators for the Representativeness of Survey Re.pdf:/Users/shannondickson/Zotero/storage/5ULJ6YT9/Bethlehem et al. - Indicators for the Representativeness of Survey Re.pdf:application/pdf},
}

@article{huaLinkSurveyResponse,
	title = {The {Link} {Between} {Survey} {Response} {Rates} and {Nonresponse} {Bias}: {Theory}, {Simulations}, and {Empirical} {Evidence} {From} the {Household} {Pulse} {Survey}},
	language = {en},
	author = {Hua, Tim Tian},
	pages = {20},
	file = {Hua - The Link Between Survey Response Rates and Nonresp.pdf:/Users/shannondickson/Zotero/storage/4F52CTRH/Hua - The Link Between Survey Response Rates and Nonresp.pdf:application/pdf},
}

@article{gundgaardEffectNonresponseEstimates2007,
	title = {The effect of non-response on estimates of health care utilisation: linking health surveys and registers},
	volume = {18},
	issn = {1101-1262, 1464-360X},
	shorttitle = {The effect of non-response on estimates of health care utilisation},
	url = {https://academic.oup.com/eurpub/article-lookup/doi/10.1093/eurpub/ckm103},
	doi = {10.1093/eurpub/ckm103},
	language = {en},
	number = {2},
	urldate = {2022-11-21},
	journal = {The European Journal of Public Health},
	author = {Gundgaard, J. and Ekholm, O. and Hansen, E. H. and Rasmussen, N. Kr.},
	month = dec,
	year = {2007},
	pages = {189--194},
	file = {Gundgaard et al. - 2007 - The effect of non-response on estimates of health .pdf:/Users/shannondickson/Zotero/storage/FQRIK5ZW/Gundgaard et al. - 2007 - The effect of non-response on estimates of health .pdf:application/pdf},
}

@article{masseyWhereWeGo2013,
	title = {Where {Do} {We} {Go} from {Here}? {Nonresponse} and {Social} {Measurement}},
	volume = {645},
	issn = {0002-7162, 1552-3349},
	shorttitle = {Where {Do} {We} {Go} from {Here}?},
	url = {http://journals.sagepub.com/doi/10.1177/0002716212464191},
	doi = {10.1177/0002716212464191},
	abstract = {Surveys undergird government statistical systems and social scientific research throughout the world. Rates of nonresponse are rising in cross-sectional surveys (those conducted during a fixed period of time and not repeated). Although this trend worries those concerned with the validity of survey data, there is no necessary relationship between the rate of nonresponse and the degree of bias. A high rate of nonresponse merely creates the potential for bias, but the degree of bias depends on how factors promoting nonresponse are related to variables of interest. Nonresponse can be reduced by offering financial incentives to respondents and by careful design before entering the field, creating a trade-off between cost and potential bias. When bias is suspected, it can be countered by weighting individual cases by the inverse of their response propensity. Response propensities are typically estimated using a logistic regression equation to predict the dichotomous outcome of survey participation as a function of auxiliary variables. The Multi-level Integrated Database Approach employs multiple databases to collect as much information as possible about the target sample during the initial sampling stage and at all possible levels of aggregation to maximize the accuracy of estimated response propensities.},
	language = {en},
	number = {1},
	urldate = {2022-11-21},
	journal = {The ANNALS of the American Academy of Political and Social Science},
	author = {Massey, Douglas S. and Tourangeau, Roger},
	month = jan,
	year = {2013},
	pages = {222--236},
	file = {Massey and Tourangeau - 2013 - Where Do We Go from Here Nonresponse and Social M.pdf:/Users/shannondickson/Zotero/storage/ANSYLER6/Massey and Tourangeau - 2013 - Where Do We Go from Here Nonresponse and Social M.pdf:application/pdf;v48i04.pdf:/Users/shannondickson/Zotero/storage/5GDLBDRV/v48i04.pdf:application/pdf},
}

@article{wagnerComparisonAlternativeIndicators2012,
	title = {A {Comparison} of {Alternative} {Indicators} for the {Risk} of {Nonresponse} {Bias}},
	volume = {76},
	issn = {0033-362X, 1537-5331},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfs032},
	doi = {10.1093/poq/nfs032},
	abstract = {The response rate has played a key role in measuring the risk of nonresponse bias. However, recent empirical evidence has called into question the utility of the response rate for predicting nonresponse bias. The search for alternatives to the response rate has begun. The present article offers a typology for these indicators, briefly describes the strengths and weaknesses of each type, and suggests directions for future research. New standards for reporting on the risk of nonresponse bias may be needed. Certainly, any analysis into the risk of nonresponse bias will need to be multifaceted and include sensitivity analyses designed to test the impact of key assumptions about the data that are missing due to nonresponse.},
	language = {en},
	number = {3},
	urldate = {2022-11-21},
	journal = {Public Opinion Quarterly},
	author = {Wagner, J.},
	month = sep,
	year = {2012},
	pages = {555--575},
	file = {Wagner - 2012 - A Comparison of Alternative Indicators for the Ris.pdf:/Users/shannondickson/Zotero/storage/FT7926UI/Wagner - 2012 - A Comparison of Alternative Indicators for the Ris.pdf:application/pdf},
}

@article{nishimuraAlternativeIndicatorsRisk2016,
	title = {Alternative {Indicators} for the {Risk} of {Non}-response {Bias}: {A} {Simulation} {Study}: {Alternative} {Indicators} for {Non}-response {Bias}},
	volume = {84},
	issn = {03067734},
	shorttitle = {Alternative {Indicators} for the {Risk} of {Non}-response {Bias}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/insr.12100},
	doi = {10.1111/insr.12100},
	abstract = {The growth of nonresponse rates for social science surveys has led to increased concern about the risk of nonresponse bias. Unfortunately, the nonresponse rate is a poor indicator of when nonresponse bias is likely to occur. We consider in this paper a set of alternative indicators. A large-scale simulation study is used to explore how each of these indicators performs in a variety of circumstances. Although, as expected, none of the indicators fully depicts the impact of nonresponse in survey esti mates, we discuss how they can be used when creating a plausible account of the risks for nonresponse bias for a survey. We also describe an interesting characteristic of the FMI that may be helpful in diagnosing NMAR mechanisms in certain situations.},
	language = {en},
	number = {1},
	urldate = {2022-11-21},
	journal = {International Statistical Review},
	author = {Nishimura, Raphael and Wagner, James and Elliott, Michael},
	month = apr,
	year = {2016},
	pages = {43--62},
	file = {Nishimura et al. - 2016 - Alternative Indicators for the Risk of Non-respons.pdf:/Users/shannondickson/Zotero/storage/UQE8F8WA/Nishimura et al. - 2016 - Alternative Indicators for the Risk of Non-respons.pdf:application/pdf},
}

@article{grovesImpactNonresponseRates2008,
	title = {The {Impact} of {Nonresponse} {Rates} on {Nonresponse} {Bias}: {A} {Meta}-{Analysis}},
	volume = {72},
	issn = {0033-362X, 1537-5331},
	shorttitle = {The {Impact} of {Nonresponse} {Rates} on {Nonresponse} {Bias}},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfn011},
	doi = {10.1093/poq/nfn011},
	abstract = {Fifty-nine methodological studies were designed to estimate the magnitude of nonresponse bias in statistics of interest. These studies use a variety of designs: sampling frames with rich variables, data from administrative records matched to sample case, use of screeninginterview data to describe nonrespondents to main interviews, followup of nonrespondents to initial phases of ﬁeld effort, and measures of behavior intentions to respond to a survey. This permits exploration of which circumstances produce a relationship between nonresponse rates and nonresponse bias and which, do not. The predictors are design features of the surveys, characteristics of the sample, and attributes of the survey statistics computed in the surveys.},
	language = {en},
	number = {2},
	urldate = {2022-11-21},
	journal = {Public Opinion Quarterly},
	author = {Groves, R. M. and Peytcheva, E.},
	month = may,
	year = {2008},
	pages = {167--189},
	file = {Groves and Peytcheva - 2008 - The Impact of Nonresponse Rates on Nonresponse Bia.pdf:/Users/shannondickson/Zotero/storage/GVCDTC5L/Groves and Peytcheva - 2008 - The Impact of Nonresponse Rates on Nonresponse Bia.pdf:application/pdf},
}

@article{kohlerAllOddsRobustness,
	title = {Against all odds: {On} the robustness of probability samples against decreases in response rates},
	abstract = {Responses rates in surveys with probability samples have decreased in the last decades, but has this decrease caused a decline in sample quality? Our presentation addresses this question with an analysis of methodological data describing 739 surveys from four cross-national survey projects: European Quality of Life Survey, European Social Survey, European Values Study, and International Social Survey Programme, between 1999 and 2019. Based on a theoretical model of factors that shape unit nonresponse and unit nonresponse bias, we estimate causal effects of historical time on both, nonresponse and nonresponse bias, as well as the contribution of nonresponse to nonresponse bias. Results show that the decline in response rates was not accompanied by a drop in nonresponse bias.},
	language = {en},
	author = {Kohler, Ulrich and Jabkowski, Piotr and Kołczyńska, Marta},
	pages = {44},
	file = {Kohler et al. - Against all odds On the robustness of probability.pdf:/Users/shannondickson/Zotero/storage/HHC28DUF/Kohler et al. - Against all odds On the robustness of probability.pdf:application/pdf},
}

@article{tourangeauSensitiveTopicsReluctant2010,
	title = {Sensitive {Topics} and {Reluctant} {Respondents}: {Demonstrating} a {Link} between {Nonresponse} {Bias} and {Measurement} {Error}},
	volume = {74},
	issn = {0033-362X, 1537-5331},
	shorttitle = {Sensitive {Topics} and {Reluctant} {Respondents}},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfq004},
	doi = {10.1093/poq/nfq004},
	language = {en},
	number = {3},
	urldate = {2022-11-24},
	journal = {Public Opinion Quarterly},
	author = {Tourangeau, R. and Groves, R. M. and Redline, C. D.},
	month = sep,
	year = {2010},
	pages = {413--432},
	file = {0002716212461748.pdf:/Users/shannondickson/Zotero/storage/SBWLMMYS/0002716212461748.pdf:application/pdf;0049124117701479.pdf:/Users/shannondickson/Zotero/storage/CF4YPSMZ/0049124117701479.pdf:application/pdf;brick and tourangeau (2017) re-analysis of grives and peytcheva.pdf:/Users/shannondickson/Zotero/storage/IFY9MJGA/brick and tourangeau (2017) re-analysis of grives and peytcheva.pdf:application/pdf;draft_20221019_clean.pdf:/Users/shannondickson/Zotero/storage/5WVT84TX/draft_20221019_clean.pdf:application/pdf;MAGNA_preprint.pdf:/Users/shannondickson/Zotero/storage/FXXPZ5WA/MAGNA_preprint.pdf:application/pdf;ORM2007-Nonresponse.pdf:/Users/shannondickson/Zotero/storage/Q5QHPZ5H/ORM2007-Nonresponse.pdf:application/pdf;preprint_sample_size_analysis_method.pdf:/Users/shannondickson/Zotero/storage/HMVJEYGJ/preprint_sample_size_analysis_method.pdf:application/pdf;Tourangeau (2017) presidential address.pdf:/Users/shannondickson/Zotero/storage/MW78U97Z/Tourangeau (2017) presidential address.pdf:application/pdf;Tourangeau et al. - 2010 - Sensitive Topics and Reluctant Respondents Demons.pdf:/Users/shannondickson/Zotero/storage/AXCK5N7U/Tourangeau et al. - 2010 - Sensitive Topics and Reluctant Respondents Demons.pdf:application/pdf;TourangeauGrovesRedline.pdf:/Users/shannondickson/Zotero/storage/UH3EUFGE/TourangeauGrovesRedline.pdf:application/pdf;v93i08.pdf:/Users/shannondickson/Zotero/storage/X9ZPMJTH/v93i08.pdf:application/pdf},
}

@article{peytchevNotAllSurvey2009,
	title = {Not {All} {Survey} {Effort} {Is} {Equal}: {Reduction} of {Nonresponse} {Bias} and {Nonresponse} {Error}},
	volume = {73},
	issn = {0033-362X},
	shorttitle = {Not {All} {Survey} {Effort} {Is} {Equal}},
	url = {http://www.jstor.org/stable/40467642},
	abstract = {Nonexperimental and experimental studies have shown a lack of association between survey effort and nonresponse bias. This does not necessarily mean, however, that additional effort could not reduce nonresponse bias. Theories on nonresponse would suggest the use of different recruiting methods for additional survey effort in order to address nonresponse bias. This study looks at changes in survey estimates as a function of making additional calls under the same protocol and additional calls under a different protocol. Respondents who were interviewed as a result of more than five call attempts were not significantly different on any of the key survey variables than those interviewed with fewer than five calls. Those interviewed under a different survey protocol, however, were different on 5 of 12 measures. Additional interviews under both the same and different protocols contributed to the reduction of total nonresponse error. In sum, the use of multiple protocols for part of the survey effort increased the response rate, changed point estimates, and achieved lower total nonresponse error. Future work is needed on optimizing survey designs that implement multiple survey protocols.},
	number = {4},
	urldate = {2022-12-01},
	journal = {The Public Opinion Quarterly},
	author = {Peytchev, Andy and Baxter, Rodney K. and Carley-Baxter, Lisa R.},
	year = {2009},
	note = {Publisher: [Oxford University Press, American Association for Public Opinion Research]},
	pages = {785--806},
	file = {JSTOR Full Text PDF:/Users/shannondickson/Zotero/storage/9TSKX9XD/Peytchev et al. - 2009 - Not All Survey Effort Is Equal Reduction of Nonre.pdf:application/pdf},
}

@misc{AssessingTrendsDecomposing,
	title = {Assessing {Trends} and {Decomposing} {Change} in {Nonresponse} {Bias}: {The} {Case} of {Bias} in {Cohort} {Distributions}},
	shorttitle = {Assessing {Trends} and {Decomposing} {Change} in {Nonresponse} {Bias}},
	url = {http://journals.sagepub.com/doi/epub/10.1177/0049124117701479},
	language = {en},
	urldate = {2022-12-01},
	doi = {10.1177/0049124117701479},
}

@article{hedlinThereSafeArea2020,
	title = {Is there a 'safe area' where the nonresponse rate has only a modest effect on bias despite non-ignorable nonresponse?},
	volume = {88},
	issn = {1751-5823},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12359},
	doi = {10.1111/insr.12359},
	abstract = {Rising nonresponse rates in social surveys make the issue of nonresponse bias contentious. There are conflicting messages about the importance of high response rates and the hazards of low rates. Some articles (e.g. Groves and Peytcheva, 2008) suggest that the response rate is in general not a good predictor of survey quality. Equally, it is well known that nonresponse may induce bias and increase data collection costs. We go back in the history of the literature of nonresponse and suggest a possible reason to the notion that even a rather small nonresponse rate makes the quality of a survey debatable. We also explore the relationship between nonresponse rate and bias, assuming non-ignorable nonresponse and focusing on estimates of totals or means. We show that there is a ‘safe area’ enclosed by the response rate on the one hand and the correlation between the response propensity and the study variable on the other hand; in this area, (1) the response rate does not greatly affect the nonresponse bias, and (2) the nonresponse bias is small.},
	language = {en},
	number = {3},
	urldate = {2022-12-01},
	journal = {International Statistical Review},
	author = {Hedlin, Dan},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12359},
	keywords = {non-ignorable response, nonresponse bias, nonresponse rate, survey quality},
	pages = {642--657},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/XNEQ2BLQ/Hedlin - 2020 - Is there a 'safe area' where the nonresponse rate .pdf:application/pdf},
}

@article{borsboomNetworkAnalysisMultivariate2021,
	title = {Network analysis of multivariate data in psychological science},
	volume = {1},
	copyright = {2021 Springer Nature Limited},
	issn = {2662-8449},
	url = {https://www.nature.com/articles/s43586-021-00055-w},
	doi = {10.1038/s43586-021-00055-w},
	abstract = {In recent years, network analysis has been applied to identify and analyse patterns of statistical association in multivariate psychological data. In these approaches, network nodes represent variables in a data set, and edges represent pairwise conditional associations between variables in the data, while conditioning on the remaining variables. This Primer provides an anatomy of these techniques, describes the current state of the art and discusses open problems. We identify relevant data structures in which network analysis may be applied: cross-sectional data, repeated measures and intensive longitudinal data. We then discuss the estimation of network structures in each of these cases, as well as assessment techniques to evaluate network robustness and replicability. Successful applications of the technique in different research areas are highlighted. Finally, we discuss limitations and challenges for future research.},
	language = {en},
	number = {1},
	urldate = {2022-12-01},
	journal = {Nature Reviews Methods Primers},
	author = {Borsboom, Denny and Deserno, Marie K. and Rhemtulla, Mijke and Epskamp, Sacha and Fried, Eiko I. and McNally, Richard J. and Robinaugh, Donald J. and Perugini, Marco and Dalege, Jonas and Costantini, Giulio and Isvoranu, Adela-Maria and Wysocki, Anna C. and van Borkulo, Claudia D. and van Bork, Riet and Waldorp, Lourens J.},
	month = aug,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Scientific data, Statistics},
	pages = {1--18},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/DS3WBAZ3/Borsboom et al. - 2021 - Network analysis of multivariate data in psycholog.pdf:application/pdf},
}

@misc{haslbeckMgmEstimatingTimeVarying2020,
	title = {mgm: {Estimating} {Time}-{Varying} {Mixed} {Graphical} {Models} in {High}-{Dimensional} {Data}},
	shorttitle = {mgm},
	url = {http://arxiv.org/abs/1510.06871},
	abstract = {We present the R-package mgm for the estimation of k-order Mixed Graphical Models (MGMs) and mixed Vector Autoregressive (mVAR) models in high-dimensional data. These are a useful extensions of graphical models for only one variable type, since data sets consisting of mixed types of variables (continuous, count, categorical) are ubiquitous. In addition, we allow to relax the stationarity assumption of both models by introducing time-varying versions MGMs and mVAR models based on a kernel weighting approach. Time-varying models oﬀer a rich description of temporally evolving systems and allow to identify external inﬂuences on the model structure such as the impact of interventions. We provide the background of all implemented methods and provide fully reproducible examples that illustrate how to use the package.},
	language = {en},
	urldate = {2022-12-01},
	publisher = {arXiv},
	author = {Haslbeck, Jonas M. B. and Waldorp, Lourens J.},
	month = feb,
	year = {2020},
	note = {arXiv:1510.06871 [stat]},
	keywords = {Statistics - Applications},
	file = {Haslbeck and Waldorp - 2020 - mgm Estimating Time-Varying Mixed Graphical Model.pdf:/Users/shannondickson/Zotero/storage/S8955N6K/Haslbeck and Waldorp - 2020 - mgm Estimating Time-Varying Mixed Graphical Model.pdf:application/pdf},
}

@article{peytchevConsequencesSurveyNonresponse2013a,
	title = {Consequences of {Survey} {Nonresponse}},
	volume = {645},
	issn = {0002-7162, 1552-3349},
	url = {http://journals.sagepub.com/doi/10.1177/0002716212461748},
	doi = {10.1177/0002716212461748},
	abstract = {Nonresponse is a prominent problem in sample surveys. At face value, it reduces the trust in survey estimates. Nonresponse undermines the probability-based inferential mechanism and introduces the potential for nonresponse bias. In addition, there are other important consequences. The effort to limit increasing nonresponse has led to higher survey costs—allocation of greater resources to measure and reduce nonresponse. Nonresponse has also led to greater survey complexity in terms of design, implementation, and processing of survey data, such as the use of multiphase and responsive designs. The use of mixed-mode and multiframe designs to address nonresponse increases complexity but also introduces other sources of error. Surveys have to rely to a greater extent on statistical adjustments and auxiliary data. This article describes the major consequences of survey nonresponse, with particular attention to recent years.},
	language = {en},
	number = {1},
	urldate = {2022-12-01},
	journal = {The ANNALS of the American Academy of Political and Social Science},
	author = {Peytchev, Andy},
	month = jan,
	year = {2013},
	pages = {88--111},
	file = {Peytchev - 2013 - Consequences of Survey Nonresponse.pdf:/Users/shannondickson/Zotero/storage/YBSUTPXN/Peytchev - 2013 - Consequences of Survey Nonresponse.pdf:application/pdf},
}

@article{gummerAssessingTrendsDecomposing2019,
	title = {Assessing {Trends} and {Decomposing} {Change} in {Nonresponse} {Bias}: {The} {Case} of {Bias} in {Cohort} {Distributions}},
	volume = {48},
	issn = {0049-1241, 1552-8294},
	shorttitle = {Assessing {Trends} and {Decomposing} {Change} in {Nonresponse} {Bias}},
	url = {http://journals.sagepub.com/doi/10.1177/0049124117701479},
	doi = {10.1177/0049124117701479},
	abstract = {Survey research is still confronted by a trend of increasing nonresponse rates. In this context, several methodological advances have been made to stimulate participation and avoid bias. Yet, despite the growing number of tools and methods to deal with nonresponse, little is known about whether nonresponse biases show similar trends as nonresponse rates and what mechanisms (if any) drive changes in bias. Our article focuses on biases in cohort distributions in the U.S. and German general social surveys from 1980 to 2012 as one of the key variables in the social sciences. To supplement our cross-national comparison of these trends, we decompose changes into within-cohort change (WCC) and between-cohort change. We find that biases in cohort distributions have remained relatively stable and at a relatively low level in both countries. Furthermore, WCC (i.e., survey climate) accounts for the major part of the change in nonresponse bias.},
	language = {en},
	number = {1},
	urldate = {2022-12-01},
	journal = {Sociological Methods \& Research},
	author = {Gummer, Tobias},
	month = feb,
	year = {2019},
	pages = {92--115},
	file = {Gummer - 2019 - Assessing Trends and Decomposing Change in Nonresp.pdf:/Users/shannondickson/Zotero/storage/R2FEWDZZ/Gummer - 2019 - Assessing Trends and Decomposing Change in Nonresp.pdf:application/pdf},
}

@article{brickResponsiveSurveyDesigns2017,
	title = {Responsive {Survey} {Designs} for {Reducing} {Nonresponse} {Bias}},
	volume = {33},
	issn = {2001-7367},
	url = {https://www.sciendo.com/article/10.1515/jos-2017-0034},
	doi = {10.1515/jos-2017-0034},
	abstract = {Abstract
            Survey researchers have been investigating alternative approaches to reduce data collection costs while mitigating the risk of nonresponse bias or to produce more accurate estimates within the same budget. Responsive or adaptive design has been suggested as one means for doing this. Falling survey response rates and the need to find effective ways of implementing responsive design has focused attention on the relationship between response rates and nonresponse bias. In our article, we re-examine the data compiled by Groves and Peytcheva (2008) in their influential article and show there is an important between-study component of variance in addition to the within-study variance highlighted in the original analysis. We also show that theory implies that raising response rates can help reduce the nonresponse bias on average across the estimates within a study. We then propose a typology of response propensity models that help explain the empirical findings, including the relative weak relationship between nonresponse rates and nonresponse bias. Using these results, we explore when responsive design tools such as switching modes, giving monetary incentives, and increasing the level of effort are likely to be effective. We conclude with some comments on the use of responsive design and weighting to control nonresponse bias.},
	language = {en},
	number = {3},
	urldate = {2022-12-01},
	journal = {Journal of Official Statistics},
	author = {Brick, J. Michael and Tourangeau, Roger},
	month = sep,
	year = {2017},
	pages = {735--752},
	file = {Brick and Tourangeau - 2017 - Responsive Survey Designs for Reducing Nonresponse.pdf:/Users/shannondickson/Zotero/storage/Z4Q8YCM8/Brick and Tourangeau - 2017 - Responsive Survey Designs for Reducing Nonresponse.pdf:application/pdf},
}

@article{wernerReportingNonresponseAnalyses2007,
	title = {The {Reporting} of {Nonresponse} {Analyses} in {Survey} {Research}},
	volume = {10},
	issn = {1094-4281, 1552-7425},
	url = {http://journals.sagepub.com/doi/10.1177/1094428106292892},
	doi = {10.1177/1094428106292892},
	abstract = {Because survey respondents may not be representative of the population being studied, the external validity of many research conclusions may be of concern. Nonresponse analyses helps address this concern. The purpose of this article is to identify how frequently nonresponse analyses are reported and what variables are related to these rates. The authors find that less than one third of the survey studies include nonresponse analyses. A number of journal and article quality measures and sample characteristics were found to be related to the reporting of nonresponse analyses.},
	language = {en},
	number = {2},
	urldate = {2022-12-01},
	journal = {Organizational Research Methods},
	author = {Werner, Steve and Praxedes, Moira and Kim, Hyun-Gyu},
	month = apr,
	year = {2007},
	pages = {287--295},
	file = {Werner et al. - 2007 - The Reporting of Nonresponse Analyses in Survey Re.pdf:/Users/shannondickson/Zotero/storage/2PH5639R/Werner et al. - 2007 - The Reporting of Nonresponse Analyses in Survey Re.pdf:application/pdf},
}

@techreport{constantinGeneralMonteCarlo2021,
	type = {preprint},
	title = {A {General} {Monte} {Carlo} {Method} for {Sample} {Size} {Analysis} in the {Context} of {Network} {Models}},
	url = {https://osf.io/j5v7u},
	abstract = {We introduce a general method for sample size computations in the context of cross-sectional network models. The method takes the form of an automated Monte Carlo algorithm, designed to find an optimal sample size while iteratively concentrating the computations on the sample sizes that seem most relevant. The method requires three inputs: 1) a hypothesized network structure or desired characteristics of that structure, 2) an estimation performance measure and its corresponding target value (e.g., a sensitivity of 0.6), and 3) a statistic and its corresponding target value that determines how the target value for the performance measure be reached (e.g., reaching a sensitivity of 0.6 with a probability of 0.8). The method consists of a Monte Carlo simulation step for computing the performance measure and the statistic for several sample sizes selected from an initial candidate sample size range, a curve-fitting step for interpolating the statistic across the entire candidate range, and a stratified bootstrapping step to quantify the uncertainty around the recommendation provided. We evaluated the performance of the method for the Gaussian Graphical Model, but it can easily extend to other models. The method displayed good performance, providing sample size recommendations that were, on average, within three observations of a benchmark sample size, with the highest standard deviation of 25.87 observations. The method discussed is implemented in the form of an R package called powerly, available on GitHub and CRAN.},
	language = {en},
	urldate = {2022-12-01},
	institution = {PsyArXiv},
	author = {Constantin, Mihai Alexandru and Schuurman, Noémi Katalin and Vermunt, Jeroen},
	month = sep,
	year = {2021},
	doi = {10.31234/osf.io/j5v7u},
	file = {Constantin et al. - 2021 - A General Monte Carlo Method for Sample Size Analy.pdf:/Users/shannondickson/Zotero/storage/IDYYL72L/Constantin et al. - 2021 - A General Monte Carlo Method for Sample Size Analy.pdf:application/pdf},
}

@article{tourangeauPresidentialAddress2017,
	title = {Presidential {Address}},
	volume = {81},
	issn = {0033-362X, 1537-5331},
	url = {http://academic.oup.com/poq/article/81/3/803/4085250/Presidential-AddressParadoxes-of-Nonresponse},
	doi = {10.1093/poq/nfx031},
	language = {en},
	number = {3},
	urldate = {2022-12-01},
	journal = {Public Opinion Quarterly},
	author = {Tourangeau, Roger},
	year = {2017},
	pages = {803--814},
	file = {Tourangeau - 2017 - Presidential Address.pdf:/Users/shannondickson/Zotero/storage/VMXIBYYF/Tourangeau - 2017 - Presidential Address.pdf:application/pdf},
}

@article{haslbeckMgmEstimatingTimeVarying2020a,
	title = {\textbf{mgm} : {Estimating} {Time}-{Varying} {Mixed} {Graphical} {Models} in {High}-{Dimensional} {Data}},
	volume = {93},
	issn = {1548-7660},
	shorttitle = {\textbf{mgm}},
	url = {http://www.jstatsoft.org/v93/i08/},
	doi = {10.18637/jss.v093.i08},
	abstract = {We present the R package mgm for the estimation of k-order mixed graphical models (MGMs) and mixed vector autoregressive (mVAR) models in high-dimensional data. These are a useful extensions of graphical models for only one variable type, since data sets consisting of mixed types of variables (continuous, count, categorical) are ubiquitous. In addition, we allow to relax the stationarity assumption of both models by introducing time-varying versions of MGMs and mVAR models based on a kernel weighting approach. Time-varying models oﬀer a rich description of temporally evolving systems and allow to identify external inﬂuences on the model structure such as the impact of interventions. We provide the background of all implemented methods and provide fully reproducible examples that illustrate how to use the package.},
	language = {en},
	number = {8},
	urldate = {2022-12-01},
	journal = {Journal of Statistical Software},
	author = {Haslbeck, Jonas M. B. and Waldorp, Lourens J.},
	year = {2020},
	file = {Haslbeck and Waldorp - 2020 - mgm  Estimating Time-Varying Mixed Graphic.pdf:/Users/shannondickson/Zotero/storage/GJ4MBNNP/Haslbeck and Waldorp - 2020 - mgm  Estimating Time-Varying Mixed Graphic.pdf:application/pdf},
}

@misc{haslbeckStructureEstimationMixed2015,
	title = {Structure estimation for mixed graphical models in high-dimensional data},
	url = {http://arxiv.org/abs/1510.05677},
	abstract = {Undirected graphical models are a key component in the analysis of complex observational data in a large variety of disciplines. In many of these applications one is interested in estimating the undirected graphical model underlying a distribution over variables with different domains. Despite the pervasive need for such an estimation method, to date there is no such method that models all variables on their proper domain. We close this methodological gap by combining a new class of mixed graphical models with a structure estimation approach based on generalized covariance matrices. We report the performance of our methods using simulations, illustrate the method with a dataset on Autism Spectrum Disorder (ASD) and provide an implementation as an R-package.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Haslbeck, Jonas M. B. and Waldorp, Lourens J.},
	month = oct,
	year = {2015},
	note = {arXiv:1510.05677 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Applications},
	file = {arXiv Fulltext PDF:/Users/shannondickson/Zotero/storage/INYKH4EP/Haslbeck and Waldorp - 2015 - Structure estimation for mixed graphical models in.pdf:application/pdf},
}

@article{ryanChallengeGeneratingCausal2022,
	title = {The {Challenge} of {Generating} {Causal} {Hypotheses} {Using} {Network} {Models}},
	volume = {29},
	issn = {1070-5511, 1532-8007},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2022.2056039},
	doi = {10.1080/10705511.2022.2056039},
	abstract = {Statistical network models based on Pairwise Markov Random Fields (PMRFs) are popular tools for analyzing multivariate psychological data, in large part due to their perceived role in generating insights into causal relationships: a practice known as causal discovery in the causal modeling literature. However, since network models are not presented as causal discovery tools, the role they play in generating causal insights is poorly understood among empirical researchers. In this paper, we provide a treatment of how PMRFs such as the Gaussian Graphical Model (GGM) work as causal discovery tools, using Directed Acyclic Graphs (DAGs) and Structural Equation Models (SEMs) as causal models. We describe the key assumptions needed for causal discovery and show the equivalence class of causal models that networks identify from data. We clarify four common misconceptions found in the empirical literature relating to networks as causal skeletons; chains of relationships; collider bias; and cyclic causal models.},
	language = {en},
	number = {6},
	urldate = {2022-12-21},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Ryan, Oisín and Bringmann, Laura F. and Schuurman, Noémi K.},
	month = nov,
	year = {2022},
	pages = {953--970},
	file = {Ryan et al. - 2022 - The Challenge of Generating Causal Hypotheses Usin.pdf:/Users/shannondickson/Zotero/storage/CKUM5P3M/Ryan et al. - 2022 - The Challenge of Generating Causal Hypotheses Usin.pdf:application/pdf},
}

@article{daikelerWebOtherSurvey2020,
	title = {Web {Versus} {Other} {Survey} {Modes}: {An} {Updated} and {Extended} {Meta}-{Analysis} {Comparing} {Response} {Rates}},
	volume = {8},
	issn = {2325-0984},
	shorttitle = {Web {Versus} {Other} {Survey} {Modes}},
	url = {https://doi.org/10.1093/jssam/smz008},
	doi = {10.1093/jssam/smz008},
	abstract = {Do web surveys still yield lower response rates compared with other survey modes? To answer this question, we replicated and extended a meta-analysis done in 2008 which found that, based on 45 experimental comparisons, web surveys had an 11 percentage points lower response rate compared with other survey modes. Fundamental changes in internet accessibility and use since the publication of the original meta-analysis would suggest that people’s propensity to participate in web surveys has changed considerably in the meantime. However, in our replication and extension study, which comprised 114 experimental comparisons between web and other survey modes, we found almost no change: web surveys still yielded lower response rates than other modes (a difference of 12 percentage points in response rates). Furthermore, we found that prenotifications, the sample recruitment strategy, the survey’s solicitation mode, the type of target population, the number of contact attempts, and the country in which the survey was conducted moderated the magnitude of the response rate differences. These findings have substantial implications for web survey methodology and operations.},
	number = {3},
	urldate = {2023-01-15},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Daikeler, Jessica and Bošnjak, Michael and Lozar Manfreda, Katja},
	month = jun,
	year = {2020},
	pages = {513--539},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/SXY6EY7G/Daikeler et al. - 2020 - Web Versus Other Survey Modes An Updated and Exte.pdf:application/pdf},
}

@article{boseNONRESPONSEBIASANALYSES,
	title = {{NONRESPONSE} {BIAS} {ANALYSES} {AT} {THE} {NATIONAL} {CENTER} {FOR} {EDUCATION} {STATISTICS}},
	abstract = {In surveys with low response rates, nonresponse bias can be a major concern. While it is not always possible to measure the actual bias due to nonresponse, there are different approaches that help identify potential sources of nonresponse bias. In the National Center for Education Statistics (NCES), surveys with a response rate lower than 70 percent must conduct a nonresponse bias analysis. This paper discusses the different approaches to nonresponse bias analyses using examples from NCES.},
	language = {en},
	author = {Bose, Jonaki},
	file = {Bose - NONRESPONSE BIAS ANALYSES AT THE NATIONAL CENTER F.pdf:/Users/shannondickson/Zotero/storage/5WVD3G56/Bose - NONRESPONSE BIAS ANALYSES AT THE NATIONAL CENTER F.pdf:application/pdf},
}

@article{brickExplainingRisingNonresponse2013,
	title = {Explaining {Rising} {Nonresponse} {Rates} in {Cross}-{Sectional} {Surveys}},
	volume = {645},
	issn = {0002-7162},
	url = {https://doi.org/10.1177/0002716212456834},
	doi = {10.1177/0002716212456834},
	abstract = {This review of nonresponse in cross-sectional household surveys in the United States shows trends in nonresponse rates, the main reasons for nonresponse, and changes in the components of nonresponse. It shows that nonresponse is increasing but that existing methods for modeling response mechanisms do not adequately explain these changes.},
	language = {en},
	number = {1},
	urldate = {2023-01-15},
	journal = {The ANNALS of the American Academy of Political and Social Science},
	author = {Brick, J. Michael and Williams, Douglas},
	month = jan,
	year = {2013},
	note = {Publisher: SAGE Publications Inc},
	pages = {36--59},
	file = {SAGE PDF Full Text:/Users/shannondickson/Zotero/storage/J34DGYMC/Brick and Williams - 2013 - Explaining Rising Nonresponse Rates in Cross-Secti.pdf:application/pdf},
}

@article{teitlerCostsBenefitsImproving2003,
	title = {Costs and {Benefits} of {Improving} {Response} {Rates} for a {Hard}-to-{Reach} {Population}},
	volume = {67},
	issn = {0033-362X},
	url = {https://www.jstor.org/stable/3521668},
	number = {1},
	urldate = {2023-01-16},
	journal = {The Public Opinion Quarterly},
	author = {Teitler, Julien O. and Reichman, Nancy E. and Sprachman, Susan},
	year = {2003},
	note = {Publisher: [Oxford University Press, American Association for Public Opinion Research]},
	pages = {126--138},
	file = {JSTOR Full Text PDF:/Users/shannondickson/Zotero/storage/39GYHEZ8/Teitler et al. - 2003 - Costs and Benefits of Improving Response Rates for.pdf:application/pdf},
}

@article{kreuterFacingNonresponseChallenge2013,
	title = {Facing the {Nonresponse} {Challenge}},
	volume = {645},
	issn = {0002-7162},
	url = {https://doi.org/10.1177/0002716212456815},
	doi = {10.1177/0002716212456815},
	abstract = {This article provides a brief overview of key trends in the survey research to address the nonresponse challenge. Noteworthy are efforts to develop new quality measures and to combine several data sources to enhance either the data collection process or the quality of resulting survey estimates. Mixtures of survey data collection modes and less burdensome survey designs are additional steps taken by survey researchers to address nonresponse.},
	language = {en},
	number = {1},
	urldate = {2023-01-16},
	journal = {The ANNALS of the American Academy of Political and Social Science},
	author = {Kreuter, Frauke},
	month = jan,
	year = {2013},
	note = {Publisher: SAGE Publications Inc},
	pages = {23--35},
	file = {SAGE PDF Full Text:/Users/shannondickson/Zotero/storage/LMRSTXRB/Kreuter - 2013 - Facing the Nonresponse Challenge.pdf:application/pdf},
}

@article{mcgonagleEffectsIncentiveBoost2020,
	title = {The {Effects} of an {Incentive} {Boost} on {Response} {Rates}, {Fieldwork} {Effort}, and {Costs} across {Two} {Waves} of a {Panel} {Study}},
	volume = {14},
	issn = {1864-6956},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8136583/},
	doi = {10.12758/mda.2020.04},
	abstract = {This paper describes the association between an incentive boost and data collection outcomes across two waves of a long-running panel study. In a recent wave, with the aim of achieving response rate goals, all remaining sample members were offered a substantial incentive increase in the final weeks of data collection, despite uncertainty about potential effects on fieldwork outcomes in the following wave. The analyses examine response rates and the average number of interviewer attempts to complete the interview in the waves during and after the incentive boost, and provide an estimate of the cost of the incentives and fieldwork in the waves during and following the boost. The findings provide suggestive evidence that the use of variable incentive strategies from one wave to the next in the context of an ongoing panel study may be an effective strategy to reduce nonresponse and may yield enduring positive effects on subsequent data collection outcomes.},
	number = {2},
	urldate = {2023-01-17},
	journal = {Methoden, daten, analysen},
	author = {McGonagle, Katherine A.},
	year = {2020},
	pmid = {34025812},
	pmcid = {PMC8136583},
	pages = {241--250},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/CS4XX5YI/McGonagle - 2020 - The Effects of an Incentive Boost on Response Rate.pdf:application/pdf},
}

@article{andreskiResponseRatesNational2012,
	title = {Response {Rates} in {National} {Panel} {Surveys} - {Robert} {F}. {Schoeni}, {Frank} {Stafford}, {Katherine} {A}. {Mcgonagle}, {Patricia} {Andreski}, 2013},
	url = {https://journals.sagepub.com/doi/full/10.1177/0002716212456363},
	abstract = {It has been well documented that response rates to cross-sectional surveys have declined over the past few decades. It is less clear whether response rates to l...},
	language = {en},
	urldate = {2023-01-17},
	journal = {The ANNALS of the American Academy of Political and Social Science},
	author = {Andreski, Frank Stafford, Katherine A. Mcgonagle, Patricia, Robert F. Schoeni},
	month = nov,
	year = {2012},
}

@article{sedgewickLearningMixedGraphical2016,
	title = {Learning mixed graphical models with separate sparsity parameters and stability-based model selection},
	volume = {17},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-016-1039-0},
	doi = {10.1186/s12859-016-1039-0},
	abstract = {Mixed graphical models (MGMs) are graphical models learned over a combination of continuous and discrete variables. Mixed variable types are common in biomedical datasets. MGMs consist of a parameterized joint probability density, which implies a network structure over these heterogeneous variables. The network structure reveals direct associations between the variables and the joint probability density allows one to ask arbitrary probabilistic questions on the data. This information can be used for feature selection, classification and other important tasks.},
	number = {5},
	urldate = {2023-01-17},
	journal = {BMC Bioinformatics},
	author = {Sedgewick, Andrew J. and Shi, Ivy and Donovan, Rory M. and Benos, Panayiotis V.},
	month = jun,
	year = {2016},
	keywords = {Chronic Obstructive Pulmonary Disease, Edge Type, Idiopathic Pulmonary Fibrosis, Lasso, Model Selection Method},
	pages = {S175},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/B6LAZ8FJ/Sedgewick et al. - 2016 - Learning mixed graphical models with separate spar.pdf:application/pdf},
}

@article{vandeschootOpenSourceMachine2021,
	title = {An open source machine learning framework for efficient and transparent systematic reviews},
	volume = {3},
	copyright = {2021 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00287-7},
	doi = {10.1038/s42256-020-00287-7},
	abstract = {To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool to accelerate the step of screening titles and abstracts. For many tasks—including but not limited to systematic reviews and meta-analyses—the scientific literature needs to be checked systematically. Scholars and practitioners currently screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that active learning can yield far more efficient reviewing than manual reviewing while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice.},
	language = {en},
	number = {2},
	urldate = {2023-01-20},
	journal = {Nature Machine Intelligence},
	author = {van de Schoot, Rens and de Bruin, Jonathan and Schram, Raoul and Zahedi, Parisa and de Boer, Jan and Weijdema, Felix and Kramer, Bianca and Huijts, Martijn and Hoogerwerf, Maarten and Ferdinands, Gerbrich and Harkema, Albert and Willemsen, Joukje and Ma, Yongchao and Fang, Qixiang and Hindriks, Sybren and Tummers, Lars and Oberski, Daniel L.},
	month = feb,
	year = {2021},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Computer science, Medical research, SARS-CoV-2},
	pages = {125--133},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/XUAQNIGV/van de Schoot et al. - 2021 - An open source machine learning framework for effi.pdf:application/pdf},
}

@article{yangMixedGraphicalModels,
	title = {Mixed {Graphical} {Models} via {Exponential} {Families}},
	abstract = {Markov Random Fields, or undirected graphical models are widely used to model highdimensional multivariate data. Classical instances of these models, such as Gaussian Graphical and Ising Models, as well as recent extensions (Yang et al., 2012) to graphical models speciﬁed by univariate exponential families, assume all variables arise from the same distribution. Complex data from high-throughput genomics and social networking for example, often contain discrete, count, and continuous variables measured on the same set of samples. To model such heterogeneous data, we develop a novel class of mixed graphical models by specifying that each node-conditional distribution is a member of a possibly diﬀerent univariate exponential family. We study several instances of our model, and propose scalable M -estimators for recovering the underlying network structure. Simulations as well as an application to learning mixed genomic networks from next generation sequencing and mutation data demonstrate the versatility of our methods.},
	language = {en},
	author = {Yang, Eunho and Baker, Yulia and Ravikumar, Pradeep and Allen, Genevera I and Liu, Zhandong},
	file = {Yang et al. - Mixed Graphical Models via Exponential Families.pdf:/Users/shannondickson/Zotero/storage/PCFPUF2D/Yang et al. - Mixed Graphical Models via Exponential Families.pdf:application/pdf},
}

@inproceedings{yangMixedGraphicalModels2014,
	title = {Mixed {Graphical} {Models} via {Exponential} {Families}},
	url = {https://proceedings.mlr.press/v33/yang14a.html},
	abstract = {Markov Random Fields, or undirected graphical models are widely used to model high-dimensional multivariate data. Classical instances of these models, such as Gaussian Graphical and Ising Models, as well as recent extensions to graphical models specified by univariate exponential families, assume all variables arise from the same distribution. Complex data from high-throughput genomics and social networking for example, often contain discrete, count, and continuous variables measured on the same set of samples. To model such heterogeneous data, we develop a {\textbackslash}emphnovel class of mixed graphical models by specifying that each node-conditional distribution is a member of a possibly different univariate exponential family. We study several instances of our model, and propose scalable M-estimators for recovering the underlying network structure. Simulations as well as an application to learning mixed genomic networks from next generation sequencing and mutation data demonstrate the versatility of our methods.},
	language = {en},
	urldate = {2023-01-21},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Yang, Eunho and Baker, Yulia and Ravikumar, Pradeep and Allen, Genevera and Liu, Zhandong},
	month = apr,
	year = {2014},
	note = {ISSN: 1938-7228},
	pages = {1042--1050},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/L2IX6UM4/Yang et al. - 2014 - Mixed Graphical Models via Exponential Families.pdf:application/pdf},
}

@article{chenSelectionEstimationMixed2015,
	title = {Selection and estimation for mixed graphical models},
	volume = {102},
	issn = {0006-3444},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5018402/},
	doi = {10.1093/biomet/asu051},
	abstract = {We consider the problem of estimating the parameters in a pairwise graphical model in which the distribution of each node, conditioned on the others, may have a different exponential family form. We identify restrictions on the parameter space required for the existence of a well-defined joint density, and establish the consistency of the neighbourhood selection approach for graph reconstruction in high dimensions when the true underlying graph is sparse. Motivated by our theoretical results, we investigate the selection of edges between nodes whose conditional distributions take different parametric forms, and show that efficiency can be gained if edge estimates obtained from the regressions of particular nodes are used to reconstruct the graph. These results are illustrated with examples of Gaussian, Bernoulli, Poisson and exponential distributions. Our theoretical findings are corroborated by evidence from simulation studies.},
	number = {1},
	urldate = {2023-01-21},
	journal = {Biometrika},
	author = {Chen, Shizhe and Witten, Daniela M. and shojaie, Ali},
	month = mar,
	year = {2015},
	pmid = {27625437},
	pmcid = {PMC5018402},
	pages = {47--64},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/M4QUSGL3/Chen et al. - 2015 - Selection and estimation for mixed graphical model.pdf:application/pdf},
}

@article{epskampQgraphNetworkVisualizations2012,
	title = {\textbf{qgraph} : {Network} {Visualizations} of {Relationships} in {Psychometric} {Data}},
	volume = {48},
	issn = {1548-7660},
	shorttitle = {\textbf{qgraph}},
	url = {http://www.jstatsoft.org/v48/i04/},
	doi = {10.18637/jss.v048.i04},
	abstract = {We present the qgraph package for R, which provides an interface to visualize data through network modeling techniques. For instance, a correlation matrix can be represented as a network in which each variable is a node and each correlation an edge; by varying the width of the edges according to the magnitude of the correlation, the structure of the correlation matrix can be visualized. A wide variety of matrices that are used in statistics can be represented in this fashion, for example matrices that contain (implied) covariances, factor loadings, regression parameters and p values. qgraph can also be used as a psychometric tool, as it performs exploratory and conﬁrmatory factor analysis, using sem and lavaan; the output of these packages is automatically visualized in qgraph, which may aid the interpretation of results. In this article, we introduce qgraph by applying the package functions to data from the NEO-PI-R, a widely used personality questionnaire.},
	language = {en},
	number = {4},
	urldate = {2023-01-22},
	journal = {Journal of Statistical Software},
	author = {Epskamp, Sacha and Cramer, Angélique O. J. and Waldorp, Lourens J. and Schmittmann, Verena D. and Borsboom, Denny},
	year = {2012},
	file = {Epskamp et al. - 2012 - qgraph  Network Visualizations of Relation.pdf:/Users/shannondickson/Zotero/storage/U53VH3QH/Epskamp et al. - 2012 - qgraph  Network Visualizations of Relation.pdf:application/pdf},
}

@article{bethlehemSelectionBiasWeb2010,
	title = {Selection {Bias} in {Web} {Surveys}},
	volume = {78},
	issn = {1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2010.00112.x},
	doi = {10.1111/j.1751-5823.2010.00112.x},
	abstract = {At first sight, web surveys seem to be an interesting and attractive means of data collection. They provide simple, cheap, and fast access to a large group of potential respondents. However, web surveys are not without methodological problems. Specific groups in the populations are under-represented because they have less access to Internet. Furthermore, recruitment of respondents is often based on self-selection. Both under-coverage and self-selection may lead to biased estimates. This paper describes these methodological problems. It also explores the effect of various correction techniques (adjustment weighting and use of reference surveys). This all leads to the question whether properly design web surveys can be used for data collection. The paper attempts to answer this question. It concludes that under-coverage problems may solve itself in the future, but that self-selection leads to unreliable survey outcomes.},
	language = {en},
	number = {2},
	urldate = {2023-01-22},
	journal = {International Statistical Review},
	author = {Bethlehem, Jelke},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1751-5823.2010.00112.x},
	keywords = {Adjustment weighting, bias, online survey, reference survey, self-selection, under-coverage, web survey},
	pages = {161--188},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/SEYGEGMN/Bethlehem - 2010 - Selection Bias in Web Surveys.pdf:application/pdf},
}

@article{curtinEffectsResponseRate2000,
	title = {The {Effects} of {Response} {Rate} {Changes} on the {Index} of {Consumer} {Sentiment}},
	volume = {64},
	issn = {0033362X},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1086/318638},
	doi = {10.1086/318638},
	abstract = {From 1979 to 1996, the Survey of Consumer Attitudes response rate remained roughly 70 percent. But number of calls to complete an interview and proportion of interviews requiring refusal conversion doubled. Using call-record histories, we explore what the consequences of lower response rates would have been if these additional efforts had not been undertaken. Both number of calls and initially cooperating (vs. initially refusing) are related to the Index of Consumer Sentiment (ICS), but only number of calls survives a control for demographic characteristics. We assess the impact of excluding respondents who required refusal conversion (which reduces the response rate 5–10 percentage points), respondents who required more than ﬁve calls to complete the interview (reducing the response rate about 25 percentage points), and those who required more than two calls (a reduction of about 50 percentage points). We found no effect of excluding any of these respondent groups on cross-sectional estimates of the ICS using monthly samples of hundreds of cases. For yearly estimates, based on thousands of cases, the exclusion of respondents who required more calls (though not of initial refusers) had an effect, but a very small one. One of the exclusions generally affected estimates of change over time in the ICS, irrespective of sample size.},
	language = {en},
	number = {4},
	urldate = {2023-01-23},
	journal = {Public Opinion Quarterly},
	author = {Curtin, Richard and Presser, Stanley and Singer, Eleanor},
	year = {2000},
	pages = {413--428},
}

@article{keeterConsequencesReducingNonresponse2000,
	title = {Consequences of {Reducing} {Nonresponse} in a {National} {Telephone} {Survey}},
	volume = {64},
	issn = {0033-362X},
	url = {https://www.jstor.org/stable/3078812},
	abstract = {Critics of public opinion polls often claim that methodological shortcuts taken to collect timely data produce biased results. This study compares two random digit dial national telephone surveys that used identical questionnaires but very different levels of effort: a "Standard" survey conducted over a 5-day period that used a sample of adults who were home when the interviewer called, and a "Rigorous" survey conducted over an 8-week period that used random selection from among all adult household members. Response rates, computed according to AAPOR guidelines, were 60.6 percent for the Rigorous and 36.0 percent for the Standard study. Nonetheless, the two surveys produced similar results. Across 91 comparisons, no difference exceeded 9 percentage points, and the average difference was about 2 percentage points. Most of the statistically significant differences were among demographic items. Very few significant differences were found on attention to media and engagement in politics, social trust and connectedness, and most social and political attitudes, including even those toward surveys.},
	number = {2},
	urldate = {2023-01-23},
	journal = {The Public Opinion Quarterly},
	author = {Keeter, Scott and Miller, Carolyn and Kohut, Andrew and Groves, R. M. and Presser, Stanley},
	year = {2000},
	note = {Publisher: [Oxford University Press, American Association for Public Opinion Research]},
	pages = {125--148},
	file = {3097-should-high-response-rates-really-be-a-primary-objective.pdf:/Users/shannondickson/Zotero/storage/CLQVESS9/3097-should-high-response-rates-really-be-a-primary-objective.pdf:application/pdf;JSTOR Full Text PDF:/Users/shannondickson/Zotero/storage/3ZSHYW35/Keeter et al. - 2000 - Consequences of Reducing Nonresponse in a National.pdf:application/pdf},
}

@article{beullensShouldHighResponse2012,
	title = {Should high response rates really be a primary objective?},
	volume = {5},
	issn = {2168-0094},
	url = {https://surveypractice.scholasticahq.com/article/3097-should-high-response-rates-really-be-a-primary-objective},
	doi = {10.29115/SP-2012-0019},
	language = {en},
	number = {3},
	urldate = {2023-01-23},
	journal = {Survey Practice},
	author = {Beullens, Koen and Loosveldt, Geert},
	month = oct,
	year = {2012},
	pages = {1--5},
	file = {Beullens and Loosveldt - 2012 - Should high response rates really be a primary obj.pdf:/Users/shannondickson/Zotero/storage/NYL96Z4Z/Beullens and Loosveldt - 2012 - Should high response rates really be a primary obj.pdf:application/pdf},
}

@article{cornesseReviewConceptualApproaches2020,
	title = {A {Review} of {Conceptual} {Approaches} and {Empirical} {Evidence} on {Probability} and {Nonprobability} {Sample} {Survey} {Research}},
	volume = {8},
	issn = {2325-0984},
	url = {https://doi.org/10.1093/jssam/smz041},
	doi = {10.1093/jssam/smz041},
	abstract = {There is an ongoing debate in the survey research literature about whether and when probability and nonprobability sample surveys produce accurate estimates of a larger population. Statistical theory provides a justification for confidence in probability sampling as a function of the survey design, whereas inferences based on nonprobability sampling are entirely dependent on models for validity. This article reviews the current debate about probability and nonprobability sample surveys. We describe the conditions under which nonprobability sample surveys may provide accurate results in theory and discuss empirical evidence on which types of samples produce the highest accuracy in practice. From these theoretical and empirical considerations, we derive best-practice recommendations and outline paths for future research.},
	number = {1},
	urldate = {2023-01-30},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Cornesse, Carina and Blom, Annelies G and Dutwin, David and Krosnick, Jon A and De Leeuw, Edith D and Legleye, Stéphane and Pasek, Josh and Pennay, Darren and Phillips, Benjamin and Sakshaug, Joseph W and Struminskaya, Bella and Wenz, Alexander},
	month = feb,
	year = {2020},
	pages = {4--36},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/5XSVW587/Cornesse et al. - 2020 - A Review of Conceptual Approaches and Empirical Ev.pdf:application/pdf},
}

@article{epskampGaussianGraphicalModel2018,
	title = {The {Gaussian} {Graphical} {Model} in {Cross}-{Sectional} and {Time}-{Series} {Data}},
	volume = {53},
	issn = {0027-3171},
	url = {https://doi.org/10.1080/00273171.2018.1454823},
	doi = {10.1080/00273171.2018.1454823},
	abstract = {We discuss the Gaussian graphical model (GGM; an undirected network of partial correlation coefficients) and detail its utility as an exploratory data analysis tool. The GGM shows which variables predict one-another, allows for sparse modeling of covariance structures, and may highlight potential causal relationships between observed variables. We describe the utility in three kinds of psychological data sets: data sets in which consecutive cases are assumed independent (e.g., cross-sectional data), temporally ordered data sets (e.g., n = 1 time series), and a mixture of the 2 (e.g., n {\textgreater} 1 time series). In time-series analysis, the GGM can be used to model the residual structure of a vector-autoregression analysis (VAR), also termed graphical VAR. Two network models can then be obtained: a temporal network and a contemporaneous network. When analyzing data from multiple subjects, a GGM can also be formed on the covariance structure of stationary means—the between-subjects network. We discuss the interpretation of these models and propose estimation methods to obtain these networks, which we implement in the R packages graphicalVAR and mlVAR. The methods are showcased in two empirical examples, and simulation studies on these methods are included in the supplementary materials.},
	number = {4},
	urldate = {2023-01-30},
	journal = {Multivariate Behavioral Research},
	author = {Epskamp, Sacha and Waldorp, Lourens J. and Mõttus, René and Borsboom, Denny},
	month = jul,
	year = {2018},
	pmid = {29658809},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171.2018.1454823},
	keywords = {exploratory-data analysis, multilevel modeling, multivariate analysis, network modeling, Time-series analysis},
	pages = {453--480},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/RBCLMYJZ/Epskamp et al. - 2018 - The Gaussian Graphical Model in Cross-Sectional an.pdf:application/pdf},
}

@article{grovesNonresponseRatesNonresponse2006,
	title = {Nonresponse {Rates} and {Nonresponse} {Bias} in {Household} {Surveys}},
	volume = {70},
	issn = {0033-362X},
	url = {https://www.jstor.org/stable/4124220},
	abstract = {Many surveys of the U.S. household population are experiencing higher refusal rates. Nonresponse can, but need not, induce nonresponse bias in survey estimates. Recent empirical findings illustrate cases when the linkage between nonresponse rates and nonresponse biases is absent. Despite this, professional standards continue to urge high response rates. Statistical expressions of nonresponse bias can be translated into causal models to guide hypotheses about when nonresponse causes bias. Alternative designs to measure nonresponse bias exist, providing different but incomplete information about the nature of the bias. A synthesis of research studies estimating nonresponse bias shows the bias often present. A logical question at this moment in history is what advantage probability sample surveys have if they suffer from high nonresponse rates. Since postsurvey adjustment for nonresponse requires auxiliary variables, the answer depends on the nature of the design and the quality of the auxiliary variables.},
	number = {5},
	urldate = {2023-01-31},
	journal = {The Public Opinion Quarterly},
	author = {Groves, R. M.},
	year = {2006},
	note = {Publisher: [Oxford University Press, American Association for Public Opinion Research]},
	pages = {646--675},
	file = {JSTOR Full Text PDF:/Users/shannondickson/Zotero/storage/WDDD7PQ4/Groves - 2006 - Nonresponse Rates and Nonresponse Bias in Househol.pdf:application/pdf},
}

@article{epskampTutorialRegularizedPartial2018a,
	title = {A tutorial on regularized partial correlation networks},
	volume = {23},
	issn = {1939-1463},
	doi = {10.1037/met0000167},
	abstract = {Recent years have seen an emergence of network modeling applied to moods, attitudes, and problems in the realm of psychology. In this framework, psychological variables are understood to directly affect each other rather than being caused by an unobserved latent entity. In this tutorial, we introduce the reader to estimating the most popular network model for psychological data: the partial correlation network. We describe how regularization techniques can be used to efficiently estimate a parsimonious and interpretable network structure in psychological data. We show how to perform these analyses in R and demonstrate the method in an empirical example on posttraumatic stress disorder data. In addition, we discuss the effect of the hyperparameter that needs to be manually set by the researcher, how to handle non-normal data, how to determine the required sample size for a network analysis, and provide a checklist with potential solutions for problems that can arise when estimating regularized partial correlation networks. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
	journal = {Psychological Methods},
	author = {Epskamp, Sacha and Fried, Eiko I.},
	year = {2018},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Methodology, Models, Neural Networks, Posttraumatic Stress Disorder, Statistical Analysis},
	pages = {617--634},
	file = {Full Text:/Users/shannondickson/Zotero/storage/2YWJR4EU/Epskamp and Fried - 2018 - A tutorial on regularized partial correlation netw.pdf:application/pdf},
}

@article{epskampEstimatingPsychologicalNetworks2018,
	title = {Estimating psychological networks and their accuracy: {A} tutorial paper},
	volume = {50},
	issn = {1554-3528},
	shorttitle = {Estimating psychological networks and their accuracy},
	url = {https://doi.org/10.3758/s13428-017-0862-1},
	doi = {10.3758/s13428-017-0862-1},
	abstract = {The usage of psychological networks that conceptualize behavior as a complex interplay of psychological and other components has gained increasing popularity in various research fields. While prior publications have tackled the topics of estimating and interpreting such networks, little work has been conducted to check how accurate (i.e., prone to sampling variation) networks are estimated, and how stable (i.e., interpretation remains similar with less observations) inferences from the network structure (such as centrality indices) are. In this tutorial paper, we aim to introduce the reader to this field and tackle the problem of accuracy under sampling variation. We first introduce the current state-of-the-art of network estimation. Second, we provide a rationale why researchers should investigate the accuracy of psychological networks. Third, we describe how bootstrap routines can be used to (A) assess the accuracy of estimated network connections, (B) investigate the stability of centrality indices, and (C) test whether network connections and centrality estimates for different variables differ from each other. We introduce two novel statistical methods: for (B) the correlation stability coefficient, and for (C) the bootstrapped difference test for edge-weights and centrality indices. We conducted and present simulation studies to assess the performance of both methods. Finally, we developed the free R-package bootnet that allows for estimating psychological networks in a generalized framework in addition to the proposed bootstrap methods. We showcase bootnet in a tutorial, accompanied by R syntax, in which we analyze a dataset of 359 women with posttraumatic stress disorder available online.},
	language = {en},
	number = {1},
	urldate = {2023-01-31},
	journal = {Behavior Research Methods},
	author = {Epskamp, Sacha and Borsboom, Denny and Fried, Eiko I.},
	month = feb,
	year = {2018},
	pages = {195--212},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/GNJZBLIY/Epskamp et al. - 2018 - Estimating psychological networks and their accura.pdf:application/pdf},
}

@article{laurieStrategiesReducingNonresponse,
	title = {Strategies for {Reducing} {Nonresponse} in a {Longitudinal} {Panel} {Survey}},
	language = {en},
	author = {Laurie, Heather and Smith, Rachel and Scott, Lynne},
	file = {Laurie et al. - Strategies for Reducing Nonresponse in a Longitudi.pdf:/Users/shannondickson/Zotero/storage/ZWTBNJF5/Laurie et al. - Strategies for Reducing Nonresponse in a Longitudi.pdf:application/pdf},
}

@article{sakshaugReducingNonresponseData2022,
	title = {Reducing {Nonresponse} and {Data} {Linkage} {Consent} {Bias} in {Large}-{Scale} {Panel} {Surveys}},
	volume = {25},
	issn = {1558-9544},
	url = {https://www.degruyter.com/document/doi/10.1515/fhep-2021-0060/html?lang=en},
	doi = {10.1515/fhep-2021-0060},
	abstract = {Selection bias is an ongoing concern in large-scale panel surveys where the cumulative effects of unit nonresponse increase at each subsequent wave of data collection. A second source of selection bias in panel studies is the inability to link respondents to supplementary administrative records, either because respondents do not consent to link or the matching algorithm fails to locate their administrative records. Both sources of selection bias can affect the validity of conclusions drawn from these data sources. In this article, I discuss recently proposed methods of reducing both sources of selection bias in panel studies, with a special emphasis on reducing selection bias in the US Health and Retirement Study.},
	language = {en},
	number = {1-2},
	urldate = {2023-01-31},
	journal = {Forum for Health Economics and Policy},
	author = {Sakshaug, Joseph W.},
	month = dec,
	year = {2022},
	note = {Publisher: De Gruyter
Section: Forum for Health Economics \& Policy},
	keywords = {health and retirement study, post-survey adjustments, questionnaire design, selection bias},
	pages = {41--55},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/JTY9SR62/Sakshaug - 2022 - Reducing Nonresponse and Data Linkage Consent Bias.pdf:application/pdf},
}

@article{lippsAttritionHouseholdsIndividuals2009,
	title = {Attrition of {Households} and {Individuals} in {Panel} {Surveys}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=1367371},
	doi = {10.2139/ssrn.1367371},
	abstract = {Attrition is mostly caused by not contacted or refusing sample members. On one hand it is well-known that reasons to attrite due to non-contact are different from those that are due to refusal. On the other hand does non-contact most probably affect household attrition, while refusal can be effective on both households and individuals. In this article, attrition on both the household and (conditional on household participation) the individual level is analysed in three panel surveys from the Cross National Equivalent File (CNEF): the German SocioEconomic Panel (GSOEP), the British Household Panel Study (BHPS), and the Swiss Household Panel (SHP). To follow households over time we use a common rule in all three surveys. First, we find different attrition magnitudes and patterns both across the surveys and also on the household and the individual level. Second, there is more evidence for reinforced rather than compensated household level selection effects if the individual level is also taken into account.},
	language = {en},
	urldate = {2023-01-31},
	journal = {SSRN Electronic Journal},
	author = {Lipps, Oliver},
	year = {2009},
	file = {Lipps - 2009 - Attrition of Households and Individuals in Panel S.pdf:/Users/shannondickson/Zotero/storage/39UUB26G/Lipps - 2009 - Attrition of Households and Individuals in Panel S.pdf:application/pdf},
}

@misc{kernLongitudinalFrameworkPredicting2019,
	title = {A {Longitudinal} {Framework} for {Predicting} {Nonresponse} in {Panel} {Surveys}},
	url = {http://arxiv.org/abs/1909.13361},
	abstract = {Nonresponse in panel studies can lead to a substantial loss in data quality due to its potential to introduce bias and distort survey estimates. Recent work investigates the usage of machine learning to predict nonresponse in advance, such that predicted nonresponse propensities can be used to inform the data collection process. However, predicting nonresponse in panel studies requires accounting for the longitudinal data structure in terms of model building, tuning, and evaluation. This study proposes a longitudinal framework for predicting nonresponse with machine learning and multiple panel waves and illustrates its application. With respect to model building, this approach utilizes information from multiple waves by introducing features that aggregate previous (non)response patterns. Concerning model tuning and evaluation, temporal cross-validation is employed by iterating through pairs of panel waves such that the training and test sets move in time. Implementing this approach with data from a German probability-based mixed-mode panel shows that aggregating information over multiple panel waves can be used to build prediction models with competitive and robust performance over all test waves.},
	language = {en},
	urldate = {2023-01-31},
	publisher = {arXiv},
	author = {Kern, Christoph and Weiss, Bernd and Kolb, Jan-Philipp},
	month = nov,
	year = {2019},
	note = {arXiv:1909.13361 [cs, stat]},
	keywords = {Statistics - Applications, Statistics - Methodology, Computer Science - Machine Learning},
	file = {Kern et al. - 2019 - A Longitudinal Framework for Predicting Nonrespons.pdf:/Users/shannondickson/Zotero/storage/ZBZB4AMR/Kern et al. - 2019 - A Longitudinal Framework for Predicting Nonrespons.pdf:application/pdf},
}

@article{lippsEffectsDifferentIncentives,
	title = {Eﬀects of diﬀerent {Incentives} on {Attrition} and {Fieldwork} {Eﬀort} in {Telephone} {Household} {Panel} {Surveys}},
	abstract = {Little is known about sample behavior and ﬁeldwork eﬀects of diﬀerent incentives introduced in a household panel survey. This is especially true for telephone surveys. In a randomized experiment, the Swiss Household Panel implemented one prepaid and two promised nonmonetary incentives in the range of 10 to 15 Swiss Francs (7-10 e), plus a no incentive control group. The aim of the paper is to compare eﬀects of these incentives especially on cooperation, but also on sample selection and ﬁeldwork eﬀort, separated by the household and the subsequent individual level. We ﬁnd small positive cooperation eﬀects of the prepaid incentive on both the household and the individual level especially in larger households. Sample composition is aﬀected to a very minor extent. Finally, incentives tend to save ﬁeldwork time and partially the number of contacts needed on the individual level.},
	language = {en},
	author = {Lipps, Oliver},
	file = {Lipps - Eﬀects of diﬀerent Incentives on Attrition and Fie.pdf:/Users/shannondickson/Zotero/storage/TTD6Y2FT/Lipps - Eﬀects of diﬀerent Incentives on Attrition and Fie.pdf:application/pdf},
}

@article{schoeniResponseRatesNational2013,
	title = {Response {Rates} in {National} {Panel} {Surveys}},
	volume = {645},
	issn = {0002-7162},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3555140/},
	doi = {10.1177/0002716212456363},
	number = {1},
	urldate = {2023-01-31},
	journal = {The Annals of the American Academy of Political and Social Science},
	author = {Schoeni, Robert F. and Stafford, Frank and McGonagle, Katherine A. and Andreski, Patricia},
	month = jan,
	year = {2013},
	pmid = {23358122},
	pmcid = {PMC3555140},
	pages = {60--87},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/CMP9K4SL/Schoeni et al. - 2013 - Response Rates in National Panel Surveys.pdf:application/pdf},
}

@article{leeperWhereHaveRespondents2019,
	title = {Where {Have} the {Respondents} {Gone}? {Perhaps} {We} {Ate} {Them} {All}},
	volume = {83},
	issn = {0033-362X},
	shorttitle = {Where {Have} the {Respondents} {Gone}?},
	url = {https://doi.org/10.1093/poq/nfz010},
	doi = {10.1093/poq/nfz010},
	abstract = {Rising rates of nonresponse are one of the most-debated issues in contemporary survey research. While early survey research regularly achieved response rates close to 100 percent, contemporary telephone interviewing methods in the United States regularly obtain response rates below 10 percent, due to a mix of noncontact and refusals. Existing research has examined a number of factors that explain variation in response rates, yet almost all such work considers survey response as an isolated, independent event. This note aims to stimulate debate by suggesting that a paradigm shift in theorizing nonresponse is needed. I diagnose the problem of nonresponse not only as an individual-level, survey-specific phenomenon, but as something larger and more collective: namely, as a common pool resource (CPR) problem. Because researchers acting independently might each seek to maximize their response rate and achieve intended sample sizes, the common pool resource of human respondents can be prone to overextraction. In addition to thinking about “benefit-cost” explanations for why respondents might respond to a specific survey, considering responses as a shared resource focuses attention on cross-level theory on how the survey field might collectively govern responses from human populations. Rather than testing CPR theory directly, I instead describe why thinking of nonresponse as a CPR problem may be useful, use the United States as a case study to demonstrate the possible scale of response extraction, and leverage findings from CPR studies to suggest directions for future research into nonresponse.},
	number = {S1},
	urldate = {2023-01-31},
	journal = {Public Opinion Quarterly},
	author = {Leeper, Thomas J},
	month = jul,
	year = {2019},
	pages = {280--288},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/DJ9YWIIJ/Leeper - 2019 - Where Have the Respondents Gone Perhaps We Ate Th.pdf:application/pdf},
}

@misc{czajkaDecliningResponseRates2016,
	title = {Declining {Response} {Rates} in {Federal} {Surveys}: {Trends} and {Implications}},
	language = {en},
	publisher = {Office of the Assistant Secretary for Planning and Evaluation},
	author = {Czajka, John L and Beyler, Amy},
	year = {2016},
	file = {Czajka and Beyler - Declining Response Rates in Federal Surveys Trend.pdf:/Users/shannondickson/Zotero/storage/GEQDML6J/Czajka and Beyler - Declining Response Rates in Federal Surveys Trend.pdf:application/pdf},
}

@article{beullensResponseRatesEuropean2018,
	title = {Response {Rates} in the {European} {Social} {Survey}: {Increasing}, {Decreasing}, or a {Matter} of {Fieldwork} {Efforts}?},
	issn = {2296-4754},
	shorttitle = {Response {Rates} in the {European} {Social} {Survey}},
	url = {https://surveyinsights.org/?p=9673},
	doi = {10.13094/SMIF-2018-00003},
	abstract = {Response rates are declining increasing the risk of nonresponse error. The reasons for this decline are multiple: the rise of online surveys, mobile phones, and information requests, societal changes, greater awareness of privacy issues, etc. To combat this decline, fieldwork efforts have become increasingly intensive: widespread use of respondent incentives, advance letters, and an increased number of contact attempts. In addition, complex fieldwork strategies such as adaptive call scheduling or responsive designs have been implemented. The additional efforts to counterbalance nonresponse complicate the measurement of the increased difficulty of contacting potential respondents and convincing them to cooperate. 
To observe developments in response rates we use the first seven rounds of the European Social Survey, a biennial face-to-face survey. Despite some changes to the fieldwork efforts in some countries (choice of survey agency, available sample frame, incentives, number of contact attempts), many characteristics have been stable: effective sample size,  (contact and) survey mode, and questionnaire design. To control for the different country composition in different rounds, we use a multilevel model with countries as level 2 units and response rates in each country-year combination as level 1 units. The results show a declining trend, although only round 7 has a significant negative effect.},
	language = {en-US},
	urldate = {2023-01-31},
	journal = {Survey Methods: Insights from the Field (SMIF)},
	author = {Beullens, Koen and Loosveldt, Geert and Vandenplas, Caroline and Stoop, Ineke},
	month = apr,
	year = {2018},
}

@article{cornesseThereAssociationSurvey2018,
	title = {Is there an association between survey characteristics and representativeness? {A} meta-analysis},
	volume = {Vol 12},
	shorttitle = {Is there an association between survey characteristics and representativeness?},
	url = {https://ojs.ub.uni-konstanz.de/srm/article/view/7205},
	doi = {10.18148/SRM/2018.V12I1.7205},
	abstract = {How to achieve survey representativeness is a controversially debated issue in the field of survey methodology. Common questions include whether probability-based samples produce more representative data than nonprobability samples, whether the response rate determines the overall degree of survey representativeness, and which survey modes are effective in generating highly representative data. This meta-analysis contributes to this debate by synthesizing and analyzing the literature on two common measures of survey representativeness (R-Indicators and descriptive benchmark comparisons). Our findings indicate that probability-based samples (compared to nonprobability samples), mixed-mode surveys (compared to single-mode surveys), and other-than-Web modes (compared to Web surveys) are more representative, respectively. In addition, we find that there is a positive association between representativeness and the response rate as well as the number of auxiliary variables used in representativeness assessments. Furthermore, we identify significant gaps in the research literature that we hope might encourage further research in this area.},
	language = {en},
	urldate = {2023-01-31},
	journal = {Survey Research Methods},
	author = {Cornesse, Carina and Bosnjak, Michael},
	month = apr,
	year = {2018},
	note = {Artwork Size: 1-13 Pages
Publisher: European Survey Research Association},
	keywords = {Meta-analysis, representativeness, R-Indicator, response rate, nonprobability sampling, mixed mode, web surveys, auxiliary data},
	pages = {1--13 Pages},
	annote = {SeriesInformation
Survey Research Methods, Vol 12, No 1 (2018)},
	file = {Cornesse and Bosnjak - 2018 - Is there an association between survey characteris.pdf:/Users/shannondickson/Zotero/storage/IQX8FWEZ/Cornesse and Bosnjak - 2018 - Is there an association between survey characteris.pdf:application/pdf},
}

@article{lippsEffectsDifferentIncentives2010,
	title = {Effects of different {Incentives} on {Attrition} and {Fieldwork} {Effort} in {Telephone} {Household} {Panel} {Surveys}},
	volume = {4},
	copyright = {Copyright (c) 2015 Survey Research Methods},
	issn = {1864-3361},
	url = {https://ojs.ub.uni-konstanz.de/srm/article/view/3538},
	doi = {10.18148/srm/2010.v4i2.3538},
	abstract = {Little is known about sample behavior and fieldwork effects of different incentives introduced in a household panel survey. This is especially true for telephone surveys. In a randomized experiment, the Swiss Household Panel implemented one prepaid and two promised non-monetary incentives in the range of 10 to 15 Swiss Francs (7-10 EUR), plus a no incentive control group. The aim of the paper is to compare effects of these incentives especially on cooperation, but also on sample selection and fieldwork effort, separated by the household and the subsequent individual level. We find small positive cooperation effects of the prepaid incentive on both the household and the individual level especially in larger households. Sample composition is affected to a very minor extent. Finally, incentives tend to save fieldwork time and partially the number of contacts needed on the individual level.},
	language = {en},
	number = {2},
	urldate = {2023-01-31},
	journal = {Survey Research Methods},
	author = {Lipps, Oliver},
	month = sep,
	year = {2010},
	note = {Number: 2},
	keywords = {attrition, bias, fieldwork effort, incentive effects, nonresponse},
	pages = {81--90},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/BCIJ2MNZ/Lipps - 2010 - Effects of different Incentives on Attrition and F.pdf:application/pdf},
}

@misc{SamplingHardtoreachPopulations,
	title = {Sampling 'hard-to-reach' populations in health research: yield from a study targeting {Americans} living in {Canada} {\textbar} {BMC} {Medical} {Research} {Methodology} {\textbar} {Full} {Text}},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-8-57},
	urldate = {2023-01-31},
}

@article{southernSamplingHardtoreachPopulations2008,
	title = {Sampling 'hard-to-reach' populations in health research: yield from a study targeting {Americans} living in {Canada}},
	volume = {8},
	issn = {1471-2288},
	shorttitle = {Sampling 'hard-to-reach' populations in health research},
	url = {https://doi.org/10.1186/1471-2288-8-57},
	doi = {10.1186/1471-2288-8-57},
	abstract = {Some populations targeted in survey research can be hard to reach, either because of lack of contact information, or non-existent databases to inform sampling. Here, we present a methodological "case-report" of the yield of a multi-step survey study assessing views on health care among American emigres to Canada, a hard-to-reach population.},
	number = {1},
	urldate = {2023-01-31},
	journal = {BMC Medical Research Methodology},
	author = {Southern, Danielle A. and Lewis, Steven and Maxwell, Colleen J. and Dunn, James R. and Noseworthy, Tom W. and Corbett, Gail and Thomas, Karen and Ghali, William A.},
	month = aug,
	year = {2008},
	keywords = {Internet Protocol Address, Media Coverage, Newspaper Story, Press Release, Recruitment Goal},
	pages = {57},
	file = {Full Text PDF:/Users/shannondickson/Zotero/storage/3K7E29NW/Southern et al. - 2008 - Sampling 'hard-to-reach' populations in health res.pdf:application/pdf},
}

@article{beebeTestingImpactMixed2018,
	title = {Testing the {Impact} of {Mixed}‐{Mode} {Designs} ({Mail} and {Web}) and {Multiple} {Contact} {Attempts} within {Mode} ({Mail} or {Web}) on {Clinician} {Survey} {Response}},
	volume = {53},
	issn = {0017-9124},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6056581/},
	doi = {10.1111/1475-6773.12827},
	abstract = {Objective
To compare response rate and nonresponse bias across two mixed‐mode survey designs and two single‐mode designs.

Data Sources
This experiment was embedded in a clinician survey of knowledge and attitudes regarding HPV vaccination (n = 275).

Study Design
Clinicians were randomly assigned one of two mixed‐mode (mail/web or web/mail) or single‐mode designs (mail‐only/web‐only). Differences in response rate and nonresponse bias were assessed.

Principal Findings
Using a multiple‐contact protocol increased response, and sending a web survey first provided the more rapid response. Overall, the mixed‐mode survey designs generated final response rates approximately 10 percentage points higher than their single‐mode counterparts, although only the final response differences between the mail‐only and web/mail conditions attained statistical significance (32.1 percent vs. 48 percent, respectively; p = .005). Observed differences did not result in nonresponse bias.

Conclusions
Results support mixing modes of survey administration and web‐based data collection in a multiple contact survey data collection protocol.},
	number = {Suppl Suppl 1},
	urldate = {2023-02-01},
	journal = {Health Services Research},
	author = {Beebe, Timothy J. and Jacobson, Robert M. and Jenkins, Sarah M. and Lackore, Kandace A. and Rutten, Lila J. Finney},
	month = aug,
	year = {2018},
	pmid = {29355920},
	pmcid = {PMC6056581},
	pages = {3070--3083},
	file = {PubMed Central Full Text PDF:/Users/shannondickson/Zotero/storage/VHXNZN9C/Beebe et al. - 2018 - Testing the Impact of Mixed‐Mode Designs (Mail and.pdf:application/pdf},
}
